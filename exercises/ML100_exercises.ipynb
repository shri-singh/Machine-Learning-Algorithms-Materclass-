{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML100 Exercises: Data Splitting & Feature Fundamentals\n",
    "\n",
    "These exercises cover core data preparation skills: train/test splitting, cross-validation,\n",
    "data leakage detection, feature engineering, and preprocessing pipelines.\n",
    "\n",
    "**Difficulty increases with each exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup: Run this cell first\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Stratified Train/Test Split\n",
    "\n",
    "**Goal:** Split a dataset using `train_test_split` with stratification and verify that\n",
    "the class proportions in the train and test sets match the original dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset.\n",
    "2. Perform an 80/20 stratified split (`random_state=42`).\n",
    "3. Print the class proportions (as fractions) in the original, train, and test sets.\n",
    "4. Assert that all three sets have approximately the same proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# TODO 1: Perform an 80/20 stratified split\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# TODO 2: Compute class proportions for original, train, and test sets\n",
    "# Hint: use np.bincount(y) / len(y) for proportions\n",
    "# original_proportions = ...\n",
    "# train_proportions = ...\n",
    "# test_proportions = ...\n",
    "\n",
    "# TODO 3: Print all three proportion arrays\n",
    "\n",
    "# TODO 4: Assert train proportions are close to original proportions\n",
    "# np.testing.assert_array_almost_equal(..., ..., decimal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 1. Stratified 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Compute class proportions\n",
    "original_proportions = np.bincount(y) / len(y)\n",
    "train_proportions = np.bincount(y_train) / len(y_train)\n",
    "test_proportions = np.bincount(y_test) / len(y_test)\n",
    "\n",
    "# 3. Print proportions\n",
    "print(f\"Original proportions: {original_proportions}\")\n",
    "print(f\"Train proportions:    {train_proportions}\")\n",
    "print(f\"Test proportions:     {test_proportions}\")\n",
    "\n",
    "# 4. Assert approximately equal\n",
    "np.testing.assert_array_almost_equal(\n",
    "    train_proportions, original_proportions, decimal=1\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    test_proportions, original_proportions, decimal=1\n",
    ")\n",
    "print(\"\\nAll proportions match!\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: 5-Fold Cross-Validation on Iris\n",
    "\n",
    "**Goal:** Implement 5-fold cross-validation on the Iris dataset using\n",
    "`LogisticRegression` and report the mean and standard deviation of accuracy.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset.\n",
    "2. Create a `LogisticRegression` model (`max_iter=200, random_state=42`).\n",
    "3. Run 5-fold cross-validation using `cross_val_score`.\n",
    "4. Print mean accuracy +/- standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# TODO 1: Create LogisticRegression model (max_iter=200, random_state=42)\n",
    "# model = ...\n",
    "\n",
    "# TODO 2: Run 5-fold cross-validation\n",
    "# scores = cross_val_score(..., ..., ..., cv=5, scoring='accuracy')\n",
    "\n",
    "# TODO 3: Print individual fold scores\n",
    "# print(f\"Fold scores: {scores}\")\n",
    "\n",
    "# TODO 4: Print mean +/- std\n",
    "# print(f\"Accuracy: {scores.mean():.4f} +/- {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 1. Create model\n",
    "model = LogisticRegression(max_iter=200, random_state=42)\n",
    "\n",
    "# 2. Run 5-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# 3. Print individual fold scores\n",
    "print(f\"Fold scores: {scores}\")\n",
    "\n",
    "# 4. Print mean +/- std\n",
    "print(f\"Accuracy: {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Identify and Fix Data Leakage\n",
    "\n",
    "**Goal:** The code below contains a common data leakage mistake. Identify the\n",
    "problem, explain why it causes leakage, and rewrite the code correctly.\n",
    "\n",
    "**Tasks:**\n",
    "1. Read the \"bad\" code below.\n",
    "2. In a comment, explain what the leakage is.\n",
    "3. Rewrite the code so that no leakage occurs.\n",
    "4. Compare the scores from the leaky and correct approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Starter Code\n",
    "# ---- BAD CODE (contains data leakage) ----\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- LEAKY approach ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # <-- Scaling on ALL data before splitting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "leaky_model = LogisticRegression(random_state=42, max_iter=200)\n",
    "leaky_model.fit(X_train, y_train)\n",
    "leaky_score = leaky_model.score(X_test, y_test)\n",
    "print(f\"Leaky test accuracy: {leaky_score:.4f}\")\n",
    "\n",
    "# TODO 1: Explain in a comment why the code above has data leakage.\n",
    "#\n",
    "# Your explanation: ...\n",
    "#\n",
    "\n",
    "# TODO 2: Rewrite the code correctly (split FIRST, then scale)\n",
    "# X_train_raw, X_test_raw, y_train, y_test = ...\n",
    "# scaler_correct = StandardScaler()\n",
    "# X_train_correct = scaler_correct.fit_transform(...)  # fit on train only\n",
    "# X_test_correct = scaler_correct.transform(...)        # transform test only\n",
    "# correct_model = ...\n",
    "# correct_score = ...\n",
    "\n",
    "# TODO 3: Print both scores and compare\n",
    "# print(f\"Correct test accuracy: {correct_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- LEAKY approach (for comparison) ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Leakage: test data stats leak into training\n",
    "\n",
    "X_train_leak, X_test_leak, y_train_leak, y_test_leak = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "leaky_model = LogisticRegression(random_state=42, max_iter=200)\n",
    "leaky_model.fit(X_train_leak, y_train_leak)\n",
    "leaky_score = leaky_model.score(X_test_leak, y_test_leak)\n",
    "print(f\"Leaky test accuracy: {leaky_score:.4f}\")\n",
    "\n",
    "# Explanation: The StandardScaler is fit on the ENTIRE dataset (train + test)\n",
    "# before splitting. This means the mean and standard deviation used for\n",
    "# scaling include information from the test set, which the model should\n",
    "# never see during training. This is data leakage.\n",
    "\n",
    "# --- CORRECT approach ---\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_correct = StandardScaler()\n",
    "X_train_correct = scaler_correct.fit_transform(X_train_raw)  # fit on train only\n",
    "X_test_correct = scaler_correct.transform(X_test_raw)         # transform test\n",
    "\n",
    "correct_model = LogisticRegression(random_state=42, max_iter=200)\n",
    "correct_model.fit(X_train_correct, y_train)\n",
    "correct_score = correct_model.score(X_test_correct, y_test)\n",
    "print(f\"Correct test accuracy: {correct_score:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference: {abs(leaky_score - correct_score):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Feature Engineering from Dates and Numerics\n",
    "\n",
    "**Goal:** Given a synthetic dataset with a date column and numeric columns,\n",
    "engineer at least 3 new meaningful features.\n",
    "\n",
    "**Tasks:**\n",
    "1. From the `date` column, extract: year, month, and day-of-week.\n",
    "2. Create an interaction feature: `feature_A * feature_B`.\n",
    "3. Create a ratio feature: `feature_A / (feature_B + 1)`.\n",
    "4. Create a binned version of `feature_A` (e.g., low / medium / high).\n",
    "5. Print the resulting DataFrame showing all new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Starter Code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n = 200\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2020-01-01', periods=n, freq='D'),\n",
    "    'feature_A': np.random.normal(50, 15, n),\n",
    "    'feature_B': np.random.exponential(10, n),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Food'], n)\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# TODO 1: Extract year, month, and day-of-week from 'date'\n",
    "# df['year'] = ...\n",
    "# df['month'] = ...\n",
    "# df['day_of_week'] = ...\n",
    "\n",
    "# TODO 2: Create interaction feature\n",
    "# df['A_times_B'] = ...\n",
    "\n",
    "# TODO 3: Create ratio feature (add 1 to denominator to avoid division by zero)\n",
    "# df['A_over_B'] = ...\n",
    "\n",
    "# TODO 4: Create binned version of feature_A (low / medium / high)\n",
    "# Hint: use pd.cut or pd.qcut\n",
    "# df['A_binned'] = ...\n",
    "\n",
    "# TODO 5: Print the DataFrame with new features\n",
    "# print(df.head(10))\n",
    "# print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n = 200\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2020-01-01', periods=n, freq='D'),\n",
    "    'feature_A': np.random.normal(50, 15, n),\n",
    "    'feature_B': np.random.exponential(10, n),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Food'], n)\n",
    "})\n",
    "\n",
    "# 1. Extract date features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "# 2. Interaction feature\n",
    "df['A_times_B'] = df['feature_A'] * df['feature_B']\n",
    "\n",
    "# 3. Ratio feature\n",
    "df['A_over_B'] = df['feature_A'] / (df['feature_B'] + 1)\n",
    "\n",
    "# 4. Binned version of feature_A\n",
    "df['A_binned'] = pd.qcut(df['feature_A'], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# 5. Print results\n",
    "print(\"DataFrame with engineered features:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nNew shape: {df.shape}\")\n",
    "print(f\"\\nNew columns: {[c for c in df.columns if c not in ['date', 'feature_A', 'feature_B', 'category']]}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: ColumnTransformer Pipeline\n",
    "\n",
    "**Goal:** Build a `ColumnTransformer` pipeline that applies `StandardScaler` to\n",
    "numeric features and `OneHotEncoder` to categorical features, then train a\n",
    "`LogisticRegression` through the pipeline.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a DataFrame with numeric and categorical columns.\n",
    "2. Define numeric and categorical column lists.\n",
    "3. Build a `ColumnTransformer` with appropriate transformers.\n",
    "4. Create a `Pipeline` with the preprocessor and `LogisticRegression`.\n",
    "5. Evaluate with 5-fold cross-validation and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Starter Code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a synthetic mixed-type dataset\n",
    "n = 500\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'income': np.random.normal(50000, 15000, n),\n",
    "    'hours_per_week': np.random.randint(10, 60, n),\n",
    "    'education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], n),\n",
    "    'occupation': np.random.choice(['Tech', 'Healthcare', 'Finance', 'Education', 'Other'], n),\n",
    "})\n",
    "# Binary target\n",
    "df['target'] = (df['income'] + df['age'] * 100 + np.random.normal(0, 5000, n) > 55000).astype(int)\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"Features:\")\n",
    "print(X.head())\n",
    "print(f\"\\nTarget distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# TODO 1: Define numeric and categorical columns\n",
    "# numeric_cols = [...]\n",
    "# categorical_cols = [...]\n",
    "\n",
    "# TODO 2: Build ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numeric_cols),\n",
    "#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# TODO 3: Create Pipeline with preprocessor + LogisticRegression\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression(random_state=42, max_iter=200))\n",
    "# ])\n",
    "\n",
    "# TODO 4: Evaluate with 5-fold cross-validation\n",
    "# scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# TODO 5: Print results\n",
    "# print(f\"CV Accuracy: {scores.mean():.4f} +/- {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a synthetic mixed-type dataset\n",
    "n = 500\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'income': np.random.normal(50000, 15000, n),\n",
    "    'hours_per_week': np.random.randint(10, 60, n),\n",
    "    'education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], n),\n",
    "    'occupation': np.random.choice(['Tech', 'Healthcare', 'Finance', 'Education', 'Other'], n),\n",
    "})\n",
    "df['target'] = (df['income'] + df['age'] * 100 + np.random.normal(0, 5000, n) > 55000).astype(int)\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# 1. Define column types\n",
    "numeric_cols = ['age', 'income', 'hours_per_week']\n",
    "categorical_cols = ['education', 'occupation']\n",
    "\n",
    "# 2. Build ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=200))\n",
    "])\n",
    "\n",
    "# 4. Evaluate with 5-fold cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# 5. Print results\n",
    "print(f\"Fold scores: {scores}\")\n",
    "print(f\"CV Accuracy: {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}