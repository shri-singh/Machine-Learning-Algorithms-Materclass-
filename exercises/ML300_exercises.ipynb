{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML300 Exercises: Logistic Regression & Classification\n",
    "\n",
    "These exercises cover classification fundamentals: logistic regression, evaluation metrics,\n",
    "ROC/PR curves, threshold optimization, imbalanced data handling, and probability calibration.\n",
    "\n",
    "**Difficulty increases with each exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup: Run this cell first\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Logistic Regression on Breast Cancer Dataset\n",
    "\n",
    "**Goal:** Train a logistic regression classifier on the breast cancer dataset and\n",
    "compute key classification metrics.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset and split 80/20.\n",
    "2. Train a `LogisticRegression` model (`max_iter=10000, random_state=42`).\n",
    "3. Generate predictions and compute the confusion matrix.\n",
    "4. Compute precision, recall, and F1 score.\n",
    "5. Display the confusion matrix and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# TODO 1: Split 80/20\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# TODO 2: Train LogisticRegression\n",
    "# model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# TODO 3: Predict and compute confusion matrix\n",
    "# y_pred = model.predict(X_test)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "# TODO 4: Compute precision, recall, F1\n",
    "# precision = precision_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# print(f\"\\nPrecision: {precision:.4f}\")\n",
    "# print(f\"Recall:    {recall:.4f}\")\n",
    "# print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# TODO 5: Display confusion matrix and classification report\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 1. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict and confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 4. Metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# 5. Display\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: ROC and Precision-Recall Curves\n",
    "\n",
    "**Goal:** Plot ROC and Precision-Recall curves for the logistic regression model\n",
    "trained on the breast cancer dataset, and report AUC scores.\n",
    "\n",
    "**Tasks:**\n",
    "1. Train a logistic regression model (reuse from Exercise 1 or retrain).\n",
    "2. Get probability predictions with `predict_proba`.\n",
    "3. Plot the ROC curve with the AUC score in the legend.\n",
    "4. Plot the Precision-Recall curve with the average precision in the legend.\n",
    "5. Display both plots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO 1: Get probability predictions for the positive class\n",
    "# y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# TODO 2: Compute ROC curve and AUC\n",
    "# fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)\n",
    "# roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# TODO 3: Compute Precision-Recall curve and average precision\n",
    "# precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "# avg_precision = average_precision_score(y_test, y_prob)\n",
    "\n",
    "# TODO 4: Plot both curves side by side\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "#\n",
    "# # ROC curve\n",
    "# axes[0].plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "# axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "# axes[0].set_xlabel('False Positive Rate')\n",
    "# axes[0].set_ylabel('True Positive Rate')\n",
    "# axes[0].set_title('ROC Curve')\n",
    "# axes[0].legend()\n",
    "#\n",
    "# # PR curve\n",
    "# axes[1].plot(recall_vals, precision_vals, label=f'PR (AP = {avg_precision:.4f})')\n",
    "# axes[1].set_xlabel('Recall')\n",
    "# axes[1].set_ylabel('Precision')\n",
    "# axes[1].set_title('Precision-Recall Curve')\n",
    "# axes[1].legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 1. Probability predictions\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. ROC\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# 3. PR\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "avg_precision = average_precision_score(y_test, y_prob)\n",
    "\n",
    "# 4. Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(recall_vals, precision_vals, label=f'PR (AP = {avg_precision:.4f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Optimal Threshold for F1 Score\n",
    "\n",
    "**Goal:** Find the decision threshold that maximizes the F1 score rather than\n",
    "using the default 0.5 threshold.\n",
    "\n",
    "**Tasks:**\n",
    "1. Train a logistic regression model on the breast cancer dataset.\n",
    "2. Compute precision and recall at various thresholds using `precision_recall_curve`.\n",
    "3. Compute F1 score at each threshold.\n",
    "4. Find the threshold that maximizes F1.\n",
    "5. Compare the F1 score at the default threshold (0.5) vs. the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# TODO 1: Compute precision, recall at various thresholds\n",
    "# precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# TODO 2: Compute F1 at each threshold\n",
    "# Note: precisions and recalls have one more element than thresholds\n",
    "# f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "\n",
    "# TODO 3: Find optimal threshold\n",
    "# best_idx = np.argmax(f1_scores)\n",
    "# best_threshold = thresholds[best_idx]\n",
    "# best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# TODO 4: Compare with default threshold\n",
    "# default_f1 = f1_score(y_test, (y_prob >= 0.5).astype(int))\n",
    "# optimal_f1 = f1_score(y_test, (y_prob >= best_threshold).astype(int))\n",
    "# print(f\"Default threshold (0.5): F1 = {default_f1:.4f}\")\n",
    "# print(f\"Optimal threshold ({best_threshold:.4f}): F1 = {optimal_f1:.4f}\")\n",
    "\n",
    "# TODO 5: Plot F1 vs threshold\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(thresholds, f1_scores)\n",
    "# plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Optimal: {best_threshold:.4f}')\n",
    "# plt.axvline(x=0.5, color='g', linestyle='--', label='Default: 0.5')\n",
    "# plt.xlabel('Threshold')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('F1 Score vs Decision Threshold')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 1. Precision-recall at various thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# 2. F1 at each threshold\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "\n",
    "# 3. Optimal threshold\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# 4. Compare\n",
    "default_f1 = f1_score(y_test, (y_prob >= 0.5).astype(int))\n",
    "optimal_f1 = f1_score(y_test, (y_prob >= best_threshold).astype(int))\n",
    "print(f\"Default threshold (0.5):            F1 = {default_f1:.4f}\")\n",
    "print(f\"Optimal threshold ({best_threshold:.4f}): F1 = {optimal_f1:.4f}\")\n",
    "\n",
    "# 5. Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, f1_scores)\n",
    "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Optimal: {best_threshold:.4f}')\n",
    "plt.axvline(x=0.5, color='g', linestyle='--', label='Default: 0.5')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Decision Threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: Handling Imbalanced Data with class_weight\n",
    "\n",
    "**Goal:** Train on an imbalanced dataset with and without `class_weight='balanced'`\n",
    "and compare the results.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create an imbalanced dataset using `make_classification(weights=[0.9, 0.1])`.\n",
    "2. Train logistic regression WITHOUT `class_weight` (default).\n",
    "3. Train logistic regression WITH `class_weight='balanced'`.\n",
    "4. Compare confusion matrices and classification reports.\n",
    "5. Discuss the effect of class weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Starter Code\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Create imbalanced dataset: 90% class 0, 10% class 1\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=20, n_informative=10,\n",
    "    weights=[0.9, 0.1], flip_y=0.01, random_state=42\n",
    ")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class 0: {np.mean(y == 0):.1%}, Class 1: {np.mean(y == 1):.1%}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# TODO 1: Train WITHOUT class_weight\n",
    "# model_default = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# model_default.fit(X_train, y_train)\n",
    "# y_pred_default = model_default.predict(X_test)\n",
    "\n",
    "# TODO 2: Train WITH class_weight='balanced'\n",
    "# model_balanced = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "# model_balanced.fit(X_train, y_train)\n",
    "# y_pred_balanced = model_balanced.predict(X_test)\n",
    "\n",
    "# TODO 3: Compare confusion matrices side by side\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test, y_pred_default, ax=axes[0])\n",
    "# axes[0].set_title('Default (No Weighting)')\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test, y_pred_balanced, ax=axes[1])\n",
    "# axes[1].set_title('Balanced Weighting')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 4: Print classification reports for both\n",
    "# print(\"=== Default ===\")\n",
    "# print(classification_report(y_test, y_pred_default))\n",
    "# print(\"=== Balanced ===\")\n",
    "# print(classification_report(y_test, y_pred_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Imbalanced dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=20, n_informative=10,\n",
    "    weights=[0.9, 0.1], flip_y=0.01, random_state=42\n",
    ")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class 0: {np.mean(y == 0):.1%}, Class 1: {np.mean(y == 1):.1%}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 1. Without class_weight\n",
    "model_default = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_default.fit(X_train, y_train)\n",
    "y_pred_default = model_default.predict(X_test)\n",
    "\n",
    "# 2. With class_weight='balanced'\n",
    "model_balanced = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "model_balanced.fit(X_train, y_train)\n",
    "y_pred_balanced = model_balanced.predict(X_test)\n",
    "\n",
    "# 3. Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_default, ax=axes[0])\n",
    "axes[0].set_title('Default (No Weighting)')\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_balanced, ax=axes[1])\n",
    "axes[1].set_title('Balanced Weighting')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Classification reports\n",
    "print(\"=== Default (No Weighting) ===\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "print(\"=== Balanced Weighting ===\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n",
    "\n",
    "# Discussion:\n",
    "# - Without class_weight, the model tends to predict the majority class (0)\n",
    "#   more often, resulting in low recall for the minority class (1).\n",
    "# - With class_weight='balanced', the model penalizes misclassification of\n",
    "#   the minority class more, improving recall for class 1 at some cost\n",
    "#   to precision.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: Probability Calibration\n",
    "\n",
    "**Goal:** Apply `CalibratedClassifierCV` to a logistic regression model and\n",
    "compare calibration curves before and after calibration.\n",
    "\n",
    "**Tasks:**\n",
    "1. Train a logistic regression model on the breast cancer dataset.\n",
    "2. Compute the calibration curve for the uncalibrated model.\n",
    "3. Apply `CalibratedClassifierCV` with `method='sigmoid'`.\n",
    "4. Compute the calibration curve for the calibrated model.\n",
    "5. Plot both calibration curves on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 1: Train uncalibrated model\n",
    "# model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# TODO 2: Get calibration curve for uncalibrated model\n",
    "# y_prob_uncalib = model.predict_proba(X_test)[:, 1]\n",
    "# fraction_pos_uncalib, mean_pred_uncalib = calibration_curve(\n",
    "#     y_test, y_prob_uncalib, n_bins=10\n",
    "# )\n",
    "\n",
    "# TODO 3: Apply CalibratedClassifierCV\n",
    "# calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "# calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# TODO 4: Get calibration curve for calibrated model\n",
    "# y_prob_calib = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "# fraction_pos_calib, mean_pred_calib = calibration_curve(\n",
    "#     y_test, y_prob_calib, n_bins=10\n",
    "# )\n",
    "\n",
    "# TODO 5: Plot both calibration curves\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(mean_pred_uncalib, fraction_pos_uncalib, 's-', label='Uncalibrated')\n",
    "# plt.plot(mean_pred_calib, fraction_pos_calib, 's-', label='Calibrated (sigmoid)')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "# plt.xlabel('Mean Predicted Probability')\n",
    "# plt.ylabel('Fraction of Positives')\n",
    "# plt.title('Calibration Curves')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Train uncalibrated model\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Calibration curve - uncalibrated\n",
    "y_prob_uncalib = model.predict_proba(X_test)[:, 1]\n",
    "fraction_pos_uncalib, mean_pred_uncalib = calibration_curve(\n",
    "    y_test, y_prob_uncalib, n_bins=10\n",
    ")\n",
    "\n",
    "# 3. Calibrate\n",
    "calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Calibration curve - calibrated\n",
    "y_prob_calib = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "fraction_pos_calib, mean_pred_calib = calibration_curve(\n",
    "    y_test, y_prob_calib, n_bins=10\n",
    ")\n",
    "\n",
    "# 5. Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_pred_uncalib, fraction_pos_uncalib, 's-', label='Uncalibrated')\n",
    "plt.plot(mean_pred_calib, fraction_pos_calib, 's-', label='Calibrated (sigmoid)')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Note: Logistic regression is already fairly well-calibrated,\n",
    "# so the improvement may be small. Calibration is more impactful\n",
    "# for models like SVMs or Naive Bayes.\n",
    "print(f\"Uncalibrated accuracy: {model.score(X_test, y_test):.4f}\")\n",
    "print(f\"Calibrated accuracy:   {calibrated_model.score(X_test, y_test):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}