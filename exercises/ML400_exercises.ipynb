{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML400 Exercises: KNN & Clustering\n",
    "\n",
    "These exercises cover K-Nearest Neighbors classification, KMeans clustering,\n",
    "hierarchical clustering, DBSCAN, and PCA for dimensionality reduction.\n",
    "\n",
    "**Difficulty increases with each exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup: Run this cell first\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "from sklearn.datasets import load_iris, make_blobs, make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: KNN Classifier -- Accuracy vs. k (Scaled vs. Unscaled)\n",
    "\n",
    "**Goal:** Train KNN classifiers on the Iris dataset with different values of k,\n",
    "both with and without feature scaling, and plot accuracy vs. k.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset and split 80/20.\n",
    "2. For k in [1, 3, 5, 7, 9], train KNN **without** scaling and record test accuracy.\n",
    "3. For k in [1, 3, 5, 7, 9], train KNN **with** `StandardScaler` and record test accuracy.\n",
    "4. Plot both accuracy curves on the same figure.\n",
    "5. Print the best k for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "\n",
    "# TODO 1: Train KNN without scaling for each k\n",
    "# unscaled_accuracies = []\n",
    "# for k in k_values:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(X_train, y_train)\n",
    "#     acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "#     unscaled_accuracies.append(acc)\n",
    "\n",
    "# TODO 2: Scale the data and train KNN for each k\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# scaled_accuracies = []\n",
    "# for k in k_values:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(X_train_scaled, y_train)\n",
    "#     acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
    "#     scaled_accuracies.append(acc)\n",
    "\n",
    "# TODO 3: Plot accuracy vs k\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(k_values, unscaled_accuracies, 'o-', label='Unscaled')\n",
    "# plt.plot(k_values, scaled_accuracies, 's-', label='Scaled')\n",
    "# plt.xlabel('k (Number of Neighbors)')\n",
    "# plt.ylabel('Test Accuracy')\n",
    "# plt.title('KNN: Accuracy vs k')\n",
    "# plt.legend()\n",
    "# plt.xticks(k_values)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 4: Print best k for each approach\n",
    "# best_unscaled_k = k_values[np.argmax(unscaled_accuracies)]\n",
    "# best_scaled_k = k_values[np.argmax(scaled_accuracies)]\n",
    "# print(f\"Best k (unscaled): {best_unscaled_k} with accuracy {max(unscaled_accuracies):.4f}\")\n",
    "# print(f\"Best k (scaled):   {best_scaled_k} with accuracy {max(scaled_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "\n",
    "# 1. Without scaling\n",
    "unscaled_accuracies = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "    unscaled_accuracies.append(acc)\n",
    "\n",
    "# 2. With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "scaled_accuracies = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
    "    scaled_accuracies.append(acc)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, unscaled_accuracies, 'o-', label='Unscaled')\n",
    "plt.plot(k_values, scaled_accuracies, 's-', label='Scaled')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('KNN: Accuracy vs k')\n",
    "plt.legend()\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Best k\n",
    "best_unscaled_k = k_values[np.argmax(unscaled_accuracies)]\n",
    "best_scaled_k = k_values[np.argmax(scaled_accuracies)]\n",
    "print(f\"Best k (unscaled): {best_unscaled_k} with accuracy {max(unscaled_accuracies):.4f}\")\n",
    "print(f\"Best k (scaled):   {best_scaled_k} with accuracy {max(scaled_accuracies):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: KMeans -- Elbow Method and Silhouette Score\n",
    "\n",
    "**Goal:** Apply KMeans to a blob dataset, use the elbow method (inertia) and\n",
    "silhouette score to find the optimal number of clusters.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate data with `make_blobs` (4 centers, `random_state=42`).\n",
    "2. Run KMeans for k = 2 through 8, recording inertia and silhouette scores.\n",
    "3. Plot the elbow curve (inertia vs. k).\n",
    "4. Plot silhouette score vs. k.\n",
    "5. Identify the optimal k from both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Starter Code\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate blob data with 4 centers\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=500, centers=4, cluster_std=1.0, random_state=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.5, s=20)\n",
    "plt.title('True Clusters')\n",
    "plt.show()\n",
    "\n",
    "k_range = range(2, 9)\n",
    "\n",
    "# TODO 1: Run KMeans for each k, record inertia and silhouette scores\n",
    "# inertias = []\n",
    "# silhouette_scores = []\n",
    "# for k in k_range:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "#     labels = kmeans.fit_predict(X)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "#     silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# TODO 2: Plot elbow curve and silhouette score side by side\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "#\n",
    "# axes[0].plot(list(k_range), inertias, 'o-')\n",
    "# axes[0].set_xlabel('Number of Clusters (k)')\n",
    "# axes[0].set_ylabel('Inertia')\n",
    "# axes[0].set_title('Elbow Method')\n",
    "#\n",
    "# axes[1].plot(list(k_range), silhouette_scores, 's-')\n",
    "# axes[1].set_xlabel('Number of Clusters (k)')\n",
    "# axes[1].set_ylabel('Silhouette Score')\n",
    "# axes[1].set_title('Silhouette Score')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 3: Print the optimal k\n",
    "# best_k = list(k_range)[np.argmax(silhouette_scores)]\n",
    "# print(f\"Best k by silhouette score: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=500, centers=4, cluster_std=1.0, random_state=42\n",
    ")\n",
    "\n",
    "k_range = range(2, 9)\n",
    "\n",
    "# 1. Run KMeans for each k\n",
    "inertias = []\n",
    "silhouette_scores_list = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores_list.append(silhouette_score(X, labels))\n",
    "\n",
    "# 2. Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(list(k_range), inertias, 'o-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "\n",
    "axes[1].plot(list(k_range), silhouette_scores_list, 's-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Optimal k\n",
    "best_k = list(k_range)[np.argmax(silhouette_scores_list)]\n",
    "print(f\"Best k by silhouette score: {best_k}\")\n",
    "print(f\"Silhouette scores: {dict(zip(k_range, [f'{s:.3f}' for s in silhouette_scores_list]))}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Hierarchical Clustering with Dendrogram\n",
    "\n",
    "**Goal:** Create a dendrogram using `scipy` hierarchical clustering on a small\n",
    "dataset to visualize the cluster merging process.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate a small dataset with `make_blobs` (50 samples, 3 centers).\n",
    "2. Compute the linkage matrix using Ward's method.\n",
    "3. Plot the dendrogram.\n",
    "4. Draw a horizontal line at the cut height for 3 clusters.\n",
    "5. Extract cluster labels and visualize the clustered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Starter Code\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Generate small dataset\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=50, centers=3, cluster_std=1.0, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 1: Compute linkage matrix (Ward's method)\n",
    "# Z = linkage(X, method='ward')\n",
    "\n",
    "# TODO 2: Plot dendrogram\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# dendrogram(Z, leaf_rotation=90, leaf_font_size=8)\n",
    "# plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Distance')\n",
    "\n",
    "# TODO 3: Draw horizontal line for 3-cluster cut\n",
    "# Choose a distance threshold that yields 3 clusters\n",
    "# plt.axhline(y=..., color='r', linestyle='--', label='3-cluster cut')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 4: Extract cluster labels for 3 clusters\n",
    "# labels = fcluster(Z, t=3, criterion='maxclust')\n",
    "\n",
    "# TODO 5: Visualize clustered data\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50)\n",
    "# axes[0].set_title('True Labels')\n",
    "# axes[1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "# axes[1].set_title('Hierarchical Clustering Labels')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=50, centers=3, cluster_std=1.0, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Linkage\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "# 2. Dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z, leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "\n",
    "# 3. Cut line (pick a height between the 2-cluster and 3-cluster merges)\n",
    "# Inspect Z to find appropriate height; a value like 15 typically works\n",
    "cut_height = Z[-3, 2] + (Z[-2, 2] - Z[-3, 2]) / 2\n",
    "plt.axhline(y=cut_height, color='r', linestyle='--', label=f'3-cluster cut (h={cut_height:.1f})')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Extract labels\n",
    "labels = fcluster(Z, t=3, criterion='maxclust')\n",
    "\n",
    "# 5. Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50)\n",
    "axes[0].set_title('True Labels')\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "axes[1].set_title('Hierarchical Clustering Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: KMeans vs. DBSCAN on make_moons\n",
    "\n",
    "**Goal:** Compare KMeans and DBSCAN on the `make_moons` dataset, which has\n",
    "non-convex cluster shapes that KMeans struggles with.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate data with `make_moons(n_samples=300, noise=0.1)`.\n",
    "2. Apply KMeans with k=2.\n",
    "3. Apply DBSCAN with `eps=0.2` and `min_samples=5`.\n",
    "4. Visualize results side by side (true labels, KMeans, DBSCAN).\n",
    "5. Discuss why DBSCAN performs better on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Starter Code\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Generate moon-shaped data\n",
    "X, y_true = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# TODO 1: Apply KMeans (k=2)\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "# kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# TODO 2: Apply DBSCAN (eps=0.2, min_samples=5)\n",
    "# dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "# dbscan_labels = dbscan.fit_predict(X)\n",
    "\n",
    "# TODO 3: Visualize all three side by side\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "#\n",
    "# axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=20)\n",
    "# axes[0].set_title('True Labels')\n",
    "#\n",
    "# axes[1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=20)\n",
    "# axes[1].set_title('KMeans (k=2)')\n",
    "#\n",
    "# axes[2].scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', s=20)\n",
    "# axes[2].set_title(f'DBSCAN (eps=0.2) - {len(set(dbscan_labels) - {-1})} clusters')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 4: Discuss why DBSCAN works better here\n",
    "# Your answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "X, y_true = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# 1. KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# 2. DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "\n",
    "# 3. Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=20)\n",
    "axes[0].set_title('True Labels')\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=20)\n",
    "axes[1].set_title('KMeans (k=2)')\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels) - {-1})\n",
    "n_noise = np.sum(dbscan_labels == -1)\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', s=20)\n",
    "axes[2].set_title(f'DBSCAN (eps=0.2) - {n_clusters_dbscan} clusters, {n_noise} noise')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Discussion:\n",
    "# KMeans assumes convex (spherical) clusters and uses centroids.\n",
    "# It cannot handle the crescent-moon shape and splits the data incorrectly.\n",
    "# DBSCAN is density-based and can find arbitrarily shaped clusters,\n",
    "# making it much better suited for non-convex geometries like moons.\n",
    "print(\"KMeans struggles with non-convex shapes because it uses centroids.\")\n",
    "print(\"DBSCAN is density-based and can discover arbitrarily shaped clusters.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: PCA + Clustering Pipeline\n",
    "\n",
    "**Goal:** Apply PCA to reduce a 10-feature dataset to 2D, plot the explained\n",
    "variance ratio, and then cluster the reduced data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate a high-dimensional dataset with `make_blobs` (10 features, 4 centers).\n",
    "2. Apply PCA and plot the cumulative explained variance ratio.\n",
    "3. Reduce to 2 components and visualize the data.\n",
    "4. Apply KMeans (k=4) on the 2D PCA data.\n",
    "5. Visualize the true labels vs. KMeans labels on the PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Starter Code\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate 10-feature dataset\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=500, n_features=10, centers=4,\n",
    "    cluster_std=2.0, random_state=42\n",
    ")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "\n",
    "# Scale the data first\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# TODO 1: Apply PCA (keep all components first to see variance)\n",
    "# pca_full = PCA(random_state=42)\n",
    "# pca_full.fit(X_scaled)\n",
    "\n",
    "# TODO 2: Plot cumulative explained variance\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "# plt.plot(range(1, len(cumulative_var) + 1), cumulative_var, 'o-')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('PCA: Cumulative Explained Variance')\n",
    "# plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# TODO 3: Reduce to 2 components\n",
    "# pca_2d = PCA(n_components=2, random_state=42)\n",
    "# X_2d = pca_2d.fit_transform(X_scaled)\n",
    "# print(f\"Reduced shape: {X_2d.shape}\")\n",
    "# print(f\"Explained variance (2 components): {pca_2d.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# TODO 4: Apply KMeans on 2D data\n",
    "# kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "# kmeans_labels = kmeans.fit_predict(X_2d)\n",
    "\n",
    "# TODO 5: Visualize true vs KMeans labels\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=y_true, cmap='viridis', s=20, alpha=0.6)\n",
    "# axes[0].set_title('True Labels (PCA 2D)')\n",
    "# axes[0].set_xlabel('PC1')\n",
    "# axes[0].set_ylabel('PC2')\n",
    "# axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans_labels, cmap='viridis', s=20, alpha=0.6)\n",
    "# axes[1].set_title('KMeans Labels (PCA 2D)')\n",
    "# axes[1].set_xlabel('PC1')\n",
    "# axes[1].set_ylabel('PC2')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=500, n_features=10, centers=4,\n",
    "    cluster_std=2.0, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1. Full PCA\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# 2. Cumulative variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumulative_var) + 1), cumulative_var, 'o-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA: Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Reduce to 2D\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "print(f\"Reduced shape: {X_2d.shape}\")\n",
    "print(f\"Explained variance (2 components): {pca_2d.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# 4. KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_2d)\n",
    "\n",
    "# 5. Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=y_true, cmap='viridis', s=20, alpha=0.6)\n",
    "axes[0].set_title('True Labels (PCA 2D)')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans_labels, cmap='viridis', s=20, alpha=0.6)\n",
    "axes[1].set_title('KMeans Labels (PCA 2D)')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sil = silhouette_score(X_2d, kmeans_labels)\n",
    "print(f\"Silhouette score (PCA 2D + KMeans): {sil:.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}