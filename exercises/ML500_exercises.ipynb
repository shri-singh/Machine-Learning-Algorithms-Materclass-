{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML500 Exercises: Trees, Ensembles & Boosting\n",
    "\n",
    "These exercises cover decision trees, random forests, gradient boosting,\n",
    "histogram-based gradient boosting with early stopping, and feature importance.\n",
    "\n",
    "**Difficulty increases with each exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup: Run this cell first\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Decision Tree Visualization with Different Depths\n",
    "\n",
    "**Goal:** Train decision trees on the Iris dataset with different `max_depth`\n",
    "values and visualize them using `plot_tree`.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the Iris dataset and split 80/20.\n",
    "2. Train decision trees with `max_depth` = 2, 4, and None (unlimited).\n",
    "3. Report test accuracy for each depth.\n",
    "4. Visualize all three trees side by side using `plot_tree`.\n",
    "5. Discuss the tradeoff between depth and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "depths = [2, 4, None]\n",
    "\n",
    "# TODO 1: Train trees with different depths and record accuracies\n",
    "# models = {}\n",
    "# for depth in depths:\n",
    "#     dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "#     dt.fit(X_train, y_train)\n",
    "#     acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "#     models[depth] = dt\n",
    "#     print(f\"max_depth={str(depth):>4s}  Test accuracy: {acc:.4f}  Tree depth: {dt.get_depth()}\")\n",
    "\n",
    "# TODO 2: Visualize all three trees\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "# for idx, depth in enumerate(depths):\n",
    "#     plot_tree(\n",
    "#         models[depth], ax=axes[idx],\n",
    "#         feature_names=iris.feature_names,\n",
    "#         class_names=iris.target_names,\n",
    "#         filled=True, rounded=True, fontsize=8\n",
    "#     )\n",
    "#     axes[idx].set_title(f'max_depth={depth}')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "depths = [2, 4, None]\n",
    "\n",
    "# 1. Train and evaluate\n",
    "models = {}\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "    models[depth] = dt\n",
    "    print(f\"max_depth={str(depth):>4s}  Test accuracy: {acc:.4f}  Tree depth: {dt.get_depth()}\")\n",
    "\n",
    "# 2. Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "for idx, depth in enumerate(depths):\n",
    "    plot_tree(\n",
    "        models[depth], ax=axes[idx],\n",
    "        feature_names=iris.feature_names,\n",
    "        class_names=iris.target_names,\n",
    "        filled=True, rounded=True, fontsize=8\n",
    "    )\n",
    "    axes[idx].set_title(f'max_depth={depth}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Discussion:\n",
    "# - Shallow trees (depth=2) are simple but may underfit.\n",
    "# - Unlimited depth trees capture all training patterns but may overfit.\n",
    "# - A moderate depth (e.g., 4) often provides the best bias-variance tradeoff.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Decision Tree vs. Random Forest\n",
    "\n",
    "**Goal:** Compare a single decision tree against a random forest (100 trees) on\n",
    "the breast cancer dataset using 5-fold cross-validation.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset.\n",
    "2. Create a `DecisionTreeClassifier` (`random_state=42`).\n",
    "3. Create a `RandomForestClassifier` (100 trees, `random_state=42`).\n",
    "4. Evaluate both with 5-fold cross-validation.\n",
    "5. Print and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# TODO 1: Create models\n",
    "# dt = DecisionTreeClassifier(random_state=42)\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# TODO 2: 5-fold cross-validation for both\n",
    "# dt_scores = cross_val_score(dt, X, y, cv=5, scoring='accuracy')\n",
    "# rf_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# TODO 3: Print comparison\n",
    "# print(f\"Decision Tree:  {dt_scores.mean():.4f} +/- {dt_scores.std():.4f}\")\n",
    "# print(f\"Random Forest:  {rf_scores.mean():.4f} +/- {rf_scores.std():.4f}\")\n",
    "# print(f\"\\nImprovement: {(rf_scores.mean() - dt_scores.mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 1. Create models\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Cross-validation\n",
    "dt_scores = cross_val_score(dt, X, y, cv=5, scoring='accuracy')\n",
    "rf_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# 3. Comparison\n",
    "print(f\"Decision Tree:  {dt_scores.mean():.4f} +/- {dt_scores.std():.4f}\")\n",
    "print(f\"Random Forest:  {rf_scores.mean():.4f} +/- {rf_scores.std():.4f}\")\n",
    "print(f\"\\nImprovement: {(rf_scores.mean() - dt_scores.mean()):.4f}\")\n",
    "print(f\"\\nRandom Forest typically wins because it averages many trees,\")\n",
    "print(f\"reducing variance while maintaining low bias.\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Gradient Boosting -- Learning Rate Comparison\n",
    "\n",
    "**Goal:** Train `GradientBoostingClassifier` with different learning rates and\n",
    "plot training vs. validation error across boosting iterations.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset and split 80/20.\n",
    "2. Train GradientBoosting models with `learning_rate` = 0.01, 0.1, 0.5\n",
    "   (use `n_estimators=200`).\n",
    "3. For each model, extract the staged predictions (training and test error).\n",
    "4. Plot training and test error vs. number of iterations for all learning rates.\n",
    "5. Discuss the effect of learning rate on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "n_estimators = 200\n",
    "\n",
    "# TODO 1: Train models and collect staged scores\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "#\n",
    "# for idx, lr in enumerate(learning_rates):\n",
    "#     gb = GradientBoostingClassifier(\n",
    "#         n_estimators=n_estimators, learning_rate=lr, random_state=42\n",
    "#     )\n",
    "#     gb.fit(X_train, y_train)\n",
    "#\n",
    "#     # Staged predictions: error at each boosting iteration\n",
    "#     train_errors = []\n",
    "#     test_errors = []\n",
    "#     for y_train_pred in gb.staged_predict(X_train):\n",
    "#         train_errors.append(1 - accuracy_score(y_train, y_train_pred))\n",
    "#     for y_test_pred in gb.staged_predict(X_test):\n",
    "#         test_errors.append(1 - accuracy_score(y_test, y_test_pred))\n",
    "#\n",
    "#     # Plot\n",
    "#     axes[idx].plot(range(1, n_estimators + 1), train_errors, label='Train Error')\n",
    "#     axes[idx].plot(range(1, n_estimators + 1), test_errors, label='Test Error')\n",
    "#     axes[idx].set_xlabel('Number of Iterations')\n",
    "#     axes[idx].set_ylabel('Error Rate')\n",
    "#     axes[idx].set_title(f'Learning Rate = {lr}')\n",
    "#     axes[idx].legend()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "n_estimators = 200\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators, learning_rate=lr, random_state=42\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "\n",
    "    # Staged predictions\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for y_train_pred in gb.staged_predict(X_train):\n",
    "        train_errors.append(1 - accuracy_score(y_train, y_train_pred))\n",
    "    for y_test_pred in gb.staged_predict(X_test):\n",
    "        test_errors.append(1 - accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    axes[idx].plot(range(1, n_estimators + 1), train_errors, label='Train Error')\n",
    "    axes[idx].plot(range(1, n_estimators + 1), test_errors, label='Test Error')\n",
    "    axes[idx].set_xlabel('Number of Iterations')\n",
    "    axes[idx].set_ylabel('Error Rate')\n",
    "    axes[idx].set_title(f'Learning Rate = {lr}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "    final_acc = accuracy_score(y_test, gb.predict(X_test))\n",
    "    print(f\"LR={lr}: Final test accuracy = {final_acc:.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Discussion:\n",
    "# - Low learning rate (0.01): Slow convergence, may need more iterations.\n",
    "# - Medium learning rate (0.1): Good balance of speed and accuracy.\n",
    "# - High learning rate (0.5): Fast convergence but may overfit or oscillate.\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4: HistGradientBoosting with Early Stopping\n",
    "\n",
    "**Goal:** Use `HistGradientBoostingClassifier` with early stopping to\n",
    "automatically determine the optimal number of boosting iterations.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset and split 80/20.\n",
    "2. Train `HistGradientBoostingClassifier` with `early_stopping=True`,\n",
    "   `max_iter=500`, `validation_fraction=0.15`, `n_iter_no_change=10`.\n",
    "3. Report the best iteration and the number of iterations used.\n",
    "4. Report the test accuracy.\n",
    "5. Compare against a model without early stopping (`max_iter=500`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 1: Train with early stopping\n",
    "# hgb_early = HistGradientBoostingClassifier(\n",
    "#     max_iter=500,\n",
    "#     early_stopping=True,\n",
    "#     validation_fraction=0.15,\n",
    "#     n_iter_no_change=10,\n",
    "#     random_state=42\n",
    "# )\n",
    "# hgb_early.fit(X_train, y_train)\n",
    "\n",
    "# TODO 2: Report best iteration\n",
    "# print(f\"Number of iterations used: {hgb_early.n_iter_}\")\n",
    "# print(f\"Test accuracy (early stop): {accuracy_score(y_test, hgb_early.predict(X_test)):.4f}\")\n",
    "\n",
    "# TODO 3: Train without early stopping for comparison\n",
    "# hgb_full = HistGradientBoostingClassifier(\n",
    "#     max_iter=500, early_stopping=False, random_state=42\n",
    "# )\n",
    "# hgb_full.fit(X_train, y_train)\n",
    "# print(f\"\\nWithout early stopping:\")\n",
    "# print(f\"Iterations used: 500\")\n",
    "# print(f\"Test accuracy (no early stop): {accuracy_score(y_test, hgb_full.predict(X_test)):.4f}\")\n",
    "\n",
    "# TODO 4: Compare\n",
    "# print(f\"\\nEarly stopping saved {500 - hgb_early.n_iter_} iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1. With early stopping\n",
    "hgb_early = HistGradientBoostingClassifier(\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")\n",
    "hgb_early.fit(X_train, y_train)\n",
    "\n",
    "# 2. Report\n",
    "early_acc = accuracy_score(y_test, hgb_early.predict(X_test))\n",
    "print(f\"=== With Early Stopping ===\")\n",
    "print(f\"Iterations used: {hgb_early.n_iter_}\")\n",
    "print(f\"Test accuracy: {early_acc:.4f}\")\n",
    "\n",
    "# 3. Without early stopping\n",
    "hgb_full = HistGradientBoostingClassifier(\n",
    "    max_iter=500, early_stopping=False, random_state=42\n",
    ")\n",
    "hgb_full.fit(X_train, y_train)\n",
    "full_acc = accuracy_score(y_test, hgb_full.predict(X_test))\n",
    "print(f\"\\n=== Without Early Stopping ===\")\n",
    "print(f\"Iterations used: 500\")\n",
    "print(f\"Test accuracy: {full_acc:.4f}\")\n",
    "\n",
    "# 4. Compare\n",
    "print(f\"\\nEarly stopping saved {500 - hgb_early.n_iter_} iterations.\")\n",
    "print(f\"Accuracy difference: {early_acc - full_acc:.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 5: Permutation Importance vs. Built-in Feature Importance\n",
    "\n",
    "**Goal:** Compare permutation importance (model-agnostic) with the built-in\n",
    "`feature_importances_` of a Random Forest, and discuss differences.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset and split 80/20.\n",
    "2. Train a `RandomForestClassifier` (100 trees).\n",
    "3. Compute the built-in `feature_importances_`.\n",
    "4. Compute permutation importance on the test set.\n",
    "5. Plot both importance rankings side by side (top 10 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Starter Code\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 1: Train Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# print(f\"Test accuracy: {rf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# TODO 2: Get built-in feature importances\n",
    "# builtin_imp = rf.feature_importances_\n",
    "\n",
    "# TODO 3: Compute permutation importance\n",
    "# perm_imp = permutation_importance(\n",
    "#     rf, X_test, y_test, n_repeats=10, random_state=42\n",
    "# )\n",
    "\n",
    "# TODO 4: Get top 10 features for each method\n",
    "# builtin_sorted_idx = np.argsort(builtin_imp)[::-1][:10]\n",
    "# perm_sorted_idx = np.argsort(perm_imp.importances_mean)[::-1][:10]\n",
    "\n",
    "# TODO 5: Plot side by side\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "#\n",
    "# axes[0].barh(range(10), builtin_imp[builtin_sorted_idx])\n",
    "# axes[0].set_yticks(range(10))\n",
    "# axes[0].set_yticklabels(feature_names[builtin_sorted_idx])\n",
    "# axes[0].set_xlabel('Importance')\n",
    "# axes[0].set_title('Built-in Feature Importance (Gini)')\n",
    "# axes[0].invert_yaxis()\n",
    "#\n",
    "# axes[1].barh(range(10), perm_imp.importances_mean[perm_sorted_idx])\n",
    "# axes[1].set_yticks(range(10))\n",
    "# axes[1].set_yticklabels(feature_names[perm_sorted_idx])\n",
    "# axes[1].set_xlabel('Mean Accuracy Decrease')\n",
    "# axes[1].set_title('Permutation Importance')\n",
    "# axes[1].invert_yaxis()\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details><summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Train\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(f\"Test accuracy: {rf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# 2. Built-in importance\n",
    "builtin_imp = rf.feature_importances_\n",
    "\n",
    "# 3. Permutation importance\n",
    "perm_imp = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Top 10\n",
    "builtin_sorted_idx = np.argsort(builtin_imp)[::-1][:10]\n",
    "perm_sorted_idx = np.argsort(perm_imp.importances_mean)[::-1][:10]\n",
    "\n",
    "# 5. Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].barh(range(10), builtin_imp[builtin_sorted_idx])\n",
    "axes[0].set_yticks(range(10))\n",
    "axes[0].set_yticklabels(feature_names[builtin_sorted_idx])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Built-in Feature Importance (Gini)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(range(10), perm_imp.importances_mean[perm_sorted_idx])\n",
    "axes[1].set_yticks(range(10))\n",
    "axes[1].set_yticklabels(feature_names[perm_sorted_idx])\n",
    "axes[1].set_xlabel('Mean Accuracy Decrease')\n",
    "axes[1].set_title('Permutation Importance')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Discussion:\n",
    "# - Built-in importance is based on Gini impurity decrease (training data).\n",
    "# - Permutation importance measures accuracy drop when a feature is shuffled (test data).\n",
    "# - Permutation importance is more reliable as it uses the test set and is model-agnostic.\n",
    "# - Built-in importance can be biased toward high-cardinality features.\n",
    "print(\"\\nTop 5 features (built-in):\", list(feature_names[builtin_sorted_idx[:5]]))\n",
    "print(\"Top 5 features (permutation):\", list(feature_names[perm_sorted_idx[:5]]))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}