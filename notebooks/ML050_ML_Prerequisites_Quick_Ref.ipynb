{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Prerequisites Quick Reference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Manipulate data** using NumPy arrays and pandas DataFrames with confidence.\n",
    "2. **Recall core statistics concepts** -- mean, median, variance, and standard deviation -- and compute them in Python.\n",
    "3. **State fundamental probability rules**, including conditional probability and Bayes' theorem.\n",
    "4. **Perform basic linear algebra operations** -- dot products and matrix multiplication -- using NumPy.\n",
    "5. **Explain the intuition behind derivatives and gradients** as they relate to gradient descent.\n",
    "6. **Create common visualizations** (scatter plots, line plots, histograms, subplots) with matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "None -- this notebook *is* the prerequisite refresher. If any section feels completely unfamiliar, consider reviewing a dedicated resource before continuing with the ML modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Python Essentials for ML](#1.-Python-Essentials-for-ML)\n",
    "2. [Basic Statistics Review](#2.-Basic-Statistics-Review)\n",
    "3. [Probability Basics](#3.-Probability-Basics)\n",
    "4. [Linear Algebra Essentials](#4.-Linear-Algebra-Essentials)\n",
    "5. [Calculus Intuition](#5.-Calculus-Intuition)\n",
    "6. [Visualization Basics](#6.-Visualization-Basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Python Essentials for ML\n",
    "\n",
    "Machine learning in Python revolves around two libraries: **NumPy** for numerical computation and **pandas** for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 NumPy Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating arrays\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.arange(0, 10, 2)          # [0, 2, 4, 6, 8]\n",
    "c = np.linspace(0, 1, 5)         # 5 evenly spaced values from 0 to 1\n",
    "zeros = np.zeros((3, 4))         # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))           # 2x3 matrix of ones\n",
    "\n",
    "print(\"a      :\", a)\n",
    "print(\"b      :\", b)\n",
    "print(\"c      :\", c)\n",
    "print(\"zeros shape:\", zeros.shape)\n",
    "print(\"ones  shape:\", ones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "x = np.array([10, 20, 30])\n",
    "y = np.array([1, 2, 3])\n",
    "\n",
    "print(\"x + y  :\", x + y)\n",
    "print(\"x * y  :\", x * y)        # element-wise multiplication\n",
    "print(\"x ** 2 :\", x ** 2)\n",
    "print(\"sqrt(x):\", np.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and slicing\n",
    "arr = np.arange(10)\n",
    "print(\"arr          :\", arr)\n",
    "print(\"arr[2:5]     :\", arr[2:5])       # elements at index 2, 3, 4\n",
    "print(\"arr[::2]     :\", arr[::2])        # every other element\n",
    "print(\"arr[-3:]     :\", arr[-3:])        # last three elements\n",
    "\n",
    "# Boolean indexing\n",
    "print(\"arr[arr > 5] :\", arr[arr > 5])    # elements greater than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "mat = np.arange(12).reshape(3, 4)\n",
    "print(\"Matrix (3x4):\")\n",
    "print(mat)\n",
    "print(\"\\nTranspose (4x3):\")\n",
    "print(mat.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 pandas DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from a dictionary\n",
    "df = pd.DataFrame({\n",
    "    \"name\":   [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n",
    "    \"age\":    [25, 30, 35, 28],\n",
    "    \"salary\": [50000, 60000, 70000, 55000],\n",
    "})\n",
    "\n",
    "print(df)\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nColumn types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns and rows\n",
    "print(\"Single column (Series):\")\n",
    "print(df[\"salary\"])\n",
    "\n",
    "print(\"\\nRows where age > 27:\")\n",
    "print(df[df[\"age\"] > 27])\n",
    "\n",
    "print(\"\\nUsing .loc (label-based):\")\n",
    "print(df.loc[0:2, [\"name\", \"salary\"]])\n",
    "\n",
    "print(\"\\nUsing .iloc (integer-based):\")\n",
    "print(df.iloc[1:3, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Statistics Review\n",
    "\n",
    "Statistics provides the foundation for understanding data and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Measures of Central Tendency\n",
    "\n",
    "**Mean** (arithmetic average):\n",
    "\n",
    "$$\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "**Median**: the middle value when data is sorted. Robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([12, 15, 14, 10, 18, 22, 14, 16, 13, 100])  # 100 is an outlier\n",
    "\n",
    "print(f\"Mean   : {np.mean(data):.2f}\")\n",
    "print(f\"Median : {np.median(data):.2f}\")\n",
    "print()\n",
    "print(\"Notice how the mean is pulled up by the outlier (100),\")\n",
    "print(\"while the median remains robust.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Measures of Spread\n",
    "\n",
    "**Variance** -- average squared deviation from the mean:\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "**Standard Deviation** -- square root of variance (same units as data):\n",
    "\n",
    "$$\\sigma = \\sqrt{\\sigma^2}$$\n",
    "\n",
    "> *Note:* For a **sample** (as opposed to the full population), divide by $n - 1$ instead of $n$ (Bessel's correction). NumPy uses $n$ by default; pandas uses $n - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = np.array([12, 15, 14, 10, 18, 22, 14, 16, 13, 17])\n",
    "\n",
    "print(f\"Variance (population, ddof=0): {np.var(clean_data, ddof=0):.2f}\")\n",
    "print(f\"Variance (sample,     ddof=1): {np.var(clean_data, ddof=1):.2f}\")\n",
    "print(f\"Std Dev  (population, ddof=0): {np.std(clean_data, ddof=0):.2f}\")\n",
    "print(f\"Std Dev  (sample,     ddof=1): {np.std(clean_data, ddof=1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Probability Basics\n",
    "\n",
    "Probability underpins many ML algorithms, from Naive Bayes to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Conditional Probability\n",
    "\n",
    "The probability of event $A$ **given** that event $B$ has occurred:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "### 3.2 Bayes' Theorem\n",
    "\n",
    "Bayes' theorem lets us \"flip\" a conditional probability:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "- $P(A \\mid B)$ = **posterior** -- updated belief about $A$ after observing $B$.\n",
    "- $P(B \\mid A)$ = **likelihood** -- how probable the evidence is, given $A$.\n",
    "- $P(A)$ = **prior** -- our initial belief about $A$.\n",
    "- $P(B)$ = **marginal likelihood** -- total probability of $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Medical test\n",
    "# - 1% of the population has a disease        P(D)   = 0.01\n",
    "# - Test correctly detects disease 95% of time P(+|D) = 0.95  (sensitivity)\n",
    "# - Test false-positive rate is 5%             P(+|~D)= 0.05\n",
    "#\n",
    "# Question: If the test is positive, what is P(D|+)?\n",
    "\n",
    "P_D     = 0.01                          # prior\n",
    "P_pos_D = 0.95                          # likelihood (sensitivity)\n",
    "P_pos_notD = 0.05                       # false positive rate\n",
    "P_notD  = 1 - P_D\n",
    "\n",
    "# Total probability of testing positive\n",
    "P_pos = P_pos_D * P_D + P_pos_notD * P_notD\n",
    "\n",
    "# Bayes' theorem\n",
    "P_D_pos = (P_pos_D * P_D) / P_pos\n",
    "\n",
    "print(f\"P(Disease | Positive test) = {P_D_pos:.4f}  ({P_D_pos*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Even with a 95% sensitive test, a positive result only means\")\n",
    "print(f\"a ~{P_D_pos*100:.0f}% chance of actually having the disease when\")\n",
    "print(\"the disease is rare (1% prevalence). This is the base-rate fallacy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Linear Algebra Essentials\n",
    "\n",
    "Machine learning is, at its core, applied linear algebra. Here we cover the bare essentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Vectors\n",
    "\n",
    "A vector is an ordered list of numbers. In ML, a single data sample is often represented as a vector:\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors in NumPy\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(\"v1        :\", v1)\n",
    "print(\"v2        :\", v2)\n",
    "print(\"v1 + v2   :\", v1 + v2)\n",
    "print(\"3 * v1    :\", 3 * v1)\n",
    "print(\"Magnitude :\", np.linalg.norm(v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dot Product\n",
    "\n",
    "The dot product of two vectors $\\mathbf{a}$ and $\\mathbf{b}$:\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i \\, b_i$$\n",
    "\n",
    "The dot product measures how \"aligned\" two vectors are. It is central to linear regression ($\\hat{y} = \\mathbf{w} \\cdot \\mathbf{x} + b$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = np.dot(v1, v2)\n",
    "print(f\"v1 . v2 = {v1[0]}*{v2[0]} + {v1[1]}*{v2[1]} + {v1[2]}*{v2[2]} = {dot}\")\n",
    "\n",
    "# Equivalent ways to compute the dot product\n",
    "print(\"np.dot     :\", np.dot(v1, v2))\n",
    "print(\"v1 @ v2    :\", v1 @ v2)\n",
    "print(\"sum(v1*v2) :\", np.sum(v1 * v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Matrix Multiplication\n",
    "\n",
    "For matrices $\\mathbf{A}$ of shape $(m \\times n)$ and $\\mathbf{B}$ of shape $(n \\times p)$, the product $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ has shape $(m \\times p)$:\n",
    "\n",
    "$$C_{ij} = \\sum_{k=1}^{n} A_{ik} \\, B_{kj}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])   # 3x2\n",
    "\n",
    "B = np.array([[7, 8, 9],\n",
    "              [10, 11, 12]])  # 2x3\n",
    "\n",
    "C = A @ B                     # 3x3\n",
    "print(\"A (3x2):\")\n",
    "print(A)\n",
    "print(\"\\nB (2x3):\")\n",
    "print(B)\n",
    "print(\"\\nC = A @ B (3x3):\")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Calculus Intuition\n",
    "\n",
    "You do not need to be a calculus expert for this course, but understanding two ideas will help you grasp how ML models learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The Derivative as Slope\n",
    "\n",
    "The derivative of a function $f(x)$ at a point tells you the **slope** (rate of change) of $f$ at that point.\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "- If $f'(x) > 0$, the function is increasing at $x$.\n",
    "- If $f'(x) < 0$, the function is decreasing at $x$.\n",
    "- If $f'(x) = 0$, we are at a local minimum, maximum, or saddle point.\n",
    "\n",
    "### 5.2 Gradients and Gradient Descent\n",
    "\n",
    "When a function has **multiple inputs** (e.g., model weights $w_1, w_2, \\ldots$), the **gradient** is the vector of all partial derivatives:\n",
    "\n",
    "$$\\nabla f(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\frac{\\partial f}{\\partial w_2} \\\\ \\vdots \\end{bmatrix}$$\n",
    "\n",
    "The gradient points in the direction of **steepest ascent**. To minimize a loss function, we move in the **opposite** direction:\n",
    "\n",
    "$$\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\, \\nabla f(\\mathbf{w}_{\\text{old}})$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**.\n",
    "\n",
    "This is the essence of **gradient descent** -- the optimization engine behind most ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize gradient descent on a simple quadratic f(x) = (x - 3)^2\n",
    "def f(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Gradient descent\n",
    "x_val = -1.0           # starting point\n",
    "lr = 0.1               # learning rate\n",
    "history = [x_val]\n",
    "\n",
    "for _ in range(20):\n",
    "    x_val = x_val - lr * f_prime(x_val)\n",
    "    history.append(x_val)\n",
    "\n",
    "# Plot\n",
    "x_range = np.linspace(-2, 7, 200)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(x_range, f(x_range), label=\"$f(x) = (x-3)^2$\")\n",
    "ax.scatter(history, [f(h) for h in history], color=\"red\", zorder=5, label=\"GD steps\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.set_title(\"Gradient Descent on a Simple Quadratic\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After 20 steps, x = {history[-1]:.6f}  (minimum is at x = 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualization Basics\n",
    "\n",
    "Good visualizations are essential for understanding data and communicating results. Here are the four most common plot types you will use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate some sample data\n",
    "x = np.linspace(0, 10, 50)\n",
    "y = 2 * x + 1 + np.random.normal(0, 2, size=50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# --- Scatter plot ---\n",
    "axes[0, 0].scatter(x, y, alpha=0.7, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0, 0].set_title(\"Scatter Plot\")\n",
    "axes[0, 0].set_xlabel(\"x\")\n",
    "axes[0, 0].set_ylabel(\"y\")\n",
    "\n",
    "# --- Line plot ---\n",
    "axes[0, 1].plot(x, 2 * x + 1, color=\"blue\", label=\"y = 2x + 1\")\n",
    "axes[0, 1].plot(x, 0.5 * x + 5, color=\"orange\", linestyle=\"--\", label=\"y = 0.5x + 5\")\n",
    "axes[0, 1].set_title(\"Line Plot\")\n",
    "axes[0, 1].set_xlabel(\"x\")\n",
    "axes[0, 1].set_ylabel(\"y\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# --- Histogram ---\n",
    "samples = np.random.normal(loc=5, scale=2, size=500)\n",
    "axes[1, 0].hist(samples, bins=25, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1, 0].set_title(\"Histogram\")\n",
    "axes[1, 0].set_xlabel(\"Value\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# --- Bar chart ---\n",
    "categories = [\"A\", \"B\", \"C\", \"D\"]\n",
    "values = [23, 45, 12, 37]\n",
    "axes[1, 1].bar(categories, values, color=[\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\"])\n",
    "axes[1, 1].set_title(\"Bar Chart\")\n",
    "axes[1, 1].set_xlabel(\"Category\")\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.suptitle(\"Common Plot Types in ML\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Matplotlib Tips\n",
    "\n",
    "- Use `plt.subplots(rows, cols)` to create multi-panel figures.\n",
    "- Always label your axes with `set_xlabel` / `set_ylabel`.\n",
    "- Add a legend when plotting multiple series.\n",
    "- Use `plt.tight_layout()` to avoid overlapping labels.\n",
    "- Save figures with `plt.savefig(\"filename.png\", dpi=150, bbox_inches=\"tight\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Next Steps\n",
    "\n",
    "This notebook covered the essential prerequisites for the Machine Learning Masterclass:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|--------------|\n",
    "| **NumPy** | Arrays, element-wise ops, indexing, reshaping |\n",
    "| **pandas** | DataFrames, selection, filtering, `.describe()` |\n",
    "| **Statistics** | Mean, median, variance, standard deviation |\n",
    "| **Probability** | Conditional probability, Bayes' theorem |\n",
    "| **Linear Algebra** | Vectors, dot product, matrix multiplication |\n",
    "| **Calculus** | Derivative = slope; gradient descent minimizes loss |\n",
    "| **Visualization** | Scatter, line, histogram, subplots with matplotlib |\n",
    "\n",
    "**Next up:** Start with **Module ML100 -- Data Splitting & Feature Fundamentals** to begin building your ML toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}