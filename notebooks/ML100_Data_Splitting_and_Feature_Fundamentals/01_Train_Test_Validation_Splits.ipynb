{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Train, Test, and Validation Splits\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain **why** we split data into training and testing sets\n",
    "- Use `sklearn.model_selection.train_test_split` with common ratios\n",
    "- Apply **stratified splitting** to preserve class distributions\n",
    "- Implement a **3-way split** (train / validation / test)\n",
    "- Understand **GroupKFold** and when group-aware splitting is necessary\n",
    "- Recognize common mistakes that lead to data leakage or biased evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python (lists, functions, f-strings)\n",
    "- Familiarity with NumPy arrays and Pandas DataFrames\n",
    "- High-level understanding of classification vs. regression\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Split Data?](#1-why-split-data)\n",
    "2. [Train/Test Split with scikit-learn](#2-traintest-split-with-scikit-learn)\n",
    "3. [Stratified Splits for Classification](#3-stratified-splits-for-classification)\n",
    "4. [Three-Way Split: Train / Validation / Test](#4-three-way-split-train--validation--test)\n",
    "5. [GroupKFold: Preventing Group Leakage](#5-groupkfold-preventing-group-leakage)\n",
    "6. [Visualizing Split Sizes](#6-visualizing-split-sizes)\n",
    "7. [Common Mistakes](#7-common-mistakes)\n",
    "8. [Exercise](#8-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Split Data?\n",
    "\n",
    "The fundamental goal in machine learning is **generalization** -- performing well on *unseen* data, not just the data the model was trained on.\n",
    "\n",
    "- **Overfitting**: a model memorizes training noise instead of learning the true pattern\n",
    "- **Underfitting**: a model is too simple to capture the underlying relationship\n",
    "- We need a **held-out test set** that the model never sees during training so we can get an honest estimate of real-world performance\n",
    "\n",
    "Mathematically, we want to minimize the **expected prediction error** on new data:\n",
    "\n",
    "$$\n",
    "\\text{EPE} = \\mathbb{E}\\left[L\\big(y, \\hat{f}(x)\\big)\\right]\n",
    "$$\n",
    "\n",
    "where $L$ is a loss function (e.g., squared error, log-loss). Evaluating on training data gives an **optimistically biased** estimate of EPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Train/Test Split with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],   # imbalanced: 70% class-0, 30% class-1\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Full dataset  -> X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical ratios\n",
    "\n",
    "| Split | Small data (< 10 k) | Medium data | Large data (> 100 k) |\n",
    "|-------|---------------------|-------------|----------------------|\n",
    "| Train | 70-80 % | 80-90 % | 95-99 % |\n",
    "| Test  | 20-30 % | 10-20 % | 1-5 % |\n",
    "\n",
    "Key parameter: **`random_state`** -- set it to get reproducible splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set -> {X_train.shape[0]} samples ({X_train.shape[0]/len(y)*100:.0f}%)\")\n",
    "print(f\"Test  set -> {X_test.shape[0]} samples ({X_test.shape[0]/len(y)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 split for comparison\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"70/30 -> Train: {X_train_70.shape[0]}, Test: {X_test_30.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Stratified Splits for Classification\n",
    "\n",
    "When classes are **imbalanced**, a random split can give a test set with a very different class ratio than the full dataset. The `stratify` parameter ensures each split has approximately the same class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Non-stratified split ---\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# --- Stratified split ---\n",
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "def class_pct(arr):\n",
    "    counts = np.bincount(arr)\n",
    "    return counts / counts.sum() * 100\n",
    "\n",
    "print(\"Full dataset class %:     \", np.round(class_pct(y), 2))\n",
    "print(\"Non-stratified test %:    \", np.round(class_pct(y_te), 2))\n",
    "print(\"Stratified test %:        \", np.round(class_pct(y_te_s), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the stratified test set preserves the original 70/30 class ratio much more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Three-Way Split: Train / Validation / Test\n",
    "\n",
    "When you need to **tune hyperparameters**, a single train/test split is not enough:\n",
    "\n",
    "- **Training set** -- fit the model\n",
    "- **Validation set** -- choose hyperparameters / compare models\n",
    "- **Test set** -- final, unbiased evaluation (touch **once**)\n",
    "\n",
    "A common ratio is **60 / 20 / 20** or **70 / 15 / 15**.\n",
    "\n",
    "We achieve this by calling `train_test_split` **twice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: separate out the test set (20%)\n",
    "X_temp, X_test_3, y_temp, y_test_3 = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: split the remaining 80% into train (75% of 80% = 60%) and val (25% of 80% = 20%)\n",
    "X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "total = len(y)\n",
    "print(f\"Train:      {len(y_train_3)} samples  ({len(y_train_3)/total*100:.0f}%)\")\n",
    "print(f\"Validation: {len(y_val_3)} samples  ({len(y_val_3)/total*100:.0f}%)\")\n",
    "print(f\"Test:       {len(y_test_3)} samples  ({len(y_test_3)/total*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. GroupKFold: Preventing Group Leakage\n",
    "\n",
    "Sometimes rows are not independent. For example:\n",
    "\n",
    "- Multiple medical images from the **same patient**\n",
    "- Multiple transactions from the **same user**\n",
    "- Multiple frames from the **same video**\n",
    "\n",
    "If the same group appears in both train *and* test, the model can \"cheat\" by memorizing group-specific patterns. **GroupKFold** ensures that all samples from a given group stay together in the same fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Simulate 200 samples from 20 users (10 samples each)\n",
    "n_users = 20\n",
    "samples_per_user = 10\n",
    "n_total = n_users * samples_per_user\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "groups = np.repeat(np.arange(n_users), samples_per_user)\n",
    "X_grp = rng.randn(n_total, 5)\n",
    "y_grp = rng.randint(0, 2, n_total)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(gkf.split(X_grp, y_grp, groups)):\n",
    "    train_groups = set(groups[train_idx])\n",
    "    test_groups = set(groups[test_idx])\n",
    "    overlap = train_groups & test_groups\n",
    "    print(\n",
    "        f\"Fold {fold}: train groups {sorted(train_groups)}, \"\n",
    "        f\"test groups {sorted(test_groups)}, overlap: {overlap}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero overlap means no group leakage -- exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualizing Split Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# --- Bar chart: split sizes ---\n",
    "labels = [\"Train\", \"Validation\", \"Test\"]\n",
    "sizes = [len(y_train_3), len(y_val_3), len(y_test_3)]\n",
    "colors = [\"#2196F3\", \"#FF9800\", \"#4CAF50\"]\n",
    "\n",
    "axes[0].bar(labels, sizes, color=colors, edgecolor=\"black\")\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[0].text(i, v + 10, str(v), ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Number of samples\")\n",
    "axes[0].set_title(\"3-Way Split Sizes (60 / 20 / 20)\")\n",
    "\n",
    "# --- Bar chart: class distribution per split ---\n",
    "splits = {\n",
    "    \"Full\": y,\n",
    "    \"Train\": y_train_3,\n",
    "    \"Val\": y_val_3,\n",
    "    \"Test\": y_test_3,\n",
    "}\n",
    "x_pos = np.arange(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "class0_pcts = [np.mean(v == 0) * 100 for v in splits.values()]\n",
    "class1_pcts = [np.mean(v == 1) * 100 for v in splits.values()]\n",
    "\n",
    "axes[1].bar(x_pos - width / 2, class0_pcts, width, label=\"Class 0\", color=\"#90CAF9\")\n",
    "axes[1].bar(x_pos + width / 2, class1_pcts, width, label=\"Class 1\", color=\"#EF9A9A\")\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(splits.keys())\n",
    "axes[1].set_ylabel(\"Percentage (%)\")\n",
    "axes[1].set_title(\"Class Distribution Across Splits (Stratified)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Common Mistakes\n",
    "\n",
    "| Mistake | Why it is harmful | Fix |\n",
    "|---------|-------------------|-----|\n",
    "| **Fitting / evaluating on test data** | Gives an overoptimistic estimate of generalization | Keep the test set locked away until final evaluation |\n",
    "| **Not stratifying imbalanced data** | Splits can have very different class ratios, making metrics unreliable | Use `stratify=y` in `train_test_split` |\n",
    "| **Ignoring group structure** | Leaks information between train/test when rows are not independent | Use `GroupKFold` or `GroupShuffleSplit` |\n",
    "| **Using a fixed split on tiny datasets** | High variance in performance estimate | Use cross-validation (next notebook) |\n",
    "| **Preprocessing before splitting** | Leaks statistics from test set into training | Always split first, then preprocess (Notebook 03) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercise\n",
    "\n",
    "**Task**: Using the `make_classification` dataset created above, perform the following:\n",
    "\n",
    "1. Create a **90 / 10** stratified train/test split.\n",
    "2. Print the class distribution (as percentages) for both the train and test sets.\n",
    "3. Verify that the distributions are nearly identical.\n",
    "\n",
    "*Hint*: Use `train_test_split` with `test_size=0.10`, `stratify=y`, and `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ----------------------------------------------------------------\n",
    "# X_train_ex, X_test_ex, y_train_ex, y_test_ex = ...\n",
    "# print class distributions\n",
    "# ----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}