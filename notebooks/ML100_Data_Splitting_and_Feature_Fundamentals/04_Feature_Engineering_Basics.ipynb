{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Feature Engineering Basics\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to predictive models, resulting in improved accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Understand what feature engineering is and why it matters\n",
    "- Create interaction features using polynomial expansion and manual multiplication\n",
    "- Apply log transforms to handle skewed distributions\n",
    "- Bin continuous features using `pd.cut` and `pd.qcut`\n",
    "- Extract useful features from datetime columns\n",
    "- Derive basic text features (word count, string length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python fundamentals (lists, dictionaries, functions)\n",
    "- NumPy and Pandas basics\n",
    "- Basic understanding of train/test splits (Notebooks 01-03)\n",
    "- Familiarity with Matplotlib for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [What Is Feature Engineering?](#1-what-is-feature-engineering)\n",
    "2. [Interaction Features](#2-interaction-features)\n",
    "3. [Log Transforms for Skewed Data](#3-log-transforms-for-skewed-data)\n",
    "4. [Binning Continuous Features](#4-binning-continuous-features)\n",
    "5. [Date-Time Feature Extraction](#5-date-time-feature-extraction)\n",
    "6. [Text Feature Basics](#6-text-feature-basics)\n",
    "7. [Putting It All Together](#7-putting-it-all-together)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. What Is Feature Engineering?\n",
    "\n",
    "**Feature engineering** is the art and science of creating new input variables from existing data to improve model performance.\n",
    "\n",
    "**Why it matters:**\n",
    "- Raw data is rarely in the optimal format for a model\n",
    "- Good features can make simple models outperform complex ones\n",
    "- Domain knowledge encoded as features gives models a head start\n",
    "- It is often the single most impactful step in a ML pipeline\n",
    "\n",
    "**Types of feature engineering:**\n",
    "- **Interaction features** - combining two or more features\n",
    "- **Transformations** - log, sqrt, Box-Cox to fix distributions\n",
    "- **Binning** - converting continuous values to categories\n",
    "- **Temporal extraction** - pulling year, month, day from dates\n",
    "- **Text extraction** - word counts, lengths, pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Interaction Features\n",
    "\n",
    "Interaction features capture relationships **between** existing features that the model might not discover on its own (especially for linear models).\n",
    "\n",
    "For two features $x_1$ and $x_2$, interactions include:\n",
    "- $x_1 \\times x_2$ (product)\n",
    "- $x_1^2$, $x_2^2$ (polynomial terms)\n",
    "- $x_1^2 \\times x_2$, $x_1 \\times x_2^2$ (higher-order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Manual Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small dataset: house features\n",
    "df_house = pd.DataFrame({\n",
    "    'length': [30, 40, 35, 50, 45],\n",
    "    'width':  [20, 25, 22, 30, 28],\n",
    "    'floors': [1, 2, 1, 3, 2]\n",
    "})\n",
    "\n",
    "# Manual interaction: area = length * width\n",
    "df_house['area'] = df_house['length'] * df_house['width']\n",
    "\n",
    "# Manual interaction: total_living_space = area * floors\n",
    "df_house['total_living_space'] = df_house['area'] * df_house['floors']\n",
    "\n",
    "# Ratio feature: aspect_ratio = length / width\n",
    "df_house['aspect_ratio'] = df_house['length'] / df_house['width']\n",
    "\n",
    "print(\"House dataset with engineered features:\")\n",
    "df_house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Polynomial Features with sklearn\n",
    "\n",
    "`PolynomialFeatures` automatically generates all polynomial combinations up to a specified degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original features\n",
    "X = df_house[['length', 'width']].values\n",
    "print(\"Original features shape:\", X.shape)\n",
    "print(\"Original features (first 3 rows):\")\n",
    "print(X[:3])\n",
    "print()\n",
    "\n",
    "# Degree 2 polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(\"Polynomial features shape:\", X_poly.shape)\n",
    "print(\"Feature names:\", poly.get_feature_names_out())\n",
    "print()\n",
    "print(\"Polynomial features (first 3 rows):\")\n",
    "print(X_poly[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction_only=True: only cross-products, no powers\n",
    "poly_interact = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interact = poly_interact.fit_transform(X)\n",
    "\n",
    "print(\"Interaction-only features:\", poly_interact.get_feature_names_out())\n",
    "print(\"Shape:\", X_interact.shape)\n",
    "print()\n",
    "print(\"First 3 rows:\")\n",
    "print(X_interact[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Log Transforms for Skewed Data\n",
    "\n",
    "Many real-world features (income, house prices, populations) follow a **right-skewed** distribution. Log transforms can:\n",
    "- Reduce skewness and make distributions more normal\n",
    "- Stabilize variance\n",
    "- Help linear models that assume normally distributed features\n",
    "\n",
    "Common transforms:\n",
    "- $\\log(x)$ - natural log (requires $x > 0$)\n",
    "- $\\log(x + 1)$ - handles zeros (`np.log1p`)\n",
    "- $\\sqrt{x}$ - milder than log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate right-skewed data (simulating income)\n",
    "np.random.seed(42)\n",
    "income = np.random.lognormal(mean=10.5, sigma=0.8, size=2000)\n",
    "\n",
    "print(f\"Original income stats:\")\n",
    "print(f\"  Mean:   ${income.mean():,.0f}\")\n",
    "print(f\"  Median: ${np.median(income):,.0f}\")\n",
    "print(f\"  Skew:   {pd.Series(income).skew():.2f}\")\n",
    "print()\n",
    "\n",
    "# Apply log transform\n",
    "income_log = np.log1p(income)\n",
    "\n",
    "print(f\"Log-transformed stats:\")\n",
    "print(f\"  Mean:   {income_log.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(income_log):.2f}\")\n",
    "print(f\"  Skew:   {pd.Series(income_log).skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Before: original skewed distribution\n",
    "axes[0].hist(income, bins=50, color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(income.mean(), color='red', linestyle='--', label=f'Mean: ${income.mean():,.0f}')\n",
    "axes[0].axvline(np.median(income), color='blue', linestyle='--', label=f'Median: ${np.median(income):,.0f}')\n",
    "axes[0].set_title('Before: Original Income (Right-Skewed)', fontsize=12)\n",
    "axes[0].set_xlabel('Income ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# After: log-transformed distribution\n",
    "axes[1].hist(income_log, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(income_log.mean(), color='red', linestyle='--', label=f'Mean: {income_log.mean():.2f}')\n",
    "axes[1].axvline(np.median(income_log), color='blue', linestyle='--', label=f'Median: {np.median(income_log):.2f}')\n",
    "axes[1].set_title('After: Log-Transformed Income', fontsize=12)\n",
    "axes[1].set_xlabel('log(Income + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- The original distribution has a long right tail (high skew)\n",
    "- After log transform, the distribution is approximately normal (skew near 0)\n",
    "- Mean and median converge after the transform, indicating symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Binning Continuous Features\n",
    "\n",
    "**Binning** (discretization) converts continuous features into categorical ones. This can:\n",
    "- Capture non-linear relationships for linear models\n",
    "- Reduce the impact of outliers\n",
    "- Create interpretable categories (e.g., age groups)\n",
    "\n",
    "Two main approaches:\n",
    "- `pd.cut` - equal-width bins (uniform spacing)\n",
    "- `pd.qcut` - equal-frequency bins (same number of observations per bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data: ages\n",
    "np.random.seed(42)\n",
    "ages = np.random.randint(18, 80, size=100)\n",
    "df_age = pd.DataFrame({'age': ages})\n",
    "\n",
    "# pd.cut: equal-width bins\n",
    "df_age['age_bin_equal_width'] = pd.cut(\n",
    "    df_age['age'], \n",
    "    bins=[0, 25, 35, 50, 65, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '51-65', '66+']\n",
    ")\n",
    "\n",
    "print(\"pd.cut (equal-width bins):\")\n",
    "print(df_age['age_bin_equal_width'].value_counts().sort_index())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.qcut: equal-frequency bins (quantile-based)\n",
    "df_age['age_bin_quantile'] = pd.qcut(\n",
    "    df_age['age'], \n",
    "    q=4,  # 4 bins = quartiles\n",
    "    labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
    ")\n",
    "\n",
    "print(\"pd.qcut (equal-frequency bins, quartiles):\")\n",
    "print(df_age['age_bin_quantile'].value_counts().sort_index())\n",
    "print()\n",
    "print(\"Note: each bin has roughly the same number of observations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both binning strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df_age['age_bin_equal_width'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[0], color='coral', edgecolor='black'\n",
    ")\n",
    "axes[0].set_title('pd.cut: Equal-Width Bins', fontsize=12)\n",
    "axes[0].set_xlabel('Age Group')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "df_age['age_bin_quantile'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[1], color='steelblue', edgecolor='black'\n",
    ")\n",
    "axes[1].set_title('pd.qcut: Equal-Frequency Bins', fontsize=12)\n",
    "axes[1].set_xlabel('Quantile Group')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use each:**\n",
    "- `pd.cut` - when domain-specific boundaries matter (e.g., age groups, tax brackets)\n",
    "- `pd.qcut` - when you want balanced groups regardless of data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Date-Time Feature Extraction\n",
    "\n",
    "Raw datetime values are not directly useful for most models. We extract meaningful components:\n",
    "- **Year, month, day** - capture seasonality and trends\n",
    "- **Day of week** - distinguish weekdays from weekends\n",
    "- **Hour** - time-of-day patterns\n",
    "- **Is weekend** - binary flag for weekend behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datetime data (simulating order timestamps)\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2023-01-01', periods=365, freq='D')\n",
    "df_dates = pd.DataFrame({\n",
    "    'order_date': np.random.choice(dates, size=200),\n",
    "    'amount': np.random.lognormal(mean=4, sigma=0.5, size=200)\n",
    "})\n",
    "\n",
    "# Ensure the column is datetime type\n",
    "df_dates['order_date'] = pd.to_datetime(df_dates['order_date'])\n",
    "\n",
    "print(\"Raw data (first 5 rows):\")\n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime features\n",
    "df_dates['year'] = df_dates['order_date'].dt.year\n",
    "df_dates['month'] = df_dates['order_date'].dt.month\n",
    "df_dates['day'] = df_dates['order_date'].dt.day\n",
    "df_dates['day_of_week'] = df_dates['order_date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_dates['day_name'] = df_dates['order_date'].dt.day_name()\n",
    "df_dates['is_weekend'] = df_dates['day_of_week'].isin([5, 6]).astype(int)\n",
    "df_dates['quarter'] = df_dates['order_date'].dt.quarter\n",
    "\n",
    "print(\"With extracted features:\")\n",
    "df_dates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze: average order amount by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "avg_by_day = df_dates.groupby('day_name')['amount'].mean().reindex(day_order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "colors = ['steelblue'] * 5 + ['coral'] * 2  # weekdays blue, weekends coral\n",
    "avg_by_day.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black')\n",
    "axes[0].set_title('Average Order Amount by Day of Week', fontsize=12)\n",
    "axes[0].set_ylabel('Average Amount ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "monthly = df_dates.groupby('month')['amount'].mean()\n",
    "monthly.plot(kind='line', ax=axes[1], marker='o', color='steelblue')\n",
    "axes[1].set_title('Average Order Amount by Month', fontsize=12)\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Average Amount ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Text Feature Basics\n",
    "\n",
    "Even simple text features can add predictive power. Here are quick teasers (full NLP is a separate topic):\n",
    "- **Word count** - number of words in a text field\n",
    "- **Character length** - total string length\n",
    "- **Contains keyword** - binary flag for specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample product reviews\n",
    "df_text = pd.DataFrame({\n",
    "    'review': [\n",
    "        'Great product, love it!',\n",
    "        'Terrible quality. Broke after one day. Very disappointed with this purchase.',\n",
    "        'OK',\n",
    "        'Amazing value for money. Would definitely recommend to friends and family.',\n",
    "        'Not worth it.',\n",
    "        'Exceeded expectations! The build quality is superb and shipping was fast.'\n",
    "    ],\n",
    "    'rating': [5, 1, 3, 5, 2, 5]\n",
    "})\n",
    "\n",
    "# Extract text features\n",
    "df_text['word_count'] = df_text['review'].str.split().str.len()\n",
    "df_text['char_length'] = df_text['review'].str.len()\n",
    "df_text['has_exclamation'] = df_text['review'].str.contains('!').astype(int)\n",
    "df_text['avg_word_length'] = df_text['char_length'] / df_text['word_count']\n",
    "\n",
    "print(\"Text features:\")\n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These simple features are surprisingly useful. Longer reviews often correlate with stronger opinions (positive or negative), while very short reviews tend to be neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It All Together\n",
    "\n",
    "Let us create a synthetic dataset and apply multiple feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic e-commerce dataset\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "dates = pd.date_range(start='2022-01-01', end='2023-12-31', periods=n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'order_date': dates,\n",
    "    'quantity': np.random.randint(1, 20, size=n),\n",
    "    'unit_price': np.random.lognormal(mean=3, sigma=0.7, size=n),  # skewed!\n",
    "    'customer_age': np.random.randint(18, 75, size=n),\n",
    "})\n",
    "\n",
    "print(\"Original dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nunit_price skewness: {df['unit_price'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply feature engineering ---\n",
    "\n",
    "# 1. Interaction feature: total_revenue\n",
    "df['total_revenue'] = df['quantity'] * df['unit_price']\n",
    "\n",
    "# 2. Log transform of skewed columns\n",
    "df['log_unit_price'] = np.log1p(df['unit_price'])\n",
    "df['log_total_revenue'] = np.log1p(df['total_revenue'])\n",
    "\n",
    "# 3. Binning: age groups\n",
    "df['age_group'] = pd.cut(\n",
    "    df['customer_age'],\n",
    "    bins=[0, 25, 35, 50, 65, 100],\n",
    "    labels=['18-25', '26-35', '36-50', '51-65', '66+']\n",
    ")\n",
    "\n",
    "# 4. Datetime features\n",
    "df['month'] = df['order_date'].dt.month\n",
    "df['day_of_week'] = df['order_date'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['quarter'] = df['order_date'].dt.quarter\n",
    "\n",
    "print(\"Engineered dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nNew shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact of feature engineering\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# unit_price before/after log\n",
    "axes[0, 0].hist(df['unit_price'], bins=40, color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('unit_price (Original - Skewed)', fontsize=11)\n",
    "\n",
    "axes[0, 1].hist(df['log_unit_price'], bins=40, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('log_unit_price (Log-Transformed)', fontsize=11)\n",
    "\n",
    "# Age group distribution\n",
    "df['age_group'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[1, 0], color='mediumpurple', edgecolor='black'\n",
    ")\n",
    "axes[1, 0].set_title('Age Group Distribution', fontsize=11)\n",
    "axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Weekend vs weekday revenue\n",
    "df.groupby('is_weekend')['total_revenue'].mean().plot(\n",
    "    kind='bar', ax=axes[1, 1], color=['steelblue', 'coral'], edgecolor='black'\n",
    ")\n",
    "axes[1, 1].set_title('Avg Revenue: Weekday vs Weekend', fontsize=11)\n",
    "axes[1, 1].set_xticklabels(['Weekday', 'Weekend'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "### Mistake 1: Overfitting with Too Many Engineered Features\n",
    "\n",
    "Creating too many features (especially high-degree polynomials) can lead to overfitting. The model memorizes noise rather than learning signal.\n",
    "\n",
    "**Rule of thumb:** Start simple. Only add features that have a plausible domain reason to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: feature explosion with high-degree polynomials\n",
    "X_small = np.random.randn(100, 5)  # 5 original features\n",
    "\n",
    "for degree in [2, 3, 4, 5]:\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_expanded = poly.fit_transform(X_small)\n",
    "    print(f\"Degree {degree}: {X_small.shape[1]} features -> {X_expanded.shape[1]} features\")\n",
    "\n",
    "print(\"\\nWith 5 features, degree 5 creates 251 features from 100 samples!\")\n",
    "print(\"This is a recipe for overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Not Applying the Same Transforms to Test Data\n",
    "\n",
    "If you engineer features on the training set, you **must** apply the exact same transformations to the test set. Otherwise, the model sees different feature spaces at train and test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split first, then engineer features identically\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'price': np.random.lognormal(5, 1, 200),\n",
    "    'quantity': np.random.randint(1, 50, 200)\n",
    "})\n",
    "y = (data['price'] * data['quantity'] > 5000).astype(int)\n",
    "\n",
    "train_data, test_data, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# CORRECT: Apply same transforms to both\n",
    "def engineer_features(df):\n",
    "    \"\"\"Apply identical feature engineering to any dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['log_price'] = np.log1p(df['price'])\n",
    "    df['total'] = df['price'] * df['quantity']\n",
    "    df['log_total'] = np.log1p(df['total'])\n",
    "    return df\n",
    "\n",
    "train_fe = engineer_features(train_data)\n",
    "test_fe = engineer_features(test_data)\n",
    "\n",
    "print(\"Train columns:\", list(train_fe.columns))\n",
    "print(\"Test columns: \", list(test_fe.columns))\n",
    "print(\"\\nColumns match:\", list(train_fe.columns) == list(test_fe.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Common Mistakes\n",
    "\n",
    "| Mistake | Why It Is Bad | Fix |\n",
    "|---------|---------------|-----|\n",
    "| Too many polynomial features | Overfitting, curse of dimensionality | Keep degree low (2-3), use regularization |\n",
    "| Different transforms on train/test | Model sees inconsistent features | Use a function or Pipeline for transforms |\n",
    "| Log transform without handling zeros | `log(0)` is undefined | Use `np.log1p(x)` instead of `np.log(x)` |\n",
    "| Binning with test-set-derived boundaries | Data leakage | Compute bin edges from train set only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Given the dataset below, engineer at least 5 new features. Then split into train/test and verify that both sets have the same columns.\n",
    "\n",
    "Suggested features to create:\n",
    "1. Log-transform the `salary` column\n",
    "2. Bin `years_experience` into groups (junior, mid, senior, lead)\n",
    "3. Extract `month` and `is_weekend` from `hire_date`\n",
    "4. Create an interaction feature: `salary_per_year_exp` = salary / (years_experience + 1)\n",
    "5. Compute `name_length` from the `name` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter code\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "exercise_df = pd.DataFrame({\n",
    "    'name': [f'Employee_{i}' for i in range(n)],\n",
    "    'salary': np.random.lognormal(mean=11, sigma=0.5, size=n),\n",
    "    'years_experience': np.random.randint(0, 30, size=n),\n",
    "    'hire_date': pd.date_range('2015-01-01', periods=n, freq='5D'),\n",
    "})\n",
    "\n",
    "print(\"Exercise dataset:\")\n",
    "print(exercise_df.head())\n",
    "print(f\"\\nShape: {exercise_df.shape}\")\n",
    "print(f\"salary skewness: {exercise_df['salary'].skew():.2f}\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Log-transform salary\n",
    "# exercise_df['log_salary'] = ...\n",
    "\n",
    "# 2. Bin years_experience\n",
    "# exercise_df['experience_level'] = ...\n",
    "\n",
    "# 3. Extract datetime features\n",
    "# exercise_df['hire_month'] = ...\n",
    "# exercise_df['hire_is_weekend'] = ...\n",
    "\n",
    "# 4. Interaction feature\n",
    "# exercise_df['salary_per_year_exp'] = ...\n",
    "\n",
    "# 5. Text feature\n",
    "# exercise_df['name_length'] = ...\n",
    "\n",
    "# 6. Split and verify columns match\n",
    "# train_df, test_df = train_test_split(exercise_df, test_size=0.2, random_state=42)\n",
    "# assert list(train_df.columns) == list(test_df.columns), \"Column mismatch!\"\n",
    "# print(\"Columns match between train and test.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}