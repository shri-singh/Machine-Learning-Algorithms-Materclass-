{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Scaling, Encoding, and Imputation\n",
    "\n",
    "Before feeding data to a model, we must ensure numeric features are on comparable scales, categorical features are numerically represented, and missing values are handled. This notebook covers the essential preprocessing transformers in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Explain why feature scaling matters for distance-based and gradient-based models\n",
    "- Apply StandardScaler, MinMaxScaler, and RobustScaler and know when to use each\n",
    "- Encode categorical variables with OneHotEncoder and OrdinalEncoder\n",
    "- Impute missing values with SimpleImputer and understand KNNImputer\n",
    "- Correctly use `fit_transform` on training data and `transform` on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python fundamentals (NumPy, Pandas)\n",
    "- Train/test splitting concepts (Notebooks 01-03)\n",
    "- Basic feature engineering concepts (Notebook 04)\n",
    "- Familiarity with Matplotlib for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Why Scaling Matters](#1-why-scaling-matters)\n",
    "2. [StandardScaler](#2-standardscaler)\n",
    "3. [MinMaxScaler](#3-minmaxscaler)\n",
    "4. [RobustScaler](#4-robustscaler)\n",
    "5. [Comparing All Three Scalers](#5-comparing-all-three-scalers)\n",
    "6. [OneHotEncoder](#6-onehotencoder)\n",
    "7. [OrdinalEncoder](#7-ordinalencoder)\n",
    "8. [Imputation: Handling Missing Values](#8-imputation-handling-missing-values)\n",
    "9. [The Golden Rule: fit on Train, transform on Test](#9-the-golden-rule-fit-on-train-transform-on-test)\n",
    "10. [Common Mistakes](#10-common-mistakes)\n",
    "11. [Exercise](#11-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Scaling Matters\n",
    "\n",
    "**Features on different scales cause problems for many algorithms:**\n",
    "\n",
    "- **Distance-based models** (KNN, SVM, K-Means): features with larger ranges dominate distance calculations\n",
    "- **Gradient descent** (Linear/Logistic Regression, Neural Networks): unscaled features create elongated loss surfaces, slowing convergence\n",
    "- **Regularization** (L1/L2 penalties): penalizes coefficients unequally if features have different scales\n",
    "\n",
    "**Models that do NOT need scaling:**\n",
    "- Tree-based models (Decision Trees, Random Forest, Gradient Boosting) - they split on individual feature thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the scale problem\n",
    "df_scale = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 40, 45],           # range: 20 years\n",
    "    'income': [30000, 50000, 70000, 90000, 120000],  # range: 90,000\n",
    "    'rooms': [2, 3, 3, 4, 5]               # range: 3\n",
    "})\n",
    "\n",
    "print(\"Feature ranges before scaling:\")\n",
    "print(f\"  age:    {df_scale['age'].min()} - {df_scale['age'].max()}    (range: {df_scale['age'].max() - df_scale['age'].min()})\")\n",
    "print(f\"  income: {df_scale['income'].min()} - {df_scale['income'].max()} (range: {df_scale['income'].max() - df_scale['income'].min()})\")\n",
    "print(f\"  rooms:  {df_scale['rooms'].min()} - {df_scale['rooms'].max()}      (range: {df_scale['rooms'].max() - df_scale['rooms'].min()})\")\n",
    "print()\n",
    "print(\"Income dominates any distance calculation by a factor of ~30,000x!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. StandardScaler\n",
    "\n",
    "**StandardScaler** (Z-score normalization) transforms each feature to have **mean = 0** and **standard deviation = 1**.\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of the feature\n",
    "- $\\sigma$ = standard deviation of the feature\n",
    "\n",
    "**When to use:**\n",
    "- Features are approximately normally distributed\n",
    "- Default choice for most algorithms\n",
    "- Required for PCA, SVM, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.normal(40, 12, 200),\n",
    "    'income': np.random.normal(60000, 15000, 200),\n",
    "    'score': np.random.normal(75, 10, 200)\n",
    "})\n",
    "\n",
    "print(\"Before StandardScaler:\")\n",
    "print(data.describe().round(2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "data_std = pd.DataFrame(\n",
    "    scaler_std.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(data_std.describe().round(2))\n",
    "print()\n",
    "print(\"Mean ~ 0, Std ~ 1 for all features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. MinMaxScaler\n",
    "\n",
    "**MinMaxScaler** scales features to a given range, typically $[0, 1]$.\n",
    "\n",
    "$$x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "**When to use:**\n",
    "- When you need bounded values (e.g., image pixel values)\n",
    "- Neural networks often prefer inputs in $[0, 1]$\n",
    "- Features do not have extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "scaler_mm = MinMaxScaler()\n",
    "data_mm = pd.DataFrame(\n",
    "    scaler_mm.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(data_mm.describe().round(2))\n",
    "print()\n",
    "print(\"All features now in [0, 1] range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RobustScaler\n",
    "\n",
    "**RobustScaler** uses the **median** and **interquartile range (IQR)** instead of mean and standard deviation, making it robust to outliers.\n",
    "\n",
    "$$x' = \\frac{x - \\text{median}}{IQR}$$\n",
    "\n",
    "Where $IQR = Q3 - Q1$ (75th percentile minus 25th percentile).\n",
    "\n",
    "**When to use:**\n",
    "- Data contains significant outliers\n",
    "- You want to preserve the relative spacing of the majority of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RobustScaler\n",
    "scaler_rob = RobustScaler()\n",
    "data_rob = pd.DataFrame(\n",
    "    scaler_rob.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "print(\"After RobustScaler:\")\n",
    "print(data_rob.describe().round(2))\n",
    "print()\n",
    "print(\"Median ~ 0, IQR ~ 1 for all features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparing All Three Scalers\n",
    "\n",
    "Let us compare how each scaler handles a dataset with **outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data WITH outliers\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(50, 10, 190)\n",
    "outliers = np.array([200, 220, 250, 300, 180, -50, -80, -100, 280, 350])\n",
    "data_with_outliers = np.concatenate([normal_data, outliers]).reshape(-1, 1)\n",
    "\n",
    "print(f\"Data range: [{data_with_outliers.min():.0f}, {data_with_outliers.max():.0f}]\")\n",
    "print(f\"Mean: {data_with_outliers.mean():.1f}, Median: {np.median(data_with_outliers):.1f}\")\n",
    "print(f\"Note: Outliers pull the mean away from the median.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale with all three\n",
    "std_scaled = StandardScaler().fit_transform(data_with_outliers)\n",
    "mm_scaled = MinMaxScaler().fit_transform(data_with_outliers)\n",
    "rob_scaled = RobustScaler().fit_transform(data_with_outliers)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].hist(data_with_outliers, bins=30, color='gray', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Original Data (with outliers)', fontsize=12)\n",
    "axes[0, 0].axvline(data_with_outliers.mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].axvline(np.median(data_with_outliers), color='blue', linestyle='--', label='Median')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# StandardScaler\n",
    "axes[0, 1].hist(std_scaled, bins=30, color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('StandardScaler', fontsize=12)\n",
    "axes[0, 1].axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# MinMaxScaler\n",
    "axes[1, 0].hist(mm_scaled, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('MinMaxScaler', fontsize=12)\n",
    "axes[1, 0].axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# RobustScaler\n",
    "axes[1, 1].hist(rob_scaled, bins=30, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('RobustScaler', fontsize=12)\n",
    "axes[1, 1].axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Scaler Comparison on Data with Outliers', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Scaler': ['StandardScaler', 'MinMaxScaler', 'RobustScaler'],\n",
    "    'Min': [std_scaled.min(), mm_scaled.min(), rob_scaled.min()],\n",
    "    'Max': [std_scaled.max(), mm_scaled.max(), rob_scaled.max()],\n",
    "    'Mean': [std_scaled.mean(), mm_scaled.mean(), rob_scaled.mean()],\n",
    "    'Std': [std_scaled.std(), mm_scaled.std(), rob_scaled.std()]\n",
    "}).round(3)\n",
    "\n",
    "print(\"Scaler comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "print()\n",
    "print(\"Key takeaway: RobustScaler is least affected by outliers.\")\n",
    "print(\"MinMaxScaler squishes most data into a small range due to outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler Selection Guide\n",
    "\n",
    "| Scaler | Formula | Best For | Sensitive to Outliers? |\n",
    "|--------|---------|----------|------------------------|\n",
    "| StandardScaler | $z = \\frac{x - \\mu}{\\sigma}$ | General purpose, normally distributed data | Yes |\n",
    "| MinMaxScaler | $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$ | Bounded ranges, neural networks | Very sensitive |\n",
    "| RobustScaler | $x' = \\frac{x - \\text{median}}{IQR}$ | Data with outliers | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. OneHotEncoder\n",
    "\n",
    "**OneHotEncoder** converts each categorical value into a binary column (0 or 1). This is necessary because most ML models cannot handle string categories.\n",
    "\n",
    "For a feature `color` with values `[red, blue, green]`:\n",
    "\n",
    "| color | color_blue | color_green | color_red |\n",
    "|-------|-----------|------------|----------|\n",
    "| red   | 0 | 0 | 1 |\n",
    "| blue  | 1 | 0 | 0 |\n",
    "| green | 0 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red', 'green'],\n",
    "    'size': ['S', 'M', 'L', 'XL', 'M', 'S']\n",
    "})\n",
    "\n",
    "print(\"Original categorical data:\")\n",
    "print(df_cat)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder: default (keep all categories)\n",
    "ohe = OneHotEncoder(sparse_output=False)  # dense output for readability\n",
    "encoded = ohe.fit_transform(df_cat[['color']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out())\n",
    "print(\"OneHotEncoded (all categories):\")\n",
    "print(encoded_df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder: drop='first' to avoid multicollinearity\n",
    "# With 3 colors, we only need 2 columns (the third is implied)\n",
    "ohe_drop = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_drop = ohe_drop.fit_transform(df_cat[['color']])\n",
    "\n",
    "encoded_drop_df = pd.DataFrame(encoded_drop, columns=ohe_drop.get_feature_names_out())\n",
    "print(\"OneHotEncoded (drop='first'):\")\n",
    "print(encoded_drop_df)\n",
    "print()\n",
    "print(\"Note: 'blue' is the reference category (all zeros = blue).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle unknown categories at test time\n",
    "ohe_safe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe_safe.fit(df_cat[['color']])\n",
    "\n",
    "# Test data has a new category 'yellow' not seen during training\n",
    "test_colors = pd.DataFrame({'color': ['red', 'yellow', 'blue']})\n",
    "encoded_test = ohe_safe.transform(test_colors)\n",
    "\n",
    "print(\"Test data with unknown category 'yellow':\")\n",
    "print(pd.DataFrame(encoded_test, columns=ohe_safe.get_feature_names_out()))\n",
    "print()\n",
    "print(\"'yellow' gets all zeros (handle_unknown='ignore').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vs Dense\n",
    "\n",
    "- `sparse_output=True` (default): memory efficient for high-cardinality features\n",
    "- `sparse_output=False`: returns a regular NumPy array, easier to inspect\n",
    "\n",
    "Use sparse when you have many categories (e.g., zip codes, product IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. OrdinalEncoder\n",
    "\n",
    "**OrdinalEncoder** maps categories to integers. Use this when categories have a **natural order** (ordinal data).\n",
    "\n",
    "Examples of ordinal features:\n",
    "- Education level: high_school < bachelors < masters < phd\n",
    "- T-shirt size: S < M < L < XL\n",
    "- Satisfaction: low < medium < high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding with explicit order\n",
    "df_edu = pd.DataFrame({\n",
    "    'education': ['bachelors', 'high_school', 'phd', 'masters', 'bachelors', 'high_school']\n",
    "})\n",
    "\n",
    "# Define the order explicitly\n",
    "ord_enc = OrdinalEncoder(categories=[['high_school', 'bachelors', 'masters', 'phd']])\n",
    "df_edu['education_encoded'] = ord_enc.fit_transform(df_edu[['education']])\n",
    "\n",
    "print(\"OrdinalEncoder result:\")\n",
    "print(df_edu)\n",
    "print()\n",
    "print(\"Mapping: high_school=0, bachelors=1, masters=2, phd=3\")\n",
    "print(\"The numeric order reflects the educational hierarchy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-shirt sizes: ordinal encoding\n",
    "df_size = pd.DataFrame({'size': ['M', 'S', 'XL', 'L', 'S', 'M', 'XL']})\n",
    "\n",
    "size_enc = OrdinalEncoder(categories=[['S', 'M', 'L', 'XL']])\n",
    "df_size['size_encoded'] = size_enc.fit_transform(df_size[['size']])\n",
    "\n",
    "print(\"Size encoding:\")\n",
    "print(df_size)\n",
    "print()\n",
    "print(\"WARNING: Do NOT use OrdinalEncoder for nominal categories (e.g., color).\")\n",
    "print(\"Encoding red=0, blue=1, green=2 falsely implies blue > red and green > blue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Imputation: Handling Missing Values\n",
    "\n",
    "Real-world data almost always has missing values. **Imputation** fills them in with reasonable estimates.\n",
    "\n",
    "**Strategies:**\n",
    "- **Mean**: replace NaN with column mean (numeric, sensitive to outliers)\n",
    "- **Median**: replace NaN with column median (numeric, robust to outliers)\n",
    "- **Most frequent (mode)**: replace NaN with most common value (works for categorical)\n",
    "- **KNNImputer**: use K-nearest neighbors to impute based on similar rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "np.random.seed(42)\n",
    "df_missing = pd.DataFrame({\n",
    "    'age': [25, np.nan, 35, 40, np.nan, 55, 30, np.nan, 45, 50],\n",
    "    'income': [30000, 45000, np.nan, 80000, 55000, np.nan, 42000, 65000, np.nan, 90000],\n",
    "    'city': ['NYC', 'LA', 'NYC', np.nan, 'Chicago', 'LA', np.nan, 'NYC', 'Chicago', 'LA']\n",
    "})\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(df_missing)\n",
    "print()\n",
    "print(\"Missing value counts:\")\n",
    "print(df_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer: mean strategy (for numeric columns)\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "numeric_cols = ['age', 'income']\n",
    "\n",
    "df_imputed = df_missing.copy()\n",
    "df_imputed[numeric_cols] = imputer_mean.fit_transform(df_missing[numeric_cols])\n",
    "\n",
    "print(\"After mean imputation:\")\n",
    "print(df_imputed[numeric_cols])\n",
    "print(f\"\\nage mean used: {df_missing['age'].mean():.1f}\")\n",
    "print(f\"income mean used: {df_missing['income'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer: median strategy (more robust to outliers)\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_imputed_median = df_missing.copy()\n",
    "df_imputed_median[numeric_cols] = imputer_median.fit_transform(df_missing[numeric_cols])\n",
    "\n",
    "print(\"After median imputation:\")\n",
    "print(df_imputed_median[numeric_cols])\n",
    "print(f\"\\nage median used: {df_missing['age'].median():.1f}\")\n",
    "print(f\"income median used: {df_missing['income'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer: most_frequent strategy (for categorical columns)\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df_imputed_cat = df_missing.copy()\n",
    "df_imputed_cat[['city']] = imputer_mode.fit_transform(df_missing[['city']])\n",
    "\n",
    "print(\"After most_frequent imputation for 'city':\")\n",
    "print(df_imputed_cat[['city']])\n",
    "print(f\"\\nMost frequent city: {df_missing['city'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer: uses similar rows to impute (brief mention)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df_knn = df_missing.copy()\n",
    "df_knn[numeric_cols] = knn_imputer.fit_transform(df_missing[numeric_cols])\n",
    "\n",
    "print(\"After KNNImputer (n_neighbors=3):\")\n",
    "print(df_knn[numeric_cols])\n",
    "print()\n",
    "print(\"Note: KNNImputer considers the values of neighboring rows,\")\n",
    "print(\"so imputed values may differ from simple mean/median.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Strategy Summary\n",
    "\n",
    "| Strategy | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| Mean | Numeric, no outliers | Simple, fast | Distorted by outliers |\n",
    "| Median | Numeric, with outliers | Robust to outliers | Ignores feature relationships |\n",
    "| Most Frequent | Categorical | Works for any dtype | Overrepresents mode |\n",
    "| KNN | Numeric | Uses feature relationships | Slower, needs scaling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. The Golden Rule: fit on Train, transform on Test\n",
    "\n",
    "**This is the single most important rule in preprocessing:**\n",
    "\n",
    "1. `fit_transform(X_train)` - learn parameters (mean, std, categories) from training data AND transform it\n",
    "2. `transform(X_test)` - apply the SAME learned parameters to test data\n",
    "\n",
    "**NEVER call `fit` or `fit_transform` on test data!** Doing so causes **data leakage** - the model indirectly sees test set statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the correct approach\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(50, 15, (100, 2))\n",
    "y = (X[:, 0] + X[:, 1] > 100).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"\\nTrain stats: mean={X_train.mean(axis=0).round(2)}, std={X_train.std(axis=0).round(2)}\")\n",
    "print(f\"Test stats:  mean={X_test.mean(axis=0).round(2)}, std={X_test.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT: fit on train, transform on test\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 1: fit_transform on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Step 2: transform (NOT fit_transform!) on test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"CORRECT approach:\")\n",
    "print(f\"Scaler learned from train: mean={scaler.mean_.round(2)}, std={scaler.scale_.round(2)}\")\n",
    "print(f\"\\nTrain scaled: mean={X_train_scaled.mean(axis=0).round(4)}, std={X_train_scaled.std(axis=0).round(4)}\")\n",
    "print(f\"Test scaled:  mean={X_test_scaled.mean(axis=0).round(4)}, std={X_test_scaled.std(axis=0).round(4)}\")\n",
    "print()\n",
    "print(\"Note: Test mean/std are NOT exactly 0/1 because the scaler used TRAIN statistics.\")\n",
    "print(\"This is correct behavior!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: fitting scaler on test data separately\n",
    "scaler_wrong = StandardScaler()\n",
    "X_train_wrong = scaler_wrong.fit_transform(X_train)\n",
    "\n",
    "scaler_wrong2 = StandardScaler()\n",
    "X_test_wrong = scaler_wrong2.fit_transform(X_test)  # BAD! Fitting on test data\n",
    "\n",
    "print(\"WRONG approach (fit on test):\")\n",
    "print(f\"Train scaler mean: {scaler_wrong.mean_.round(2)}\")\n",
    "print(f\"Test scaler mean:  {scaler_wrong2.mean_.round(2)}  <-- different!\")\n",
    "print()\n",
    "print(\"PROBLEM: Train and test use DIFFERENT scaling parameters.\")\n",
    "print(\"This is data leakage and leads to inconsistent predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same rule applies to imputers and encoders\n",
    "print(\"The fit/transform rule applies to ALL preprocessors:\")\n",
    "print()\n",
    "print(\"  scaler.fit_transform(X_train)   -> scaler.transform(X_test)\")\n",
    "print(\"  imputer.fit_transform(X_train)  -> imputer.transform(X_test)\")\n",
    "print(\"  encoder.fit_transform(X_train)  -> encoder.transform(X_test)\")\n",
    "print()\n",
    "print(\"NEVER call fit() or fit_transform() on test data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Common Mistakes\n",
    "\n",
    "### Mistake 1: Fitting the Scaler on the Entire Dataset\n",
    "\n",
    "If you scale before splitting, the scaler learns statistics from **all** data, including the test set. This is subtle but real data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Scale first, then split\n",
    "np.random.seed(42)\n",
    "X_all = np.random.normal(50, 15, (100, 2))\n",
    "\n",
    "# BAD: scaler sees all data including future test data\n",
    "scaler_leak = StandardScaler()\n",
    "X_all_scaled = scaler_leak.fit_transform(X_all)  # leakage!\n",
    "X_tr, X_te = train_test_split(X_all_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"WRONG (scale then split):\")\n",
    "print(f\"  Scaler saw all {len(X_all)} samples (including test data).\")\n",
    "print()\n",
    "\n",
    "# CORRECT: Split first, then scale\n",
    "X_tr2, X_te2 = train_test_split(X_all, test_size=0.2, random_state=42)\n",
    "scaler_correct = StandardScaler()\n",
    "X_tr2_scaled = scaler_correct.fit_transform(X_tr2)\n",
    "X_te2_scaled = scaler_correct.transform(X_te2)\n",
    "\n",
    "print(\"CORRECT (split then scale):\")\n",
    "print(f\"  Scaler saw only {len(X_tr2)} training samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Using pd.get_dummies in Production\n",
    "\n",
    "`pd.get_dummies` is convenient for exploration but dangerous in production:\n",
    "- It creates columns based on the categories **present in that specific data**\n",
    "- If a category is missing from test data, the column is missing\n",
    "- If a new category appears, an unexpected column is added\n",
    "\n",
    "**Always use `OneHotEncoder`** - it remembers the categories from `fit` and handles unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the get_dummies problem\n",
    "train_cat = pd.DataFrame({'color': ['red', 'blue', 'green', 'red']})\n",
    "test_cat = pd.DataFrame({'color': ['blue', 'red']})  # no 'green' in test\n",
    "\n",
    "# pd.get_dummies: different columns!\n",
    "train_dummies = pd.get_dummies(train_cat)\n",
    "test_dummies = pd.get_dummies(test_cat)\n",
    "\n",
    "print(\"pd.get_dummies (PROBLEMATIC):\")\n",
    "print(f\"  Train columns: {list(train_dummies.columns)}\")\n",
    "print(f\"  Test columns:  {list(test_dummies.columns)}\")\n",
    "print(f\"  Column mismatch: 'color_green' missing from test!\")\n",
    "print()\n",
    "\n",
    "# OneHotEncoder: consistent columns\n",
    "ohe_prod = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "train_ohe = ohe_prod.fit_transform(train_cat)\n",
    "test_ohe = ohe_prod.transform(test_cat)\n",
    "\n",
    "cols = ohe_prod.get_feature_names_out()\n",
    "print(\"OneHotEncoder (CORRECT):\")\n",
    "print(f\"  Train columns: {list(cols)} -> shape {train_ohe.shape}\")\n",
    "print(f\"  Test columns:  {list(cols)} -> shape {test_ohe.shape}\")\n",
    "print(f\"  Columns always match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Common Mistakes\n",
    "\n",
    "| Mistake | Consequence | Fix |\n",
    "|---------|-------------|-----|\n",
    "| `fit_transform` on test data | Data leakage, optimistic metrics | Use `transform` only on test |\n",
    "| Scale before splitting | Leakage through test statistics | Split first, then scale |\n",
    "| `pd.get_dummies` in production | Column mismatch at inference | Use `OneHotEncoder` |\n",
    "| Ignoring missing values | Model crashes or silent errors | Use `SimpleImputer`/`KNNImputer` |\n",
    "| Ordinal encoding nominal data | False ordering imposed | Use `OneHotEncoder` for nominal |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exercise\n",
    "\n",
    "**Task:** You are given a dataset with numeric features, a categorical feature, and missing values. Perform all preprocessing steps correctly.\n",
    "\n",
    "Steps:\n",
    "1. Split the data into train (80%) and test (20%) with `random_state=42`\n",
    "2. Impute missing numeric values using median strategy (fit on train, transform on test)\n",
    "3. Scale numeric features with `StandardScaler` (fit on train, transform on test)\n",
    "4. Encode the categorical feature with `OneHotEncoder` (fit on train, transform on test)\n",
    "5. Verify that train and test have the same number of columns after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter code\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "exercise_data = pd.DataFrame({\n",
    "    'age': np.where(np.random.random(n) < 0.1, np.nan, np.random.normal(40, 12, n)),\n",
    "    'income': np.where(np.random.random(n) < 0.15, np.nan, np.random.normal(55000, 15000, n)),\n",
    "    'score': np.random.normal(70, 10, n),\n",
    "    'department': np.random.choice(['engineering', 'marketing', 'sales', 'hr'], n)\n",
    "})\n",
    "exercise_y = (exercise_data['score'] > 70).astype(int)\n",
    "\n",
    "print(\"Exercise dataset:\")\n",
    "print(exercise_data.head(10))\n",
    "print(f\"\\nShape: {exercise_data.shape}\")\n",
    "print(f\"Missing values:\\n{exercise_data.isnull().sum()}\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Step 1: Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# Step 2: Impute numeric columns (fit on train, transform on test)\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\n",
    "# X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n",
    "\n",
    "# Step 3: Scale numeric columns (fit on train, transform on test)\n",
    "# scaler = StandardScaler()\n",
    "# X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "# X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Step 4: Encode categorical column (fit on train, transform on test)\n",
    "# ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "# ...\n",
    "\n",
    "# Step 5: Verify shapes match\n",
    "# print(f\"Train shape: {X_train_final.shape}\")\n",
    "# print(f\"Test shape:  {X_test_final.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}