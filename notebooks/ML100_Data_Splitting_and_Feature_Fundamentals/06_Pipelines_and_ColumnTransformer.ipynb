{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Pipelines and ColumnTransformer\n",
    "\n",
    "Pipelines bundle preprocessing and modeling into a single object, preventing data leakage and simplifying deployment. This notebook shows how to build production-ready ML workflows with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Explain why pipelines prevent data leakage and simplify workflows\n",
    "- Build sklearn `Pipeline` objects that chain preprocessors and models\n",
    "- Use `ColumnTransformer` to apply different transforms to numeric vs categorical columns\n",
    "- Combine `ColumnTransformer` + `LogisticRegression` into a single pipeline\n",
    "- Use `make_column_selector` for automatic column selection\n",
    "- Integrate pipelines with `cross_val_score`\n",
    "- Compare manual preprocessing vs pipeline approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Train/test splitting (Notebooks 01-03)\n",
    "- Feature engineering basics (Notebook 04)\n",
    "- Scaling, encoding, and imputation (Notebook 05)\n",
    "- Basic understanding of Logistic Regression (used as the model in our pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Why Pipelines?](#1-why-pipelines)\n",
    "2. [sklearn Pipeline Basics](#2-sklearn-pipeline-basics)\n",
    "3. [ColumnTransformer: Different Transforms for Different Columns](#3-columntransformer-different-transforms-for-different-columns)\n",
    "4. [Full Pipeline: ColumnTransformer + LogisticRegression](#4-full-pipeline-columntransformer--logisticregression)\n",
    "5. [Using make_column_selector](#5-using-make_column_selector)\n",
    "6. [Pipelines with cross_val_score](#6-pipelines-with-cross_val_score)\n",
    "7. [Manual Preprocessing vs Pipeline: A Comparison](#7-manual-preprocessing-vs-pipeline-a-comparison)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Pipelines?\n",
    "\n",
    "**Without pipelines**, a typical preprocessing workflow looks like this:\n",
    "\n",
    "```python\n",
    "# Manual approach - error-prone\n",
    "imputer = SimpleImputer()\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "**Problems with the manual approach:**\n",
    "- Easy to accidentally call `fit_transform` on test data\n",
    "- Multiple objects to manage and serialize for production\n",
    "- Cannot be used directly with `cross_val_score` (leakage within folds)\n",
    "- Code becomes a tangled mess with many preprocessing steps\n",
    "\n",
    "**With pipelines:**\n",
    "- Single object: `pipeline.fit(X_train, y_train)` and `pipeline.predict(X_test)`\n",
    "- Leakage is impossible - each fold refits automatically\n",
    "- Easy to serialize (`joblib.dump(pipeline, 'model.pkl')`)\n",
    "- Clean, readable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. sklearn Pipeline Basics\n",
    "\n",
    "A `Pipeline` chains multiple steps sequentially. Each step (except the last) must be a **transformer** (has `fit` and `transform`). The last step can be a transformer or an **estimator** (has `fit` and `predict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple numeric dataset\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X_num = pd.DataFrame({\n",
    "    'feature1': np.where(np.random.random(n) < 0.1, np.nan, np.random.normal(50, 15, n)),\n",
    "    'feature2': np.where(np.random.random(n) < 0.1, np.nan, np.random.normal(100, 30, n)),\n",
    "})\n",
    "y_num = (X_num['feature1'].fillna(50) + X_num['feature2'].fillna(100) > 155).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_num, y_num, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Missing values in train:\\n{X_train.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Pipeline: impute -> scale -> model\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# One call does it all: impute, scale, fit the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# One call: impute, scale, predict (using train-fitted parameters)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Pipeline accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nPipeline steps: {[step[0] for step in pipe.steps]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual steps\n",
    "print(\"Imputer statistics (medians learned from train):\")\n",
    "print(f\"  {pipe.named_steps['imputer'].statistics_}\")\n",
    "print()\n",
    "print(\"Scaler parameters (learned from imputed train):\")\n",
    "print(f\"  Mean: {pipe.named_steps['scaler'].mean_.round(2)}\")\n",
    "print(f\"  Std:  {pipe.named_steps['scaler'].scale_.round(2)}\")\n",
    "print()\n",
    "print(\"Model coefficients:\")\n",
    "print(f\"  {pipe.named_steps['model'].coef_.round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: make_pipeline (auto-generates step names)\n",
    "pipe_short = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=42)\n",
    ")\n",
    "\n",
    "pipe_short.fit(X_train, y_train)\n",
    "print(f\"make_pipeline accuracy: {pipe_short.score(X_test, y_test):.4f}\")\n",
    "print(f\"Auto-named steps: {[step[0] for step in pipe_short.steps]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ColumnTransformer: Different Transforms for Different Columns\n",
    "\n",
    "Real datasets have **mixed types**: numeric columns need scaling, categorical columns need encoding. `ColumnTransformer` applies different transformers to different column subsets in parallel.\n",
    "\n",
    "```\n",
    "ColumnTransformer\n",
    "  |-- numeric_cols  -> Imputer -> Scaler\n",
    "  |-- categorical_cols -> Imputer -> OneHotEncoder\n",
    "  |-- (remainder: drop or passthrough)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mixed dataset\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "df_mixed = pd.DataFrame({\n",
    "    'age': np.where(np.random.random(n) < 0.1, np.nan, np.random.normal(40, 12, n)),\n",
    "    'income': np.where(np.random.random(n) < 0.08, np.nan, np.random.normal(55000, 15000, n)),\n",
    "    'score': np.random.normal(70, 10, n),\n",
    "    'department': np.random.choice(['engineering', 'marketing', 'sales', 'hr'], n),\n",
    "    'education': np.random.choice(['high_school', 'bachelors', 'masters', 'phd'], n),\n",
    "})\n",
    "\n",
    "# Target: high performer\n",
    "y_mixed = ((df_mixed['score'].fillna(70) > 72) & \n",
    "           (df_mixed['age'].fillna(40) > 35)).astype(int)\n",
    "\n",
    "print(\"Mixed dataset:\")\n",
    "print(df_mixed.head())\n",
    "print(f\"\\nDtypes:\\n{df_mixed.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df_mixed.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column groups\n",
    "numeric_features = ['age', 'income', 'score']\n",
    "categorical_features = ['department', 'education']\n",
    "\n",
    "# Define sub-pipelines for each column type\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "print(\"ColumnTransformer created with:\")\n",
    "print(f\"  Numeric pipeline:     imputer(median) -> StandardScaler\")\n",
    "print(f\"  Categorical pipeline: imputer(mode) -> OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and apply\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    df_mixed, y_mixed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train_m)\n",
    "X_test_processed = preprocessor.transform(X_test_m)\n",
    "\n",
    "print(f\"Train shape after preprocessing: {X_train_processed.shape}\")\n",
    "print(f\"Test shape after preprocessing:  {X_test_processed.shape}\")\n",
    "print()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"Feature names ({len(feature_names)} total):\")\n",
    "for name in feature_names:\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Full Pipeline: ColumnTransformer + LogisticRegression\n",
    "\n",
    "The real power comes from chaining the `ColumnTransformer` (preprocessor) with a model into a single `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full end-to-end pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# One call: impute + scale + encode + fit model\n",
    "full_pipeline.fit(X_train_m, y_train_m)\n",
    "\n",
    "# One call: impute + scale + encode + predict\n",
    "y_pred_m = full_pipeline.predict(X_test_m)\n",
    "\n",
    "accuracy_m = accuracy_score(y_test_m, y_pred_m)\n",
    "print(f\"Full pipeline accuracy: {accuracy_m:.4f}\")\n",
    "print(f\"\\nPipeline structure:\")\n",
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned model coefficients\n",
    "model_coefs = full_pipeline.named_steps['classifier'].coef_[0]\n",
    "feature_names = full_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': model_coefs\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature importance (Logistic Regression coefficients):\")\n",
    "print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['steelblue' if c > 0 else 'salmon' for c in coef_df['Coefficient']]\n",
    "ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('Logistic Regression Coefficients (Full Pipeline)', fontsize=13)\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Using make_column_selector\n",
    "\n",
    "Instead of manually listing column names, `make_column_selector` selects columns by dtype automatically. This makes pipelines more flexible and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic column selection based on dtype\n",
    "preprocessor_auto = ColumnTransformer([\n",
    "    ('num', numeric_transformer, make_column_selector(dtype_include=np.number)),\n",
    "    ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n",
    "])\n",
    "\n",
    "auto_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_auto),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "auto_pipeline.fit(X_train_m, y_train_m)\n",
    "y_pred_auto = auto_pipeline.predict(X_test_m)\n",
    "\n",
    "print(f\"Auto-selector pipeline accuracy: {accuracy_score(y_test_m, y_pred_auto):.4f}\")\n",
    "print()\n",
    "print(\"Numeric columns selected:\", \n",
    "      list(X_train_m.select_dtypes(include=np.number).columns))\n",
    "print(\"Categorical columns selected:\", \n",
    "      list(X_train_m.select_dtypes(include=object).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using manual column lists vs make_column_selector\n",
    "print(\"Manual column lists:\")\n",
    "print(\"  + Explicit, clear what columns are used\")\n",
    "print(\"  - Must update if columns change\")\n",
    "print()\n",
    "print(\"make_column_selector:\")\n",
    "print(\"  + Automatically adapts to new columns of the same type\")\n",
    "print(\"  - Less explicit, may pick up unwanted columns\")\n",
    "print()\n",
    "print(\"Recommendation: Use manual lists in production for safety.\")\n",
    "print(\"Use selectors for quick prototyping and exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pipelines with cross_val_score\n",
    "\n",
    "This is where pipelines truly shine. When used with `cross_val_score`, **each fold** refits the entire pipeline from scratch. This means:\n",
    "- Imputer statistics are computed per fold (no leakage)\n",
    "- Scaler parameters are computed per fold (no leakage)\n",
    "- Encoder categories are learned per fold (no leakage)\n",
    "\n",
    "Without pipelines, you would need to manually refit every preprocessor within each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with the full pipeline\n",
    "# Each fold automatically re-fits all preprocessing steps\n",
    "cv_scores = cross_val_score(\n",
    "    full_pipeline, \n",
    "    df_mixed,  # raw, unprocessed data\n",
    "    y_mixed, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"5-Fold Cross-Validation with Pipeline:\")\n",
    "print(f\"  Fold scores: {cv_scores.round(4)}\")\n",
    "print(f\"  Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "print()\n",
    "print(\"Each fold independently:\")\n",
    "print(\"  1. Split data into train/validation\")\n",
    "print(\"  2. fit_transform(train) for all preprocessors\")\n",
    "print(\"  3. transform(validation) using train-fitted parameters\")\n",
    "print(\"  4. Fit model on preprocessed train, evaluate on preprocessed validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation scores\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "folds = range(1, len(cv_scores) + 1)\n",
    "ax.bar(folds, cv_scores, color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax.axhline(cv_scores.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {cv_scores.mean():.4f}')\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('5-Fold Cross-Validation Scores (Pipeline)', fontsize=13)\n",
    "ax.set_xticks(list(folds))\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Manual Preprocessing vs Pipeline: A Comparison\n",
    "\n",
    "Let us build the same model both ways and compare the approaches side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh dataset for comparison\n",
    "np.random.seed(42)\n",
    "n = 400\n",
    "\n",
    "df_compare = pd.DataFrame({\n",
    "    'age': np.where(np.random.random(n) < 0.1, np.nan, np.random.normal(35, 10, n)),\n",
    "    'salary': np.where(np.random.random(n) < 0.12, np.nan, np.random.lognormal(10.8, 0.5, n)),\n",
    "    'tenure_years': np.random.uniform(0, 15, n),\n",
    "    'department': np.random.choice(['tech', 'finance', 'ops', 'marketing'], n),\n",
    "    'performance': np.random.choice(['low', 'medium', 'high'], n, p=[0.2, 0.5, 0.3]),\n",
    "})\n",
    "y_compare = (np.random.random(n) < 0.3).astype(int)  # 30% attrition rate\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    df_compare, y_compare, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Comparison dataset:\")\n",
    "print(df_compare.head())\n",
    "print(f\"\\nShape: {df_compare.shape}\")\n",
    "print(f\"Missing: {df_compare.isnull().sum().sum()} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# APPROACH 1: Manual Preprocessing (VERBOSE)\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Separate columns\n",
    "num_cols = ['age', 'salary', 'tenure_years']\n",
    "cat_cols = ['department', 'performance']\n",
    "\n",
    "X_tr_manual = X_tr.copy()\n",
    "X_te_manual = X_te.copy()\n",
    "\n",
    "# Step 2: Impute numeric columns\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "X_tr_manual[num_cols] = num_imputer.fit_transform(X_tr_manual[num_cols])\n",
    "X_te_manual[num_cols] = num_imputer.transform(X_te_manual[num_cols])\n",
    "\n",
    "# Step 3: Scale numeric columns\n",
    "scaler_manual = StandardScaler()\n",
    "X_tr_num_scaled = scaler_manual.fit_transform(X_tr_manual[num_cols])\n",
    "X_te_num_scaled = scaler_manual.transform(X_te_manual[num_cols])\n",
    "\n",
    "# Step 4: Encode categorical columns\n",
    "ohe_manual = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_tr_cat_encoded = ohe_manual.fit_transform(X_tr_manual[cat_cols])\n",
    "X_te_cat_encoded = ohe_manual.transform(X_te_manual[cat_cols])\n",
    "\n",
    "# Step 5: Combine\n",
    "X_tr_final = np.hstack([X_tr_num_scaled, X_tr_cat_encoded])\n",
    "X_te_final = np.hstack([X_te_num_scaled, X_te_cat_encoded])\n",
    "\n",
    "# Step 6: Fit model\n",
    "lr_manual = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_manual.fit(X_tr_final, y_tr)\n",
    "acc_manual = accuracy_score(y_te, lr_manual.predict(X_te_final))\n",
    "\n",
    "print(f\"Manual approach accuracy: {acc_manual:.4f}\")\n",
    "print(f\"Lines of preprocessing code: ~15\")\n",
    "print(f\"Objects to serialize for production: 4 (imputer, scaler, encoder, model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# APPROACH 2: Pipeline (CLEAN)\n",
    "# ============================================\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ct),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Two lines: fit and evaluate\n",
    "pipeline.fit(X_tr, y_tr)\n",
    "acc_pipeline = accuracy_score(y_te, pipeline.predict(X_te))\n",
    "\n",
    "print(f\"Pipeline approach accuracy: {acc_pipeline:.4f}\")\n",
    "print(f\"Lines of preprocessing code: ~2 (fit + predict)\")\n",
    "print(f\"Objects to serialize for production: 1 (the pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Metric':<30} {'Manual':>10} {'Pipeline':>10}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Accuracy':<30} {acc_manual:>10.4f} {acc_pipeline:>10.4f}\")\n",
    "print(f\"{'Code lines to preprocess':<30} {'~15':>10} {'~2':>10}\")\n",
    "print(f\"{'Objects to serialize':<30} {'4':>10} {'1':>10}\")\n",
    "print(f\"{'Leakage risk':<30} {'High':>10} {'None':>10}\")\n",
    "print(f\"{'Works with cross_val_score':<30} {'No':>10} {'Yes':>10}\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nSame accuracy, but pipelines are safer and cleaner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "### Mistake 1: Not Using Pipelines in Production\n",
    "\n",
    "In production, you need to apply the **exact same preprocessing** to new data. Without a pipeline, you must manage multiple objects and risk inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production scenario: new data arrives\n",
    "new_employee = pd.DataFrame({\n",
    "    'age': [28],\n",
    "    'salary': [65000],\n",
    "    'tenure_years': [3.5],\n",
    "    'department': ['tech'],\n",
    "    'performance': ['high']\n",
    "})\n",
    "\n",
    "# With pipeline: one call\n",
    "prediction = pipeline.predict(new_employee)\n",
    "probability = pipeline.predict_proba(new_employee)\n",
    "\n",
    "print(\"Production prediction with pipeline:\")\n",
    "print(f\"  Prediction: {'Will leave' if prediction[0] == 1 else 'Will stay'}\")\n",
    "print(f\"  Probability: {probability[0][1]:.4f}\")\n",
    "print()\n",
    "print(\"No need to separately call imputer, scaler, encoder - the pipeline handles it all.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Fitting a Transformer Outside the Pipeline\n",
    "\n",
    "If you fit a scaler on the full dataset and then put it in a pipeline, the pipeline does not know about the data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Pre-fitting a scaler and putting it in a pipeline\n",
    "print(\"WRONG approach:\")\n",
    "print(\"  scaler = StandardScaler()\")\n",
    "print(\"  scaler.fit(ALL_DATA)          # leakage!\")\n",
    "print(\"  pipe = Pipeline([\")\n",
    "print(\"      ('scaler', scaler),       # already fitted on all data\")\n",
    "print(\"      ('model', LogisticRegression())\")\n",
    "print(\"  ])\")\n",
    "print(\"  pipe.fit(X_train, y_train)    # scaler.fit() called again, but damage done\")\n",
    "print()\n",
    "print(\"CORRECT approach:\")\n",
    "print(\"  pipe = Pipeline([\")\n",
    "print(\"      ('scaler', StandardScaler()),  # unfitted!\")\n",
    "print(\"      ('model', LogisticRegression())\")\n",
    "print(\"  ])\")\n",
    "print(\"  pipe.fit(X_train, y_train)    # scaler learns from train only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Using cross_val_score Without a Pipeline\n",
    "\n",
    "If you preprocess first and then use `cross_val_score` on the already-preprocessed data, the scaler/imputer has already seen all folds. This leaks information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Preprocess, then cross-validate\n",
    "X_all_numeric = df_compare[num_cols].copy()\n",
    "\n",
    "# Impute + scale on ALL data first (leakage)\n",
    "imp = SimpleImputer(strategy='median')\n",
    "scl = StandardScaler()\n",
    "X_preprocessed = scl.fit_transform(imp.fit_transform(X_all_numeric))\n",
    "\n",
    "# cross_val_score on already-preprocessed data\n",
    "scores_leaked = cross_val_score(\n",
    "    LogisticRegression(random_state=42), \n",
    "    X_preprocessed, y_compare, cv=5\n",
    ")\n",
    "\n",
    "# CORRECT: Use pipeline with cross_val_score\n",
    "pipe_cv = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(random_state=42))\n",
    "])\n",
    "scores_correct = cross_val_score(pipe_cv, X_all_numeric, y_compare, cv=5)\n",
    "\n",
    "print(f\"With leakage (preprocess then CV):   {scores_leaked.mean():.4f} (+/- {scores_leaked.std():.4f})\")\n",
    "print(f\"Without leakage (pipeline + CV):     {scores_correct.mean():.4f} (+/- {scores_correct.std():.4f})\")\n",
    "print()\n",
    "print(\"The leaked scores may be optimistic.\")\n",
    "print(\"Always use a Pipeline inside cross_val_score.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Common Mistakes\n",
    "\n",
    "| Mistake | Consequence | Fix |\n",
    "|---------|-------------|-----|\n",
    "| No pipeline in production | Multiple objects, inconsistent preprocessing | Use a single Pipeline |\n",
    "| Pre-fitted transformer in pipeline | Data leakage | Always put unfitted transformers in the pipeline |\n",
    "| Preprocessing before cross_val_score | Leakage across folds | Put all preprocessing inside the Pipeline |\n",
    "| Using `remainder='drop'` without realizing | Columns silently dropped | Set `remainder='passthrough'` if you want to keep extra columns |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Build a complete pipeline for the dataset below. The dataset simulates customer churn prediction with numeric features (some missing), categorical features, and a binary target.\n",
    "\n",
    "Requirements:\n",
    "1. Define numeric and categorical column lists\n",
    "2. Create sub-pipelines: numeric (median imputer + StandardScaler), categorical (mode imputer + OneHotEncoder)\n",
    "3. Combine into a ColumnTransformer\n",
    "4. Chain with LogisticRegression into a full Pipeline\n",
    "5. Evaluate with 5-fold cross-validation using `cross_val_score`\n",
    "6. Report the mean accuracy and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise starter code\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "exercise_df = pd.DataFrame({\n",
    "    'monthly_charges': np.where(\n",
    "        np.random.random(n) < 0.08, np.nan, np.random.normal(65, 30, n)\n",
    "    ),\n",
    "    'tenure_months': np.where(\n",
    "        np.random.random(n) < 0.05, np.nan, np.random.uniform(1, 72, n)\n",
    "    ),\n",
    "    'total_charges': np.where(\n",
    "        np.random.random(n) < 0.1, np.nan, np.random.lognormal(7, 1, n)\n",
    "    ),\n",
    "    'contract': np.random.choice(\n",
    "        ['month-to-month', 'one_year', 'two_year'], n, p=[0.5, 0.3, 0.2]\n",
    "    ),\n",
    "    'internet_service': np.random.choice(\n",
    "        ['dsl', 'fiber_optic', 'none'], n, p=[0.4, 0.4, 0.2]\n",
    "    ),\n",
    "    'payment_method': np.random.choice(\n",
    "        ['electronic_check', 'mailed_check', 'bank_transfer', 'credit_card'], n\n",
    "    ),\n",
    "})\n",
    "exercise_y = (np.random.random(n) < 0.26).astype(int)  # ~26% churn rate\n",
    "\n",
    "print(\"Exercise dataset:\")\n",
    "print(exercise_df.head())\n",
    "print(f\"\\nShape: {exercise_df.shape}\")\n",
    "print(f\"Missing values:\\n{exercise_df.isnull().sum()}\")\n",
    "print(f\"\\nChurn rate: {exercise_y.mean():.2%}\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Step 1: Define column lists\n",
    "# num_features = [...]\n",
    "# cat_features = [...]\n",
    "\n",
    "# Step 2: Create sub-pipelines\n",
    "# num_pipeline = Pipeline([...])\n",
    "# cat_pipeline = Pipeline([...])\n",
    "\n",
    "# Step 3: ColumnTransformer\n",
    "# preprocessor = ColumnTransformer([...])\n",
    "\n",
    "# Step 4: Full Pipeline\n",
    "# full_pipe = Pipeline([...])\n",
    "\n",
    "# Step 5: Cross-validation\n",
    "# cv_scores = cross_val_score(full_pipe, exercise_df, exercise_y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 6: Report results\n",
    "# print(f\"Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}