{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: From Intuition to Sklearn\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Understand the intuition behind the line of best fit and residuals\n",
    "- Derive and apply the least squares cost function\n",
    "- Implement simple linear regression from scratch using NumPy\n",
    "- Use `sklearn.linear_model.LinearRegression` for single and multi-feature regression\n",
    "- Visualize regression results: fit lines, residuals, and actual vs predicted plots\n",
    "- Apply best practices: train/test splits and pipelines with scaling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python (NumPy, Matplotlib)\n",
    "- Understanding of mean, variance, and basic algebra\n",
    "- Familiarity with train/test splitting concepts\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Line of Best Fit: Intuition](#1-the-line-of-best-fit-intuition)\n",
    "2. [Residuals and the Least Squares Cost Function](#2-residuals-and-the-least-squares-cost-function)\n",
    "3. [The Normal Equation](#3-the-normal-equation)\n",
    "4. [Linear Regression from Scratch (NumPy)](#4-linear-regression-from-scratch-numpy)\n",
    "5. [Linear Regression with Sklearn](#5-linear-regression-with-sklearn)\n",
    "6. [Multi-Feature Regression](#6-multi-feature-regression)\n",
    "7. [Visualizations](#7-visualizations)\n",
    "8. [Best Practices](#8-best-practices)\n",
    "9. [Common Mistakes](#9-common-mistakes)\n",
    "10. [Exercise](#10-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Line of Best Fit: Intuition\n",
    "\n",
    "Linear regression finds the **straight line** (or hyperplane) that best describes the relationship between input features $x$ and a continuous target $y$.\n",
    "\n",
    "**The model:**\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p$$\n",
    "\n",
    "Where:\n",
    "- $w_0$ is the **intercept** (bias term)\n",
    "- $w_1, \\dots, w_p$ are the **coefficients** (weights) for each feature\n",
    "- $\\hat{y}$ is the **predicted** value\n",
    "\n",
    "For a single feature, this simplifies to the familiar equation of a line: $\\hat{y} = w_0 + w_1 x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple 1D data\n",
    "np.random.seed(42)\n",
    "X_simple = 2 * np.random.rand(50, 1)\n",
    "y_simple = 3 + 4 * X_simple.flatten() + np.random.randn(50) * 0.8\n",
    "\n",
    "plt.scatter(X_simple, y_simple, alpha=0.7, edgecolors=\"k\", label=\"Data points\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"A Simple Dataset — Where Should the Line Go?\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Residuals and the Least Squares Cost Function\n",
    "\n",
    "A **residual** is the difference between the observed value and the predicted value:\n",
    "\n",
    "$$e_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "The **Mean Squared Error (MSE)** cost function sums the squared residuals:\n",
    "\n",
    "$$J = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Why squared?**\n",
    "- Penalizes large errors more than small ones\n",
    "- Makes the function differentiable (smooth optimization)\n",
    "- Positive and negative errors don't cancel out\n",
    "\n",
    "The goal of linear regression is to find $w_0, w_1, \\dots, w_p$ that **minimize** $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residuals for two candidate lines\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bad fit\n",
    "w0_bad, w1_bad = 1.0, 2.0\n",
    "y_pred_bad = w0_bad + w1_bad * X_simple.flatten()\n",
    "axes[0].scatter(X_simple, y_simple, alpha=0.7, edgecolors=\"k\")\n",
    "axes[0].plot(X_simple, y_pred_bad, \"r-\", linewidth=2, label=f\"y = {w0_bad} + {w1_bad}x\")\n",
    "for i in range(len(X_simple)):\n",
    "    axes[0].plot([X_simple[i, 0], X_simple[i, 0]], [y_simple[i], y_pred_bad[i]],\n",
    "                 \"r--\", alpha=0.3, linewidth=0.8)\n",
    "mse_bad = np.mean((y_simple - y_pred_bad) ** 2)\n",
    "axes[0].set_title(f\"Poor Fit — MSE = {mse_bad:.2f}\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "\n",
    "# Good fit (close to true values)\n",
    "w0_good, w1_good = 3.0, 4.0\n",
    "y_pred_good = w0_good + w1_good * X_simple.flatten()\n",
    "axes[1].scatter(X_simple, y_simple, alpha=0.7, edgecolors=\"k\")\n",
    "axes[1].plot(X_simple, y_pred_good, \"g-\", linewidth=2, label=f\"y = {w0_good} + {w1_good}x\")\n",
    "for i in range(len(X_simple)):\n",
    "    axes[1].plot([X_simple[i, 0], X_simple[i, 0]], [y_simple[i], y_pred_good[i]],\n",
    "                 \"g--\", alpha=0.3, linewidth=0.8)\n",
    "mse_good = np.mean((y_simple - y_pred_good) ** 2)\n",
    "axes[1].set_title(f\"Better Fit — MSE = {mse_good:.2f}\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Normal Equation\n",
    "\n",
    "For ordinary least squares, there is a **closed-form solution** called the Normal Equation:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "Where $\\mathbf{X}$ includes a column of ones for the intercept.\n",
    "\n",
    "**Key points:**\n",
    "- Directly computes the optimal weights in one step (no iteration)\n",
    "- Computational complexity is $O(n \\cdot p^2 + p^3)$ — expensive for large $p$\n",
    "- Requires $\\mathbf{X}^T\\mathbf{X}$ to be invertible (fails with perfect multicollinearity)\n",
    "- Sklearn uses optimized solvers (SVD-based) that are more numerically stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal equation implementation\n",
    "X_b = np.c_[np.ones((X_simple.shape[0], 1)), X_simple]  # add bias column\n",
    "w_normal = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_simple\n",
    "\n",
    "print(f\"Normal Equation solution:\")\n",
    "print(f\"  Intercept (w0): {w_normal[0]:.4f}\")\n",
    "print(f\"  Slope     (w1): {w_normal[1]:.4f}\")\n",
    "print(f\"  (True values: w0=3.0, w1=4.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Linear Regression from Scratch (NumPy)\n",
    "\n",
    "Let's implement simple linear regression from scratch using both the normal equation and a manual gradient descent approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    \"\"\"Linear regression using the Normal Equation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.weights = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.weights\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.weights[0]\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "\n",
    "# Fit our scratch model\n",
    "model_scratch = SimpleLinearRegression()\n",
    "model_scratch.fit(X_simple, y_simple)\n",
    "\n",
    "print(f\"From-scratch model:\")\n",
    "print(f\"  Intercept: {model_scratch.intercept_:.4f}\")\n",
    "print(f\"  Coefficient: {model_scratch.coef_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the from-scratch fit with residuals\n",
    "y_pred_scratch = model_scratch.predict(X_simple)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter + fit line\n",
    "X_sorted = np.sort(X_simple, axis=0)\n",
    "y_line = model_scratch.predict(X_sorted)\n",
    "axes[0].scatter(X_simple, y_simple, alpha=0.7, edgecolors=\"k\", label=\"Data\")\n",
    "axes[0].plot(X_sorted, y_line, \"r-\", linewidth=2, label=\"Fit line\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"From-Scratch Linear Regression\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_simple - y_pred_scratch\n",
    "axes[1].scatter(y_pred_scratch, residuals, alpha=0.7, edgecolors=\"k\")\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1)\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"Residual Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_scratch = np.mean(residuals ** 2)\n",
    "print(f\"MSE: {mse_scratch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Linear Regression with Sklearn\n",
    "\n",
    "Sklearn's `LinearRegression` provides a production-ready implementation with a clean API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn LinearRegression on the same data\n",
    "model_sk = LinearRegression()\n",
    "model_sk.fit(X_simple, y_simple)\n",
    "\n",
    "print(\"Sklearn LinearRegression:\")\n",
    "print(f\"  intercept_:      {model_sk.intercept_:.4f}\")\n",
    "print(f\"  coef_:           {model_sk.coef_}\")\n",
    "print(f\"  n_features_in_:  {model_sk.n_features_in_}\")\n",
    "\n",
    "# Predict\n",
    "y_pred_sk = model_sk.predict(X_simple)\n",
    "\n",
    "# Compare with our scratch model\n",
    "print(f\"\\nMax difference between scratch and sklearn predictions: \"\n",
    "      f\"{np.max(np.abs(y_pred_scratch - y_pred_sk)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Multi-Feature Regression\n",
    "\n",
    "Linear regression extends naturally to multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-feature synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 5\n",
    "\n",
    "X_multi = np.random.randn(n_samples, n_features)\n",
    "true_weights = np.array([3.0, -1.5, 0.0, 2.0, -0.5])\n",
    "true_intercept = 5.0\n",
    "y_multi = X_multi @ true_weights + true_intercept + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train, y_train)\n",
    "\n",
    "print(\"Multi-feature regression results:\")\n",
    "print(f\"  Intercept: {model_multi.intercept_:.4f} (true: {true_intercept})\")\n",
    "print(f\"  Coefficients:\")\n",
    "for i, (learned, true) in enumerate(zip(model_multi.coef_, true_weights)):\n",
    "    print(f\"    w{i+1}: {learned:+.4f}  (true: {true:+.1f})\")\n",
    "print(f\"  n_features_in_: {model_multi.n_features_in_}\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = model_multi.predict(X_train)\n",
    "y_pred_test = model_multi.predict(X_test)\n",
    "\n",
    "print(f\"\\n  Train R2: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  Test  R2: {r2_score(y_test, y_pred_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted (test set)\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.7, edgecolors=\"k\")\n",
    "min_val = min(y_test.min(), y_pred_test.min())\n",
    "max_val = max(y_test.max(), y_pred_test.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], \"r--\", linewidth=2, label=\"Perfect prediction\")\n",
    "axes[0].set_xlabel(\"Actual\")\n",
    "axes[0].set_ylabel(\"Predicted\")\n",
    "axes[0].set_title(\"Actual vs Predicted (Test Set)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Coefficient comparison\n",
    "feature_names = [f\"w{i+1}\" for i in range(n_features)]\n",
    "x_pos = np.arange(n_features)\n",
    "width = 0.35\n",
    "axes[1].bar(x_pos - width/2, true_weights, width, label=\"True\", color=\"steelblue\", alpha=0.8)\n",
    "axes[1].bar(x_pos + width/2, model_multi.coef_, width, label=\"Learned\", color=\"coral\", alpha=0.8)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(feature_names)\n",
    "axes[1].set_ylabel(\"Weight value\")\n",
    "axes[1].set_title(\"True vs Learned Coefficients\")\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Best Practices\n",
    "\n",
    "- **Always split your data** into train and test sets before fitting\n",
    "- **Use a Pipeline** to combine preprocessing and model fitting\n",
    "- **Scale features** when they have different units or magnitudes\n",
    "- **Check residual plots** to validate model assumptions\n",
    "- **Report metrics on the test set**, not the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Pipeline with StandardScaler\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Pipeline Test R2: {r2_score(y_test, y_pred_pipe):.4f}\")\n",
    "print(f\"Pipeline Test MSE: {mean_squared_error(y_test, y_pred_pipe):.4f}\")\n",
    "print(\"\\nNote: For linear regression with OLS, scaling does not change R2,\")\n",
    "print(\"but it IS essential for regularized models (Ridge, Lasso).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Common Mistakes\n",
    "\n",
    "| Mistake | Why It's a Problem | Fix |\n",
    "|---|---|---|\n",
    "| Not splitting data | Overly optimistic metrics | Use `train_test_split` |\n",
    "| Ignoring residual plots | Miss assumption violations | Always plot residuals |\n",
    "| Using R2 alone | R2 can be misleading (e.g., nonlinear data) | Check MSE + residuals |\n",
    "| Forgetting to scale | Coefficients not comparable, hurts regularized models | Use `StandardScaler` in a `Pipeline` |\n",
    "| Fitting on test data | Data leakage | Only `transform` on test, never `fit` |\n",
    "| Extrapolating beyond training range | Linear models assume the trend continues | Be cautious about prediction range |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Exercise\n",
    "\n",
    "**Task:** Fit a linear regression model on the **California Housing** dataset and report the R2 score.\n",
    "\n",
    "Steps:\n",
    "1. Load the data with `fetch_california_housing()`\n",
    "2. Split into train (80%) and test (20%) with `random_state=42`\n",
    "3. Build a Pipeline: `StandardScaler` + `LinearRegression`\n",
    "4. Fit on training data, predict on test data\n",
    "5. Report R2 and create an actual vs predicted plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise Solution ---\n",
    "\n",
    "# Step 1: Load data\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "feature_names_housing = housing.feature_names\n",
    "\n",
    "print(f\"Dataset shape: {X_housing.shape}\")\n",
    "print(f\"Features: {feature_names_housing}\")\n",
    "print(f\"Target: Median house value (in $100,000s)\")\n",
    "\n",
    "# Step 2: Split\n",
    "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Pipeline\n",
    "housing_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Step 4: Fit and predict\n",
    "housing_pipeline.fit(X_h_train, y_h_train)\n",
    "y_h_pred = housing_pipeline.predict(X_h_test)\n",
    "\n",
    "# Step 5: Report\n",
    "r2 = r2_score(y_h_test, y_h_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_h_test, y_h_pred))\n",
    "print(f\"\\nTest R2:   {r2:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(y_h_test, y_h_pred, alpha=0.3, s=10, edgecolors=\"k\", linewidths=0.3)\n",
    "mn = min(y_h_test.min(), y_h_pred.min())\n",
    "mx = max(y_h_test.max(), y_h_pred.max())\n",
    "plt.plot([mn, mx], [mn, mx], \"r--\", linewidth=2, label=\"Perfect prediction\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(f\"California Housing — Actual vs Predicted (R2={r2:.3f})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}