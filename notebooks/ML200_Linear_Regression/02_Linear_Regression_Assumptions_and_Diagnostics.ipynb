{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Assumptions and Diagnostics\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- List and explain the 5 key assumptions of linear regression\n",
    "- Create and interpret diagnostic plots: residual vs fitted, Q-Q plots\n",
    "- Detect multicollinearity using correlation matrices and VIF\n",
    "- Recognize heteroscedasticity and understand its consequences\n",
    "- Know what to do when assumptions are violated\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebook 01 (Linear Regression basics with sklearn)\n",
    "- Basic understanding of residuals and MSE\n",
    "- Familiarity with correlation\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The 5 Key Assumptions](#1-the-5-key-assumptions)\n",
    "2. [Setup: Generate Well-Behaved Data](#2-setup-generate-well-behaved-data)\n",
    "3. [Checking Linearity](#3-checking-linearity)\n",
    "4. [Checking Normality of Residuals](#4-checking-normality-of-residuals)\n",
    "5. [Checking Homoscedasticity](#5-checking-homoscedasticity)\n",
    "6. [Checking Multicollinearity](#6-checking-multicollinearity)\n",
    "7. [When Assumptions Are Violated: What to Do](#7-when-assumptions-are-violated-what-to-do)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The 5 Key Assumptions\n",
    "\n",
    "Linear regression results are reliable **only if** these assumptions hold (approximately):\n",
    "\n",
    "| # | Assumption | What It Means | How to Check |\n",
    "|---|---|---|---|\n",
    "| 1 | **Linearity** | The relationship between X and y is linear | Residual vs fitted plot |\n",
    "| 2 | **Independence** | Observations are independent of each other | Study design / Durbin-Watson test |\n",
    "| 3 | **Normality of residuals** | Residuals follow a normal distribution | Q-Q plot, Shapiro-Wilk test |\n",
    "| 4 | **Homoscedasticity** | Residual variance is constant across all fitted values | Residual vs fitted plot |\n",
    "| 5 | **No multicollinearity** | Features are not highly correlated with each other | Correlation matrix, VIF |\n",
    "\n",
    "**Important:** These assumptions are about the **residuals**, not the raw data. We check them **after** fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Setup: Generate Well-Behaved Data\n",
    "\n",
    "First, let's create a dataset that satisfies all assumptions so we know what \"good\" diagnostics look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well-behaved data: linear relationship, normal errors, constant variance\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "X_good = np.random.randn(n, 2)\n",
    "y_good = 3 + 2 * X_good[:, 0] - 1.5 * X_good[:, 1] + np.random.randn(n) * 0.8\n",
    "\n",
    "# Fit model\n",
    "model_good = LinearRegression()\n",
    "model_good.fit(X_good, y_good)\n",
    "y_pred_good = model_good.predict(X_good)\n",
    "residuals_good = y_good - y_pred_good\n",
    "\n",
    "print(f\"R2: {model_good.score(X_good, y_good):.4f}\")\n",
    "print(f\"Coefficients: {model_good.coef_}\")\n",
    "print(f\"Intercept: {model_good.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Checking Linearity\n",
    "\n",
    "**Residual vs Fitted plot:** If the relationship is linear, residuals should scatter randomly around zero with no visible pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Good case: linear relationship ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Good: residuals vs fitted\n",
    "axes[0].scatter(y_pred_good, residuals_good, alpha=0.5, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Fitted values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(\"GOOD: Residual vs Fitted (Linear Data)\")\n",
    "\n",
    "# --- Bad case: nonlinear relationship ---\n",
    "np.random.seed(42)\n",
    "X_nonlinear = np.random.uniform(0, 5, (n, 1))\n",
    "y_nonlinear = 2 + 3 * X_nonlinear.flatten() ** 2 + np.random.randn(n) * 3\n",
    "\n",
    "model_bad_lin = LinearRegression()\n",
    "model_bad_lin.fit(X_nonlinear, y_nonlinear)\n",
    "y_pred_bad_lin = model_bad_lin.predict(X_nonlinear)\n",
    "residuals_bad_lin = y_nonlinear - y_pred_bad_lin\n",
    "\n",
    "axes[1].scatter(y_pred_bad_lin, residuals_bad_lin, alpha=0.5, edgecolors=\"k\",\n",
    "                linewidths=0.5, color=\"salmon\")\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[1].set_xlabel(\"Fitted values\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"BAD: Residual vs Fitted (Nonlinear Data)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Random scatter around zero = GOOD (linearity holds)\")\n",
    "print(\"Right: Curved pattern = BAD (linearity violated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Checking Normality of Residuals\n",
    "\n",
    "**Q-Q plot:** Compares residual quantiles against theoretical normal quantiles. Points should fall on the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Good: normal residuals from the well-behaved model\n",
    "stats.probplot(residuals_good, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title(\"GOOD: Q-Q Plot (Normal Residuals)\")\n",
    "axes[0].get_lines()[0].set_markerfacecolor(\"steelblue\")\n",
    "axes[0].get_lines()[0].set_alpha(0.6)\n",
    "\n",
    "# Bad: skewed residuals\n",
    "np.random.seed(42)\n",
    "X_skew = np.random.randn(n, 1)\n",
    "# Use exponential errors (right-skewed) instead of normal\n",
    "y_skew = 2 + 3 * X_skew.flatten() + np.random.exponential(2, n)\n",
    "\n",
    "model_skew = LinearRegression()\n",
    "model_skew.fit(X_skew, y_skew)\n",
    "residuals_skew = y_skew - model_skew.predict(X_skew)\n",
    "\n",
    "stats.probplot(residuals_skew, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title(\"BAD: Q-Q Plot (Skewed Residuals)\")\n",
    "axes[1].get_lines()[0].set_markerfacecolor(\"salmon\")\n",
    "axes[1].get_lines()[0].set_alpha(0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "stat_good, p_good = stats.shapiro(residuals_good)\n",
    "stat_skew, p_skew = stats.shapiro(residuals_skew)\n",
    "\n",
    "print(f\"Shapiro-Wilk test (H0: residuals are normal):\")\n",
    "print(f\"  Good data: W={stat_good:.4f}, p={p_good:.4f}  {'-> Normal' if p_good > 0.05 else '-> Not normal'}\")\n",
    "print(f\"  Skew data: W={stat_skew:.4f}, p={p_skew:.4f}  {'-> Normal' if p_skew > 0.05 else '-> Not normal'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of residuals (supplementary check)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(residuals_good, bins=25, edgecolor=\"k\", alpha=0.7, color=\"steelblue\", density=True)\n",
    "x_range = np.linspace(residuals_good.min(), residuals_good.max(), 100)\n",
    "axes[0].plot(x_range, stats.norm.pdf(x_range, residuals_good.mean(), residuals_good.std()),\n",
    "             \"r-\", linewidth=2, label=\"Normal fit\")\n",
    "axes[0].set_title(\"GOOD: Residual Distribution\")\n",
    "axes[0].set_xlabel(\"Residual\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(residuals_skew, bins=25, edgecolor=\"k\", alpha=0.7, color=\"salmon\", density=True)\n",
    "x_range2 = np.linspace(residuals_skew.min(), residuals_skew.max(), 100)\n",
    "axes[1].plot(x_range2, stats.norm.pdf(x_range2, residuals_skew.mean(), residuals_skew.std()),\n",
    "             \"r-\", linewidth=2, label=\"Normal fit\")\n",
    "axes[1].set_title(\"BAD: Residual Distribution (Right-Skewed)\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Checking Homoscedasticity\n",
    "\n",
    "**Homoscedasticity** means the variance of residuals is constant across all levels of the predicted values.\n",
    "\n",
    "**Heteroscedasticity** (the violation) means variance changes — often increasing with larger values. This:\n",
    "- Makes standard errors unreliable\n",
    "- Invalidates confidence intervals and p-values\n",
    "- Can lead to inefficient coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Good: homoscedastic residuals (from well-behaved data)\n",
    "axes[0].scatter(y_pred_good, residuals_good, alpha=0.5, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Fitted values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(\"GOOD: Constant Variance (Homoscedastic)\")\n",
    "\n",
    "# Bad: heteroscedastic data (variance increases with X)\n",
    "np.random.seed(42)\n",
    "X_hetero = np.random.uniform(1, 10, (n, 1))\n",
    "# Error variance scales with X\n",
    "noise_hetero = np.random.randn(n) * (0.5 * X_hetero.flatten())\n",
    "y_hetero = 2 + 3 * X_hetero.flatten() + noise_hetero\n",
    "\n",
    "model_hetero = LinearRegression()\n",
    "model_hetero.fit(X_hetero, y_hetero)\n",
    "y_pred_hetero = model_hetero.predict(X_hetero)\n",
    "residuals_hetero = y_hetero - y_pred_hetero\n",
    "\n",
    "axes[1].scatter(y_pred_hetero, residuals_hetero, alpha=0.5, edgecolors=\"k\",\n",
    "                linewidths=0.5, color=\"salmon\")\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[1].set_xlabel(\"Fitted values\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"BAD: Increasing Variance (Heteroscedastic)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Residuals form a uniform band — GOOD\")\n",
    "print(\"Right: Residuals fan out (funnel shape) — BAD (heteroscedasticity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Checking Multicollinearity\n",
    "\n",
    "**Multicollinearity** occurs when features are highly correlated with each other. This:\n",
    "- Makes individual coefficient estimates unstable\n",
    "- Inflates standard errors\n",
    "- Makes it hard to determine which feature is driving the prediction\n",
    "\n",
    "**Detection methods:**\n",
    "- Correlation matrix (pairwise)\n",
    "- **Variance Inflation Factor (VIF):** measures how much the variance of a coefficient is inflated due to collinearity\n",
    "  - VIF = 1: no collinearity\n",
    "  - VIF > 5: moderate concern\n",
    "  - VIF > 10: serious multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with multicollinearity\n",
    "np.random.seed(42)\n",
    "x1 = np.random.randn(n)\n",
    "x2 = x1 + np.random.randn(n) * 0.1  # x2 is almost identical to x1\n",
    "x3 = np.random.randn(n)              # x3 is independent\n",
    "\n",
    "X_collinear = np.column_stack([x1, x2, x3])\n",
    "y_collinear = 3 + 2 * x1 - 1 * x3 + np.random.randn(n) * 0.5\n",
    "\n",
    "df_collinear = pd.DataFrame(X_collinear, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "corr = df_collinear.corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1,\n",
    "            square=True, fmt=\".3f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix (x1 and x2 are highly correlated)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VIF for each feature\n",
    "def compute_vif(X_df):\n",
    "    \"\"\"Compute Variance Inflation Factor for each feature.\"\"\"\n",
    "    vif_data = []\n",
    "    for i, col in enumerate(X_df.columns):\n",
    "        # Regress feature i on all other features\n",
    "        other_cols = [c for c in X_df.columns if c != col]\n",
    "        X_other = X_df[other_cols].values\n",
    "        y_feat = X_df[col].values\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_other, y_feat)\n",
    "        r2 = model.score(X_other, y_feat)\n",
    "\n",
    "        vif = 1 / (1 - r2) if r2 < 1 else float(\"inf\")\n",
    "        vif_data.append({\"Feature\": col, \"VIF\": vif})\n",
    "\n",
    "    return pd.DataFrame(vif_data)\n",
    "\n",
    "vif_df = compute_vif(df_collinear)\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_df.to_string(index=False))\n",
    "print(\"\\nVIF > 10 for x1 and x2 indicates serious multicollinearity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact on coefficients: fit the same model many times with slight data changes\n",
    "np.random.seed(42)\n",
    "coefs_collinear = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # Bootstrap resample\n",
    "    idx = np.random.choice(n, size=n, replace=True)\n",
    "    model_temp = LinearRegression()\n",
    "    model_temp.fit(X_collinear[idx], y_collinear[idx])\n",
    "    coefs_collinear.append(model_temp.coef_)\n",
    "\n",
    "coefs_collinear = np.array(coefs_collinear)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, name in enumerate([\"x1 (collinear)\", \"x2 (collinear)\", \"x3 (independent)\"]):\n",
    "    axes[i].hist(coefs_collinear[:, i], bins=20, edgecolor=\"k\", alpha=0.7,\n",
    "                 color=\"salmon\" if i < 2 else \"steelblue\")\n",
    "    axes[i].set_title(f\"{name}\\nStd = {coefs_collinear[:, i].std():.3f}\")\n",
    "    axes[i].set_xlabel(\"Coefficient value\")\n",
    "\n",
    "plt.suptitle(\"Coefficient Instability Due to Multicollinearity\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"x1 and x2 coefficients vary wildly across resamples,\")\n",
    "print(\"while x3 (independent) remains stable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. When Assumptions Are Violated: What to Do\n",
    "\n",
    "| Violation | Possible Remedies |\n",
    "|---|---|\n",
    "| **Nonlinearity** | Add polynomial features, use log/sqrt transforms, switch to a nonlinear model |\n",
    "| **Non-normal residuals** | Transform the target (log, Box-Cox), use robust regression, increase sample size |\n",
    "| **Heteroscedasticity** | Transform the target (log), use weighted least squares, use robust standard errors |\n",
    "| **Multicollinearity** | Remove one of the correlated features, use PCA, use Ridge regression (L2 regularization) |\n",
    "| **Non-independence** | Use time-series models (for temporal data), mixed-effects models (for grouped data) |\n",
    "\n",
    "**Key takeaway:** Diagnostic plots are not just academic exercises. They tell you whether you can trust your model's predictions and coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It's a Problem | Fix |\n",
    "|---|---|---|\n",
    "| Ignoring residual plots | You miss violations that make coefficients unreliable | Always create residual vs fitted and Q-Q plots |\n",
    "| Assuming linear regression always works | Many real relationships are nonlinear | Check linearity assumption first |\n",
    "| Reporting R2 without diagnostics | High R2 can still have violated assumptions | R2 is necessary but not sufficient |\n",
    "| Ignoring multicollinearity | Coefficients become uninterpretable | Check correlation matrix and VIF |\n",
    "| Only checking normality of features | The assumption is about **residuals**, not inputs | Check residual normality after fitting |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Generate a dataset with a known violation, fit a linear regression, and create diagnostic plots.\n",
    "\n",
    "Steps:\n",
    "1. Generate data where $y = 5 + 2x^2 + \\text{noise}$ (linearity violated)\n",
    "2. Fit a `LinearRegression` model\n",
    "3. Create a residual vs fitted plot and a Q-Q plot\n",
    "4. Describe what the plots reveal about the assumption violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise Solution ---\n",
    "\n",
    "# Step 1: Generate nonlinear data\n",
    "np.random.seed(42)\n",
    "X_ex = np.random.uniform(-3, 3, (150, 1))\n",
    "y_ex = 5 + 2 * X_ex.flatten() ** 2 + np.random.randn(150) * 1.5\n",
    "\n",
    "# Step 2: Fit linear regression\n",
    "model_ex = LinearRegression()\n",
    "model_ex.fit(X_ex, y_ex)\n",
    "y_pred_ex = model_ex.predict(X_ex)\n",
    "residuals_ex = y_ex - y_pred_ex\n",
    "\n",
    "# Step 3: Diagnostic plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scatter with fit line\n",
    "X_sorted_ex = np.sort(X_ex, axis=0)\n",
    "axes[0].scatter(X_ex, y_ex, alpha=0.6, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].plot(X_sorted_ex, model_ex.predict(X_sorted_ex), \"r-\", linewidth=2, label=\"Linear fit\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"Data + Linear Fit (Clearly Wrong)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Residual vs fitted\n",
    "axes[1].scatter(y_pred_ex, residuals_ex, alpha=0.6, edgecolors=\"k\", linewidths=0.5, color=\"salmon\")\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[1].set_xlabel(\"Fitted values\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"Residual vs Fitted (U-shaped = Nonlinearity)\")\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals_ex, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title(\"Q-Q Plot of Residuals\")\n",
    "axes[2].get_lines()[0].set_markerfacecolor(\"steelblue\")\n",
    "axes[2].get_lines()[0].set_alpha(0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Interpretation\n",
    "print(\"Diagnostic Summary:\")\n",
    "print(\"- Residual vs Fitted: Clear U-shaped curve -> linearity assumption violated\")\n",
    "print(\"- Q-Q Plot: Deviations from the line -> residuals are not perfectly normal\")\n",
    "print(\"- Fix: Add polynomial feature (x^2) or use a nonlinear model\")\n",
    "print(f\"\\nR2 with linear model: {model_ex.score(X_ex, y_ex):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}