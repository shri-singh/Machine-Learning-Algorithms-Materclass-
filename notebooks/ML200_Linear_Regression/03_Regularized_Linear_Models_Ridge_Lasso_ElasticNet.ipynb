{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models: Ridge, Lasso, and ElasticNet\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Explain why regularization is needed (overfitting, multicollinearity, feature selection)\n",
    "- Describe the difference between L1 (Lasso), L2 (Ridge), and ElasticNet penalties\n",
    "- Fit Ridge, Lasso, and ElasticNet models using sklearn\n",
    "- Visualize coefficient paths as the regularization strength varies\n",
    "- Use cross-validation (`RidgeCV`, `LassoCV`) to select the best alpha\n",
    "- Apply best practices: standardize features, use pipelines\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebooks 01-02 (Linear Regression basics and assumptions)\n",
    "- Understanding of overfitting and train/test splits\n",
    "- Basic familiarity with cross-validation\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Regularize?](#1-why-regularize)\n",
    "2. [Ridge Regression (L2)](#2-ridge-regression-l2)\n",
    "3. [Lasso Regression (L1)](#3-lasso-regression-l1)\n",
    "4. [ElasticNet (L1 + L2)](#4-elasticnet-l1--l2)\n",
    "5. [Comparing Coefficients: Ridge vs Lasso vs ElasticNet](#5-comparing-coefficients)\n",
    "6. [Coefficient Paths as Alpha Varies](#6-coefficient-paths-as-alpha-varies)\n",
    "7. [Cross-Validation for Alpha Selection](#7-cross-validation-for-alpha-selection)\n",
    "8. [Best Practices](#8-best-practices)\n",
    "9. [Common Mistakes](#9-common-mistakes)\n",
    "10. [Exercise](#10-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "    RidgeCV, LassoCV, ElasticNetCV\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Regularize?\n",
    "\n",
    "Standard linear regression minimizes the MSE cost function:\n",
    "\n",
    "$$J = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "This can lead to problems:\n",
    "\n",
    "- **Overfitting:** With many features (especially more features than samples), the model memorizes noise\n",
    "- **Multicollinearity:** Correlated features cause large, unstable coefficient estimates\n",
    "- **No feature selection:** OLS keeps all features, even irrelevant ones\n",
    "\n",
    "**Regularization** adds a penalty term to the cost function that discourages large coefficients, leading to simpler, more generalizable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with many features (some irrelevant)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Only first 5 features matter\n",
    "true_coefs = np.zeros(n_features)\n",
    "true_coefs[:5] = [3.0, -2.0, 1.5, -1.0, 0.5]\n",
    "\n",
    "y = X @ true_coefs + 2.0 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"Only features 0-4 have non-zero true coefficients.\")\n",
    "print(f\"Train: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Ridge Regression (L2)\n",
    "\n",
    "Ridge adds an **L2 penalty** (sum of squared coefficients) to the cost function:\n",
    "\n",
    "$$J_{\\text{Ridge}} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "**Key properties:**\n",
    "- Shrinks all coefficients toward zero, but **never exactly to zero**\n",
    "- Handles multicollinearity well\n",
    "- $\\alpha$ controls regularization strength (higher = more shrinkage)\n",
    "- Does **not** perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Ridge with different alpha values\n",
    "alphas_demo = [0.001, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(f\"{'Alpha':<10} {'Train R2':<12} {'Test R2':<12} {'Coef L2 Norm':<14}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for alpha in alphas_demo:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_s, y_train)\n",
    "    train_r2 = ridge.score(X_train_s, y_train)\n",
    "    test_r2 = ridge.score(X_test_s, y_test)\n",
    "    coef_norm = np.sqrt(np.sum(ridge.coef_ ** 2))\n",
    "    print(f\"{alpha:<10} {train_r2:<12.4f} {test_r2:<12.4f} {coef_norm:<14.4f}\")\n",
    "\n",
    "print(\"\\nAs alpha increases, coefficients shrink and the model becomes simpler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Lasso Regression (L1)\n",
    "\n",
    "Lasso adds an **L1 penalty** (sum of absolute values of coefficients):\n",
    "\n",
    "$$J_{\\text{Lasso}} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "**Key properties:**\n",
    "- Can shrink coefficients **exactly to zero** (automatic feature selection)\n",
    "- Useful when you suspect many features are irrelevant\n",
    "- May arbitrarily pick one feature from a group of correlated features\n",
    "- Key parameters: `alpha`, `max_iter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Lasso with different alpha values\n",
    "print(f\"{'Alpha':<10} {'Train R2':<12} {'Test R2':<12} {'Non-zero coefs':<16}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for alpha in [0.001, 0.01, 0.1, 0.5, 1.0]:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, random_state=42)\n",
    "    lasso.fit(X_train_s, y_train)\n",
    "    train_r2 = lasso.score(X_train_s, y_train)\n",
    "    test_r2 = lasso.score(X_test_s, y_test)\n",
    "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "    print(f\"{alpha:<10} {train_r2:<12.4f} {test_r2:<12.4f} {n_nonzero:<16}\")\n",
    "\n",
    "print(\"\\nHigher alpha -> more coefficients set to zero (feature selection).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ElasticNet (L1 + L2)\n",
    "\n",
    "ElasticNet combines both penalties:\n",
    "\n",
    "$$J_{\\text{ElasticNet}} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\left[ \\rho \\sum_{j=1}^{p} |w_j| + \\frac{(1-\\rho)}{2} \\sum_{j=1}^{p} w_j^2 \\right]$$\n",
    "\n",
    "Where $\\rho$ (`l1_ratio` in sklearn) controls the mix:\n",
    "- $\\rho = 1$: pure Lasso\n",
    "- $\\rho = 0$: pure Ridge\n",
    "- $0 < \\rho < 1$: combination\n",
    "\n",
    "**Key properties:**\n",
    "- Can select features (like Lasso) while handling correlated features (like Ridge)\n",
    "- Useful when features are grouped and correlated\n",
    "- Key parameters: `alpha`, `l1_ratio`, `max_iter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet with different l1_ratio values\n",
    "alpha_en = 0.1\n",
    "\n",
    "print(f\"ElasticNet (alpha={alpha_en}):\")\n",
    "print(f\"{'l1_ratio':<12} {'Train R2':<12} {'Test R2':<12} {'Non-zero coefs':<16}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for l1_ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    en = ElasticNet(alpha=alpha_en, l1_ratio=l1_ratio, max_iter=10000, random_state=42)\n",
    "    en.fit(X_train_s, y_train)\n",
    "    train_r2 = en.score(X_train_s, y_train)\n",
    "    test_r2 = en.score(X_test_s, y_test)\n",
    "    n_nonzero = np.sum(en.coef_ != 0)\n",
    "    print(f\"{l1_ratio:<12} {train_r2:<12.4f} {test_r2:<12.4f} {n_nonzero:<16}\")\n",
    "\n",
    "print(\"\\nHigher l1_ratio -> more Lasso-like (more zero coefficients).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparing Coefficients: Ridge vs Lasso vs ElasticNet <a id='5-comparing-coefficients'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all models with a fixed alpha\n",
    "alpha_compare = 0.1\n",
    "\n",
    "ols = LinearRegression()\n",
    "ridge = Ridge(alpha=alpha_compare)\n",
    "lasso = Lasso(alpha=alpha_compare, max_iter=10000, random_state=42)\n",
    "elastic = ElasticNet(alpha=alpha_compare, l1_ratio=0.5, max_iter=10000, random_state=42)\n",
    "\n",
    "models = {\"OLS\": ols, \"Ridge\": ridge, \"Lasso\": lasso, \"ElasticNet\": elastic}\n",
    "coef_dict = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_s, y_train)\n",
    "    coef_dict[name] = model.coef_\n",
    "\n",
    "# Plot coefficient comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x_pos = np.arange(n_features)\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x_pos - 1.5 * width, true_coefs, width, label=\"True\", color=\"black\", alpha=0.6)\n",
    "ax.bar(x_pos - 0.5 * width, coef_dict[\"OLS\"], width, label=\"OLS\", color=\"steelblue\", alpha=0.7)\n",
    "ax.bar(x_pos + 0.5 * width, coef_dict[\"Ridge\"], width, label=\"Ridge\", color=\"coral\", alpha=0.7)\n",
    "ax.bar(x_pos + 1.5 * width, coef_dict[\"Lasso\"], width, label=\"Lasso\", color=\"mediumseagreen\", alpha=0.7)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f\"w{i}\" for i in range(n_features)], fontsize=8)\n",
    "ax.set_ylabel(\"Coefficient value\")\n",
    "ax.set_title(f\"Coefficient Comparison (alpha={alpha_compare})\")\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lasso drives irrelevant feature coefficients (w5-w19) to exactly zero.\")\n",
    "print(\"Ridge shrinks all coefficients but keeps them non-zero.\")\n",
    "print(\"OLS assigns non-trivial values even to noise features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Coefficient Paths as Alpha Varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso coefficient path\n",
    "alphas = np.logspace(-4, 1, 100)\n",
    "lasso_coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso_temp = Lasso(alpha=a, max_iter=10000, random_state=42)\n",
    "    lasso_temp.fit(X_train_s, y_train)\n",
    "    lasso_coefs.append(lasso_temp.coef_)\n",
    "\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Lasso paths\n",
    "for i in range(n_features):\n",
    "    color = \"steelblue\" if true_coefs[i] != 0 else \"lightgray\"\n",
    "    lw = 2 if true_coefs[i] != 0 else 0.8\n",
    "    label = f\"w{i} (true={true_coefs[i]:.1f})\" if true_coefs[i] != 0 else None\n",
    "    axes[0].plot(alphas, lasso_coefs[:, i], color=color, linewidth=lw, label=label)\n",
    "\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].set_xlabel(\"Alpha (log scale)\")\n",
    "axes[0].set_ylabel(\"Coefficient value\")\n",
    "axes[0].set_title(\"Lasso Coefficient Paths\")\n",
    "axes[0].legend(loc=\"upper right\", fontsize=8)\n",
    "axes[0].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "# Ridge paths for comparison\n",
    "ridge_coefs = []\n",
    "for a in alphas:\n",
    "    ridge_temp = Ridge(alpha=a)\n",
    "    ridge_temp.fit(X_train_s, y_train)\n",
    "    ridge_coefs.append(ridge_temp.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "for i in range(n_features):\n",
    "    color = \"coral\" if true_coefs[i] != 0 else \"lightgray\"\n",
    "    lw = 2 if true_coefs[i] != 0 else 0.8\n",
    "    label = f\"w{i} (true={true_coefs[i]:.1f})\" if true_coefs[i] != 0 else None\n",
    "    axes[1].plot(alphas, ridge_coefs[:, i], color=color, linewidth=lw, label=label)\n",
    "\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_xlabel(\"Alpha (log scale)\")\n",
    "axes[1].set_ylabel(\"Coefficient value\")\n",
    "axes[1].set_title(\"Ridge Coefficient Paths\")\n",
    "axes[1].legend(loc=\"upper right\", fontsize=8)\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lasso (left): Coefficients drop to exactly zero as alpha increases.\")\n",
    "print(\"Ridge (right): Coefficients shrink smoothly toward zero but never reach it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Cross-Validation for Alpha Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models using cross-validation on the training set\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_s, y_train, cv=5,\n",
    "                             scoring=\"neg_mean_squared_error\")\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    model_results[name] = {\n",
    "        \"CV RMSE Mean\": rmse_scores.mean(),\n",
    "        \"CV RMSE Std\": rmse_scores.std(),\n",
    "        \"Test R2\": r2_score(y_test, model.predict(X_test_s))\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "print(\"Cross-Validation Comparison:\")\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in CV models to find best alpha\n",
    "\n",
    "# RidgeCV\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5)\n",
    "ridge_cv.fit(X_train_s, y_train)\n",
    "print(f\"RidgeCV:\")\n",
    "print(f\"  Best alpha: {ridge_cv.alpha_:.6f}\")\n",
    "print(f\"  Test R2:    {ridge_cv.score(X_test_s, y_test):.4f}\")\n",
    "\n",
    "# LassoCV\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_train_s, y_train)\n",
    "print(f\"\\nLassoCV:\")\n",
    "print(f\"  Best alpha:     {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"  Test R2:        {lasso_cv.score(X_test_s, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefs: {np.sum(lasso_cv.coef_ != 0)}/{n_features}\")\n",
    "\n",
    "# ElasticNetCV\n",
    "en_cv = ElasticNetCV(alphas=np.logspace(-4, 1, 50), l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                     cv=5, max_iter=10000, random_state=42)\n",
    "en_cv.fit(X_train_s, y_train)\n",
    "print(f\"\\nElasticNetCV:\")\n",
    "print(f\"  Best alpha:     {en_cv.alpha_:.6f}\")\n",
    "print(f\"  Best l1_ratio:  {en_cv.l1_ratio_:.2f}\")\n",
    "print(f\"  Test R2:        {en_cv.score(X_test_s, y_test):.4f}\")\n",
    "print(f\"  Non-zero coefs: {np.sum(en_cv.coef_ != 0)}/{n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Best Practices\n",
    "\n",
    "- **Always standardize features** before applying regularization — the penalty treats all features equally, so they must be on the same scale\n",
    "- **Use a Pipeline** to avoid data leakage (scaler must be fit only on training data)\n",
    "- **Use built-in CV models** (`RidgeCV`, `LassoCV`, `ElasticNetCV`) for efficient alpha selection\n",
    "- **Compare against OLS baseline** to confirm regularization helps\n",
    "- **Check the number of non-zero coefficients** in Lasso to understand feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: Pipeline with StandardScaler + regularized model\n",
    "pipe_lasso = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit on raw (unscaled) training data — pipeline handles scaling\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "y_pred_pipe = pipe_lasso.predict(X_test)\n",
    "\n",
    "print(f\"Pipeline (StandardScaler + LassoCV):\")\n",
    "print(f\"  Best alpha: {pipe_lasso.named_steps['lasso'].alpha_:.6f}\")\n",
    "print(f\"  Test R2:    {r2_score(y_test, y_pred_pipe):.4f}\")\n",
    "print(f\"  Test RMSE:  {np.sqrt(mean_squared_error(y_test, y_pred_pipe)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Common Mistakes\n",
    "\n",
    "| Mistake | Why It's a Problem | Fix |\n",
    "|---|---|---|\n",
    "| Not standardizing features | Regularization penalizes all coefficients equally; different scales = unfair penalty | Always use `StandardScaler` before regularized models |\n",
    "| Using a fixed alpha without tuning | Suboptimal regularization strength | Use `RidgeCV`, `LassoCV`, or `ElasticNetCV` |\n",
    "| Ignoring `max_iter` warnings | Lasso/ElasticNet may not converge | Increase `max_iter` (e.g., 10000) |\n",
    "| Scaling outside the pipeline | Leaks test set statistics into training | Put `StandardScaler` inside a `Pipeline` |\n",
    "| Using Lasso with highly correlated features | Lasso arbitrarily picks one, drops others | Use Ridge or ElasticNet instead |\n",
    "| Forgetting to compare with OLS | You may not need regularization at all | Always benchmark against unregularized model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Exercise\n",
    "\n",
    "**Task:** Use `RidgeCV` and `LassoCV` (inside pipelines) to find the best alpha for the dataset below. Compare their test R2 scores.\n",
    "\n",
    "Steps:\n",
    "1. Use the dataset generated below (30 features, only 8 are relevant)\n",
    "2. Build a Pipeline with `StandardScaler` + `RidgeCV` and another with `StandardScaler` + `LassoCV`\n",
    "3. Fit both on training data, predict on test data\n",
    "4. Report: best alpha, test R2, number of non-zero coefficients (for Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise Solution ---\n",
    "\n",
    "# Generate dataset\n",
    "np.random.seed(42)\n",
    "n_ex, p_ex = 150, 30\n",
    "X_ex = np.random.randn(n_ex, p_ex)\n",
    "true_coefs_ex = np.zeros(p_ex)\n",
    "true_coefs_ex[:8] = [4.0, -3.0, 2.5, -2.0, 1.5, -1.0, 0.7, -0.3]\n",
    "y_ex = X_ex @ true_coefs_ex + 1.0 + np.random.randn(n_ex) * 0.8\n",
    "\n",
    "X_ex_train, X_ex_test, y_ex_train, y_ex_test = train_test_split(\n",
    "    X_ex, y_ex, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: RidgeCV\n",
    "pipe_ridge = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5))\n",
    "])\n",
    "pipe_ridge.fit(X_ex_train, y_ex_train)\n",
    "y_pred_ridge = pipe_ridge.predict(X_ex_test)\n",
    "\n",
    "# Pipeline: LassoCV\n",
    "pipe_lasso_ex = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000, random_state=42))\n",
    "])\n",
    "pipe_lasso_ex.fit(X_ex_train, y_ex_train)\n",
    "y_pred_lasso = pipe_lasso_ex.predict(X_ex_test)\n",
    "\n",
    "# Results\n",
    "print(\"RidgeCV Results:\")\n",
    "print(f\"  Best alpha: {pipe_ridge.named_steps['ridge'].alpha_:.6f}\")\n",
    "print(f\"  Test R2:    {r2_score(y_ex_test, y_pred_ridge):.4f}\")\n",
    "\n",
    "lasso_coefs_ex = pipe_lasso_ex.named_steps['lasso'].coef_\n",
    "print(f\"\\nLassoCV Results:\")\n",
    "print(f\"  Best alpha:     {pipe_lasso_ex.named_steps['lasso'].alpha_:.6f}\")\n",
    "print(f\"  Test R2:        {r2_score(y_ex_test, y_pred_lasso):.4f}\")\n",
    "print(f\"  Non-zero coefs: {np.sum(lasso_coefs_ex != 0)}/{p_ex}\")\n",
    "print(f\"  (True non-zero: 8/{p_ex})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}