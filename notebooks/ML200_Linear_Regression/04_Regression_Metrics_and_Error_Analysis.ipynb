{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics and Error Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Compute and interpret MAE, MSE, RMSE, and R2\n",
    "- Choose the right metric for your problem (e.g., MAE vs RMSE)\n",
    "- Create and interpret residual plots and error distributions\n",
    "- Perform error analysis: identify where a model fails\n",
    "- Compare models against a baseline (DummyRegressor)\n",
    "- Build a model comparison table\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebooks 01-03 (Linear Regression, Assumptions, Regularization)\n",
    "- Familiarity with sklearn model fitting and prediction\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Metric Definitions and Formulas](#1-metric-definitions-and-formulas)\n",
    "2. [When to Use Which Metric](#2-when-to-use-which-metric)\n",
    "3. [Computing All Metrics](#3-computing-all-metrics)\n",
    "4. [Residual Analysis](#4-residual-analysis)\n",
    "5. [Error Analysis: Where Does the Model Fail?](#5-error-analysis-where-does-the-model-fail)\n",
    "6. [Baseline Comparison with DummyRegressor](#6-baseline-comparison-with-dummyregressor)\n",
    "7. [Model Comparison Table](#7-model-comparison-table)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Metric Definitions and Formulas\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "- Average absolute difference between actual and predicted values\n",
    "- Same units as the target variable\n",
    "- Robust to outliers (linear penalty)\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- Average squared difference\n",
    "- Units are squared (harder to interpret directly)\n",
    "- Penalizes large errors disproportionately\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "- Same units as the target variable\n",
    "- More sensitive to outliers than MAE\n",
    "- Most commonly reported metric\n",
    "\n",
    "### R-squared (R2 / Coefficient of Determination)\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "- Proportion of variance explained by the model\n",
    "- Range: $(-\\infty, 1]$; 1 = perfect, 0 = predicting the mean, negative = worse than mean\n",
    "- Unitless, allows comparison across datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. When to Use Which Metric\n",
    "\n",
    "| Metric | Best For | Sensitive to Outliers? | Units |\n",
    "|---|---|---|---|\n",
    "| **MAE** | When all errors should be weighted equally | No (linear penalty) | Same as target |\n",
    "| **MSE** | When large errors are especially bad | Yes (quadratic penalty) | Squared units |\n",
    "| **RMSE** | General purpose, interpretable | Yes | Same as target |\n",
    "| **R2** | Comparing models, explaining variance | Moderate | Unitless |\n",
    "\n",
    "**Rule of thumb:**\n",
    "- Use **MAE** when outliers should not dominate your evaluation (e.g., median house price)\n",
    "- Use **RMSE** when large errors are costly (e.g., energy demand forecasting)\n",
    "- Use **R2** for quick model comparison, but never in isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MAE vs RMSE sensitivity to outliers\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal errors\n",
    "errors_normal = np.random.randn(100) * 2\n",
    "# Errors with outliers\n",
    "errors_outlier = errors_normal.copy()\n",
    "errors_outlier[:5] = np.array([20, -25, 18, -22, 30])  # 5 extreme outliers\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, errors, title in zip(\n",
    "    axes,\n",
    "    [errors_normal, errors_outlier],\n",
    "    [\"Normal Errors\", \"Errors with Outliers\"]\n",
    "):\n",
    "    ax.hist(errors, bins=30, edgecolor=\"k\", alpha=0.7, color=\"steelblue\")\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors ** 2))\n",
    "    ax.axvline(mae, color=\"green\", linewidth=2, linestyle=\"--\", label=f\"MAE = {mae:.2f}\")\n",
    "    ax.axvline(rmse, color=\"red\", linewidth=2, linestyle=\"--\", label=f\"RMSE = {rmse:.2f}\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Error\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With outliers, RMSE increases much more than MAE.\")\n",
    "print(\"If RMSE >> MAE, your data likely has large outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Computing All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and fit a model\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "n_features = 8\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_w = np.array([3.0, -2.0, 1.5, 0.0, -1.0, 0.5, 0.0, 2.0])\n",
    "y = X @ true_w + 5.0 + np.random.randn(n_samples) * 1.5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Regression Metrics (Test Set):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  MSE:  {mse:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual computation to verify understanding\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "mae_manual = np.mean(np.abs(residuals))\n",
    "mse_manual = np.mean(residuals ** 2)\n",
    "rmse_manual = np.sqrt(mse_manual)\n",
    "ss_res = np.sum(residuals ** 2)\n",
    "ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "r2_manual = 1 - ss_res / ss_tot\n",
    "\n",
    "print(\"Manual verification:\")\n",
    "print(f\"  MAE:  {mae_manual:.4f} (sklearn: {mae:.4f})\")\n",
    "print(f\"  MSE:  {mse_manual:.4f} (sklearn: {mse:.4f})\")\n",
    "print(f\"  RMSE: {rmse_manual:.4f} (sklearn: {rmse:.4f})\")\n",
    "print(f\"  R2:   {r2_manual:.4f} (sklearn: {r2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "axes[0, 0].scatter(y_pred, residuals, alpha=0.6, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0, 0].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0, 0].set_xlabel(\"Fitted values\")\n",
    "axes[0, 0].set_ylabel(\"Residuals\")\n",
    "axes[0, 0].set_title(\"Residuals vs Fitted Values\")\n",
    "\n",
    "# 2. Actual vs Predicted\n",
    "axes[0, 1].scatter(y_test, y_pred, alpha=0.6, edgecolors=\"k\", linewidths=0.5)\n",
    "mn = min(y_test.min(), y_pred.min())\n",
    "mx = max(y_test.max(), y_pred.max())\n",
    "axes[0, 1].plot([mn, mx], [mn, mx], \"r--\", linewidth=2, label=\"Perfect prediction\")\n",
    "axes[0, 1].set_xlabel(\"Actual\")\n",
    "axes[0, 1].set_ylabel(\"Predicted\")\n",
    "axes[0, 1].set_title(\"Actual vs Predicted\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Residual distribution\n",
    "axes[1, 0].hist(residuals, bins=25, edgecolor=\"k\", alpha=0.7, color=\"steelblue\", density=True)\n",
    "from scipy import stats\n",
    "x_range = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[1, 0].plot(x_range, stats.norm.pdf(x_range, residuals.mean(), residuals.std()),\n",
    "                \"r-\", linewidth=2, label=\"Normal fit\")\n",
    "axes[1, 0].set_xlabel(\"Residual\")\n",
    "axes[1, 0].set_ylabel(\"Density\")\n",
    "axes[1, 0].set_title(\"Distribution of Residuals\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Q-Q Plot of Residuals\")\n",
    "axes[1, 1].get_lines()[0].set_markerfacecolor(\"steelblue\")\n",
    "axes[1, 1].get_lines()[0].set_alpha(0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What to look for:\")\n",
    "print(\"  - Residuals vs Fitted: random scatter = good; pattern = problem\")\n",
    "print(\"  - Actual vs Predicted: points near diagonal = accurate predictions\")\n",
    "print(\"  - Residual distribution: bell-shaped = good\")\n",
    "print(\"  - Q-Q plot: points on the line = normally distributed residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Error Analysis: Where Does the Model Fail?\n",
    "\n",
    "Beyond aggregate metrics, it is important to understand **when and where** the model makes large errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by feature range\n",
    "# Pick the most important feature (feature 0 with true weight 3.0)\n",
    "feature_idx = 0\n",
    "feature_values = X_test[:, feature_idx]\n",
    "abs_errors = np.abs(residuals)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute error vs feature value\n",
    "axes[0].scatter(feature_values, abs_errors, alpha=0.6, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].set_xlabel(f\"Feature {feature_idx} value\")\n",
    "axes[0].set_ylabel(\"Absolute error\")\n",
    "axes[0].set_title(f\"Absolute Error vs Feature {feature_idx}\")\n",
    "\n",
    "# Error by target magnitude (binned)\n",
    "df_errors = pd.DataFrame({\n",
    "    \"y_actual\": y_test,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"abs_error\": abs_errors\n",
    "})\n",
    "df_errors[\"y_bin\"] = pd.cut(df_errors[\"y_actual\"], bins=5)\n",
    "error_by_bin = df_errors.groupby(\"y_bin\", observed=True)[\"abs_error\"].agg([\"mean\", \"count\"])\n",
    "\n",
    "error_by_bin[\"mean\"].plot(kind=\"bar\", ax=axes[1], color=\"steelblue\",\n",
    "                           edgecolor=\"k\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Target value range\")\n",
    "axes[1].set_ylabel(\"Mean absolute error\")\n",
    "axes[1].set_title(\"Mean Absolute Error by Target Range\")\n",
    "axes[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Error analysis summary:\")\n",
    "print(error_by_bin.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify worst predictions\n",
    "df_worst = df_errors.nlargest(10, \"abs_error\")\n",
    "print(\"Top 10 worst predictions:\")\n",
    "print(df_worst[[\"y_actual\", \"y_pred\", \"abs_error\"]].to_string(index=False, float_format=\"%.3f\"))\n",
    "print(f\"\\nMedian absolute error: {df_errors['abs_error'].median():.3f}\")\n",
    "print(f\"90th percentile error: {df_errors['abs_error'].quantile(0.9):.3f}\")\n",
    "print(f\"Max absolute error:    {df_errors['abs_error'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Baseline Comparison with DummyRegressor\n",
    "\n",
    "**Always compare your model against a simple baseline.** A model that cannot beat predicting the mean is not useful.\n",
    "\n",
    "- `DummyRegressor(strategy=\"mean\")`: predicts the training set mean for every sample\n",
    "- `DummyRegressor(strategy=\"median\")`: predicts the median\n",
    "\n",
    "If your model's R2 is close to 0, it is barely better than guessing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: DummyRegressor\n",
    "dummy_mean = DummyRegressor(strategy=\"mean\")\n",
    "dummy_mean.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_mean.predict(X_test)\n",
    "\n",
    "dummy_median = DummyRegressor(strategy=\"median\")\n",
    "dummy_median.fit(X_train, y_train)\n",
    "y_pred_dummy_med = dummy_median.predict(X_test)\n",
    "\n",
    "print(\"Baseline Comparison:\")\n",
    "print(f\"{'Model':<22} {'MAE':<10} {'RMSE':<10} {'R2':<10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for name, preds in [\n",
    "    (\"DummyRegressor (mean)\", y_pred_dummy),\n",
    "    (\"DummyRegressor (median)\", y_pred_dummy_med),\n",
    "    (\"LinearRegression\", y_pred)\n",
    "]:\n",
    "    mae_val = mean_absolute_error(y_test, preds)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2_val = r2_score(y_test, preds)\n",
    "    print(f\"{name:<22} {mae_val:<10.4f} {rmse_val:<10.4f} {r2_val:<10.4f}\")\n",
    "\n",
    "print(\"\\nA useful model must significantly beat the DummyRegressor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for regularized models\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    \"Dummy (mean)\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge (alpha=1.0)\": Ridge(alpha=1.0),\n",
    "    \"Ridge (alpha=10.0)\": Ridge(alpha=10.0),\n",
    "    \"Lasso (alpha=0.01)\": Lasso(alpha=0.01, max_iter=10000, random_state=42),\n",
    "    \"Lasso (alpha=0.1)\": Lasso(alpha=0.1, max_iter=10000, random_state=42),\n",
    "}\n",
    "\n",
    "comparison_rows = []\n",
    "for name, mdl in models_to_compare.items():\n",
    "    mdl.fit(X_train_s, y_train)\n",
    "    preds = mdl.predict(X_test_s)\n",
    "\n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(mdl, X_train_s, y_train, cv=5,\n",
    "                                scoring=\"neg_mean_squared_error\")\n",
    "    cv_rmse = np.sqrt(-cv_scores).mean()\n",
    "\n",
    "    comparison_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_test, preds),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n",
    "        \"R2\": r2_score(y_test, preds),\n",
    "        \"CV RMSE\": cv_rmse,\n",
    "        \"Non-zero coefs\": np.sum(mdl.coef_ != 0) if hasattr(mdl, \"coef_\") else \"-\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index(\"Model\")\n",
    "print(\"Model Comparison Table:\")\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Filter to only numeric columns for plotting\n",
    "plot_df = comparison_df[[\"RMSE\", \"R2\"]].copy()\n",
    "plot_df[\"RMSE\"] = plot_df[\"RMSE\"].astype(float)\n",
    "plot_df[\"R2\"] = plot_df[\"R2\"].astype(float)\n",
    "\n",
    "# RMSE comparison\n",
    "plot_df[\"RMSE\"].plot(kind=\"barh\", ax=axes[0], color=\"steelblue\",\n",
    "                       edgecolor=\"k\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"RMSE (lower is better)\")\n",
    "axes[0].set_title(\"RMSE Comparison\")\n",
    "\n",
    "# R2 comparison\n",
    "plot_df[\"R2\"].plot(kind=\"barh\", ax=axes[1], color=\"coral\",\n",
    "                     edgecolor=\"k\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"R2 (higher is better)\")\n",
    "axes[1].set_title(\"R2 Comparison\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It's a Problem | Fix |\n",
    "|---|---|---|\n",
    "| Using R2 alone | R2 can be high even with biased predictions or violated assumptions | Always report MAE/RMSE and check residual plots |\n",
    "| Not checking residual patterns | A good R2 does not guarantee the model is appropriate | Always create residual vs fitted and Q-Q plots |\n",
    "| Ignoring the baseline | You cannot tell if a model is useful without a reference | Compare against `DummyRegressor` |\n",
    "| Comparing MAE and RMSE across different datasets | They depend on the scale of the target | Compare models on the **same** test set |\n",
    "| Reporting only training metrics | Overfitting inflates training scores | Always report test set or cross-validation metrics |\n",
    "| Not investigating worst predictions | Aggregate metrics hide important failure modes | Analyze errors by feature range and target magnitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Load the California Housing dataset, fit multiple models, and build a complete evaluation.\n",
    "\n",
    "Steps:\n",
    "1. Load the data and split 80/20 with `random_state=42`\n",
    "2. Fit: `DummyRegressor`, `LinearRegression`, `Ridge(alpha=1.0)`, `Lasso(alpha=0.01)`\n",
    "3. Compute MAE, RMSE, and R2 for each model on the test set\n",
    "4. Create a residual plot for the best model\n",
    "5. Identify which target ranges have the highest errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise Solution ---\n",
    "\n",
    "# Step 1: Load and split\n",
    "housing = fetch_california_housing()\n",
    "X_h = housing.data\n",
    "y_h = housing.target\n",
    "\n",
    "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(\n",
    "    X_h, y_h, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_h = StandardScaler()\n",
    "X_h_train_s = scaler_h.fit_transform(X_h_train)\n",
    "X_h_test_s = scaler_h.transform(X_h_test)\n",
    "\n",
    "# Step 2: Fit models\n",
    "exercise_models = {\n",
    "    \"Dummy (mean)\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge (alpha=1.0)\": Ridge(alpha=1.0),\n",
    "    \"Lasso (alpha=0.01)\": Lasso(alpha=0.01, max_iter=10000, random_state=42),\n",
    "}\n",
    "\n",
    "# Step 3: Evaluate\n",
    "ex_rows = []\n",
    "ex_preds = {}\n",
    "\n",
    "for name, mdl in exercise_models.items():\n",
    "    mdl.fit(X_h_train_s, y_h_train)\n",
    "    preds = mdl.predict(X_h_test_s)\n",
    "    ex_preds[name] = preds\n",
    "\n",
    "    ex_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mean_absolute_error(y_h_test, preds),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_h_test, preds)),\n",
    "        \"R2\": r2_score(y_h_test, preds)\n",
    "    })\n",
    "\n",
    "ex_df = pd.DataFrame(ex_rows).set_index(\"Model\")\n",
    "print(\"California Housing — Model Comparison:\")\n",
    "print(ex_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Residual plot for best model (LinearRegression or Ridge)\n",
    "best_name = ex_df[\"R2\"].idxmax()\n",
    "best_preds = ex_preds[best_name]\n",
    "best_residuals = y_h_test - best_preds\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(best_preds, best_residuals, alpha=0.3, s=10, edgecolors=\"k\", linewidths=0.2)\n",
    "axes[0].axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Fitted values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(f\"Residuals vs Fitted ({best_name})\")\n",
    "\n",
    "axes[1].scatter(y_h_test, best_preds, alpha=0.3, s=10, edgecolors=\"k\", linewidths=0.2)\n",
    "mn = min(y_h_test.min(), best_preds.min())\n",
    "mx = max(y_h_test.max(), best_preds.max())\n",
    "axes[1].plot([mn, mx], [mn, mx], \"r--\", linewidth=2, label=\"Perfect\")\n",
    "axes[1].set_xlabel(\"Actual\")\n",
    "axes[1].set_ylabel(\"Predicted\")\n",
    "axes[1].set_title(f\"Actual vs Predicted ({best_name})\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Error by target range\n",
    "df_h_errors = pd.DataFrame({\n",
    "    \"y_actual\": y_h_test,\n",
    "    \"abs_error\": np.abs(best_residuals)\n",
    "})\n",
    "df_h_errors[\"target_bin\"] = pd.cut(df_h_errors[\"y_actual\"], bins=5)\n",
    "error_by_range = df_h_errors.groupby(\"target_bin\", observed=True)[\"abs_error\"].agg([\"mean\", \"median\", \"count\"])\n",
    "\n",
    "print(f\"Error Analysis by Target Range ({best_name}):\")\n",
    "print(error_by_range.round(4).to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "error_by_range[\"mean\"].plot(kind=\"bar\", ax=ax, color=\"steelblue\",\n",
    "                             edgecolor=\"k\", alpha=0.7)\n",
    "ax.set_xlabel(\"Target value range (house value in $100k)\")\n",
    "ax.set_ylabel(\"Mean Absolute Error\")\n",
    "ax.set_title(\"Where Does the Model Fail Most?\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe model tends to struggle most with high-value homes (> $3-4),\")\n",
    "print(\"which makes sense — there are fewer training examples at extremes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}