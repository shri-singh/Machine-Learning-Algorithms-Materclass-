{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Intuition and Math\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the difference between classification and regression problems\n",
    "2. Understand the sigmoid function and its role in logistic regression\n",
    "3. Derive and interpret log-odds and the decision boundary\n",
    "4. Write and interpret the binary cross-entropy loss function\n",
    "5. Use `sklearn.linear_model.LogisticRegression` with key parameters\n",
    "6. Visualize decision boundaries for 2D classification problems\n",
    "7. Distinguish between `predict` and `predict_proba`\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Linear algebra basics (dot products, vectors)\n",
    "- Linear regression concepts (weights, bias, fitting)\n",
    "- Python, NumPy, Matplotlib fundamentals\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Classification vs Regression](#1)\n",
    "2. [The Sigmoid Function](#2)\n",
    "3. [Log-Odds and Decision Boundary](#3)\n",
    "4. [Binary Cross-Entropy Loss](#4)\n",
    "5. [Logistic Regression with sklearn](#5)\n",
    "6. [Visualizing the Decision Boundary](#6)\n",
    "7. [predict vs predict_proba](#7)\n",
    "8. [Multi-class Classification (Brief)](#8)\n",
    "9. [Common Mistakes](#9)\n",
    "10. [Exercise](#10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Classification vs Regression\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|----------------|\n",
    "| Output | Continuous value (e.g., price, temperature) | Discrete category (e.g., spam/not spam, cat/dog) |\n",
    "| Goal | Predict a quantity | Predict a class label |\n",
    "| Loss | MSE, MAE | Cross-entropy, hinge loss |\n",
    "| Example | House price = \\$350,000 | Email is spam = Yes/No |\n",
    "\n",
    "**Why not just use linear regression for classification?**\n",
    "\n",
    "Linear regression outputs unbounded values $(-\\infty, +\\infty)$, but we need probabilities in $[0, 1]$. Logistic regression solves this by wrapping the linear output in a **sigmoid function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. The Sigmoid Function\n",
    "\n",
    "The sigmoid (logistic) function maps any real number to $(0, 1)$:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Properties:\n",
    "- $\\sigma(0) = 0.5$\n",
    "- $\\lim_{z \\to +\\infty} \\sigma(z) = 1$\n",
    "- $\\lim_{z \\to -\\infty} \\sigma(z) = 0$\n",
    "- Derivative: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-8, 8, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Basic sigmoid\n",
    "axes[0].plot(z, sigmoid(z), \"b-\", linewidth=2)\n",
    "axes[0].axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0].axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0].set_xlabel(\"z\", fontsize=12)\n",
    "axes[0].set_ylabel(r\"$\\sigma(z)$\", fontsize=12)\n",
    "axes[0].set_title(\"Sigmoid Function\", fontsize=14)\n",
    "axes[0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Effect of w and b: sigma(w*x + b)\n",
    "x = np.linspace(-5, 5, 200)\n",
    "for w, b, label in [(1, 0, \"w=1, b=0\"), (3, 0, \"w=3, b=0\"),\n",
    "                     (1, -2, \"w=1, b=-2\"), (1, 2, \"w=1, b=2\")]:\n",
    "    axes[1].plot(x, sigmoid(w * x + b), linewidth=2, label=label)\n",
    "\n",
    "axes[1].axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"x\", fontsize=12)\n",
    "axes[1].set_ylabel(\"P(y=1|x)\", fontsize=12)\n",
    "axes[1].set_title(\"Effect of w (steepness) and b (shift)\", fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"- Larger |w| makes the curve steeper (more confident predictions)\")\n",
    "print(\"- Changing b shifts the decision point (where P=0.5) left or right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Log-Odds and Decision Boundary\n",
    "\n",
    "Logistic regression models the **log-odds** (logit) as a linear function:\n",
    "\n",
    "$$\\log\\frac{p}{1 - p} = w^T x + b$$\n",
    "\n",
    "where $p = P(y = 1 | x)$.\n",
    "\n",
    "Solving for $p$:\n",
    "\n",
    "$$p = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n",
    "\n",
    "**Decision boundary**: the set of points where $p = 0.5$, which means $w^T x + b = 0$. In 2D, this is a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate log-odds\n",
    "p_values = np.linspace(0.01, 0.99, 200)\n",
    "log_odds = np.log(p_values / (1 - p_values))\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(p_values, log_odds, \"b-\", linewidth=2)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "plt.xlabel(\"Probability p\", fontsize=12)\n",
    "plt.ylabel(\"Log-odds = log(p / (1-p))\", fontsize=12)\n",
    "plt.title(\"Log-Odds (Logit) Function\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"When p=0.5, log-odds=0 => this is the decision boundary.\")\n",
    "print(\"When p>0.5, log-odds>0 => predict class 1.\")\n",
    "print(\"When p<0.5, log-odds<0 => predict class 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Binary Cross-Entropy Loss\n",
    "\n",
    "We cannot use MSE for logistic regression because the resulting loss surface is non-convex. Instead we use **binary cross-entropy** (log loss):\n",
    "\n",
    "$$J = -\\frac{1}{n}\\sum_{i=1}^{n}\\left[y_i \\log(\\hat{p}_i) + (1 - y_i)\\log(1 - \\hat{p}_i)\\right]$$\n",
    "\n",
    "Intuition:\n",
    "- When $y=1$: loss $= -\\log(\\hat{p})$. If $\\hat{p} \\to 1$, loss $\\to 0$. If $\\hat{p} \\to 0$, loss $\\to \\infty$.\n",
    "- When $y=0$: loss $= -\\log(1-\\hat{p})$. If $\\hat{p} \\to 0$, loss $\\to 0$. If $\\hat{p} \\to 1$, loss $\\to \\infty$.\n",
    "\n",
    "This loss is **convex**, so gradient descent finds the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-entropy loss for a single sample\n",
    "p_hat = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Case y=1\n",
    "loss_y1 = -np.log(p_hat)\n",
    "axes[0].plot(p_hat, loss_y1, \"r-\", linewidth=2)\n",
    "axes[0].set_xlabel(r\"$\\hat{p}$ (predicted probability)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "axes[0].set_title(r\"Loss when $y = 1$: $-\\log(\\hat{p})$\", fontsize=14)\n",
    "axes[0].set_ylim(0, 5)\n",
    "\n",
    "# Case y=0\n",
    "loss_y0 = -np.log(1 - p_hat)\n",
    "axes[1].plot(p_hat, loss_y0, \"b-\", linewidth=2)\n",
    "axes[1].set_xlabel(r\"$\\hat{p}$ (predicted probability)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Loss\", fontsize=12)\n",
    "axes[1].set_title(r\"Loss when $y = 0$: $-\\log(1 - \\hat{p})$\", fontsize=14)\n",
    "axes[1].set_ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The loss penalizes confident wrong predictions very heavily.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Logistic Regression with sklearn\n",
    "\n",
    "### Key Parameters of `LogisticRegression`\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `penalty` | `'l2'` | Regularization type: `'l1'`, `'l2'`, `'elasticnet'`, `None` |\n",
    "| `C` | `1.0` | Inverse regularization strength. Smaller C = stronger regularization |\n",
    "| `solver` | `'lbfgs'` | Optimization algorithm. Use `'liblinear'` for L1, `'saga'` for elasticnet |\n",
    "| `class_weight` | `None` | Set to `'balanced'` for imbalanced classes |\n",
    "| `max_iter` | `100` | Maximum iterations for solver convergence. Increase if you get convergence warnings |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data\n",
    "X, y = make_classification(\n",
    "    n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit logistic regression\n",
    "model = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Coefficients (w): {model.coef_[0]}\")\n",
    "print(f\"Intercept (b):    {model.intercept_[0]:.4f}\")\n",
    "print(f\"Train accuracy:   {model.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test accuracy:    {model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Visualizing the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"Plot decision boundary for a 2D classifier.\"\"\"\n",
    "    h = 0.02  # mesh step size\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"RdYlBu\")\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"blue\", label=\"Class 0\",\n",
    "                edgecolors=\"k\", alpha=0.7)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"red\", label=\"Class 1\",\n",
    "                edgecolors=\"k\", alpha=0.7)\n",
    "    plt.xlabel(\"Feature 1\", fontsize=12)\n",
    "    plt.ylabel(\"Feature 2\", fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X_test, y_test, \"Logistic Regression Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of regularization strength C\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, C_val in zip(axes, [0.01, 1.0, 100.0]):\n",
    "    m = LogisticRegression(C=C_val, solver=\"lbfgs\", max_iter=200, random_state=42)\n",
    "    m.fit(X_train, y_train)\n",
    "\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = m.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=\"RdYlBu\")\n",
    "    ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n",
    "               c=\"blue\", edgecolors=\"k\", alpha=0.7)\n",
    "    ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n",
    "               c=\"red\", edgecolors=\"k\", alpha=0.7)\n",
    "    ax.set_title(f\"C={C_val} | Acc={m.score(X_test, y_test):.3f}\", fontsize=13)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"Effect of Regularization Strength C\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Small C => strong regularization (simpler boundary, may underfit)\")\n",
    "print(\"Large C => weak regularization (complex boundary, may overfit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. predict vs predict_proba\n",
    "\n",
    "- `predict(X)`: returns the predicted class label (0 or 1) using threshold 0.5\n",
    "- `predict_proba(X)`: returns probability estimates for each class `[P(y=0), P(y=1)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predict vs predict_proba on a few test samples\n",
    "sample_indices = [0, 1, 2, 3, 4]\n",
    "X_sample = X_test[sample_indices]\n",
    "\n",
    "predictions = model.predict(X_sample)\n",
    "probabilities = model.predict_proba(X_sample)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"True Label\": y_test[sample_indices],\n",
    "    \"Predicted Label\": predictions,\n",
    "    \"P(class=0)\": probabilities[:, 0].round(4),\n",
    "    \"P(class=1)\": probabilities[:, 1].round(4),\n",
    "})\n",
    "print(results.to_string(index=False))\n",
    "print(\"\\nNote: predict() uses threshold=0.5 on P(class=1) by default.\")\n",
    "print(\"predict_proba() gives you the raw probabilities for more nuanced decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Multi-class Classification (Brief)\n",
    "\n",
    "Logistic regression naturally handles binary classification. For multi-class problems, sklearn uses:\n",
    "\n",
    "- **One-vs-Rest (OvR)**: Train one binary classifier per class. The class with the highest confidence wins. Set `multi_class='ovr'`.\n",
    "- **Multinomial (Softmax)**: Generalize the sigmoid to multiple classes using the softmax function. Set `multi_class='multinomial'` (default with `solver='lbfgs'`).\n",
    "\n",
    "In practice, sklearn handles this automatically when you pass labels with more than 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick multi-class example with all 3 Iris classes\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data[:, :2], iris.target  # use first 2 features for visualization\n",
    "\n",
    "model_multi = LogisticRegression(max_iter=200, random_state=42)\n",
    "model_multi.fit(X_iris, y_iris)\n",
    "\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Accuracy: {model_multi.score(X_iris, y_iris):.4f}\")\n",
    "print(f\"Coefficients shape: {model_multi.coef_.shape}  (one row per class)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Common Mistakes\n",
    "\n",
    "1. **Not scaling features**: Logistic regression with regularization is sensitive to feature scales. Always standardize your features (e.g., `StandardScaler`).\n",
    "\n",
    "2. **Ignoring convergence warnings**: If the solver does not converge, increase `max_iter` or scale your data.\n",
    "\n",
    "3. **Using accuracy alone**: Accuracy is misleading for imbalanced classes. Always check precision, recall, and F1.\n",
    "\n",
    "4. **Confusing `C` with regularization strength**: `C` is the *inverse* of regularization strength. Larger C = less regularization.\n",
    "\n",
    "5. **Treating probabilities as calibrated**: Raw `predict_proba` outputs are not always well-calibrated. See the calibration notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Exercise: Classify Iris (2 Classes)\n",
    "\n",
    "**Task**: Build a logistic regression classifier on a binary subset of the Iris dataset.\n",
    "\n",
    "1. Load the Iris dataset and keep only classes 0 and 1 (setosa and versicolor).\n",
    "2. Use all 4 features. Split 70/30 with `random_state=42`.\n",
    "3. Scale the features with `StandardScaler`.\n",
    "4. Fit `LogisticRegression` with `C=1.0` and report train/test accuracy.\n",
    "5. Print `predict_proba` for the first 5 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Load Iris, keep only classes 0 and 1\n",
    "iris = load_iris()\n",
    "mask = iris.target < 2\n",
    "X_ex, y_ex = iris.data[mask], iris.target[mask]\n",
    "print(f\"Dataset shape: {X_ex.shape}, Classes: {np.unique(y_ex)}\")\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_ex, y_ex, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "\n",
    "# Step 4: Fit and evaluate\n",
    "clf = LogisticRegression(C=1.0, max_iter=200, random_state=42)\n",
    "clf.fit(X_tr_s, y_tr)\n",
    "print(f\"Train accuracy: {clf.score(X_tr_s, y_tr):.4f}\")\n",
    "print(f\"Test accuracy:  {clf.score(X_te_s, y_te):.4f}\")\n",
    "\n",
    "# Step 5: Probabilities for first 5 test samples\n",
    "proba = clf.predict_proba(X_te_s[:5])\n",
    "print(\"\\nPredicted probabilities (first 5 test samples):\")\n",
    "print(pd.DataFrame(proba, columns=[\"P(setosa)\", \"P(versicolor)\"]).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}