{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics: Confusion Matrix, ROC, and PR Curves\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Construct and interpret a confusion matrix (TP, TN, FP, FN)\n",
    "2. Compute accuracy, precision, recall, specificity, and F1 score\n",
    "3. Plot and interpret ROC curves and compute AUC\n",
    "4. Plot and interpret Precision-Recall curves and PR-AUC\n",
    "5. Explain when accuracy is misleading and which metric to use instead\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Logistic regression basics (Notebook 01)\n",
    "- Binary classification concepts\n",
    "- Python, NumPy, Matplotlib fundamentals\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Confusion Matrix](#1)\n",
    "2. [Core Metrics](#2)\n",
    "3. [Computing Metrics with sklearn](#3)\n",
    "4. [ROC Curve and AUC](#4)\n",
    "5. [Precision-Recall Curve and PR-AUC](#5)\n",
    "6. [When Accuracy is Misleading](#6)\n",
    "7. [Common Mistakes](#7)\n",
    "8. [Exercise](#8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_curve, roc_auc_score, precision_recall_curve,\n",
    "    average_precision_score, classification_report\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data: Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train_s, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_s)\n",
    "y_proba = model.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Classes: {data.target_names} (0=malignant, 1=benign)\")\n",
    "print(f\"Class distribution (test): {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Confusion Matrix\n",
    "\n",
    "A confusion matrix summarizes prediction results for a binary classifier:\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Negative  Positive\n",
    "Actual Negative [   TN   |   FP   ]\n",
    "Actual Positive [   FN   |   TP   ]\n",
    "```\n",
    "\n",
    "- **TP (True Positive)**: Correctly predicted positive\n",
    "- **TN (True Negative)**: Correctly predicted negative\n",
    "- **FP (False Positive)**: Incorrectly predicted positive (Type I error)\n",
    "- **FN (False Negative)**: Incorrectly predicted negative (Type II error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"TN={tn}  FP={fp}\")\n",
    "print(f\"FN={fn}  TP={tp}\")\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred Malignant\", \"Pred Benign\"],\n",
    "            yticklabels=[\"Actual Malignant\", \"Actual Benign\"])\n",
    "plt.ylabel(\"Actual\", fontsize=12)\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Core Metrics\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Fraction of all predictions that are correct. Misleading for imbalanced data.\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Of all predicted positives, how many are actually positive? High precision = few false alarms.\n",
    "\n",
    "### Recall (Sensitivity / True Positive Rate)\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Of all actual positives, how many did we catch? High recall = few missed positives.\n",
    "\n",
    "### Specificity (True Negative Rate)\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "Of all actual negatives, how many did we correctly identify?\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Harmonic mean of precision and recall. Useful when you need a single metric that balances both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all metrics manually from confusion matrix values\n",
    "accuracy_manual = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision_manual = tp / (tp + fp)\n",
    "recall_manual = tp / (tp + fn)\n",
    "specificity_manual = tn / (tn + fp)\n",
    "f1_manual = 2 * precision_manual * recall_manual / (precision_manual + recall_manual)\n",
    "\n",
    "print(\"Manual calculations from confusion matrix:\")\n",
    "print(f\"  Accuracy:    {accuracy_manual:.4f}\")\n",
    "print(f\"  Precision:   {precision_manual:.4f}\")\n",
    "print(f\"  Recall:      {recall_manual:.4f}\")\n",
    "print(f\"  Specificity: {specificity_manual:.4f}\")\n",
    "print(f\"  F1 Score:    {f1_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Computing Metrics with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with sklearn\n",
    "print(\"sklearn metrics (should match manual):\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. ROC Curve and AUC\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve plots:\n",
    "- **x-axis**: False Positive Rate (FPR) $= \\frac{FP}{FP + TN} = 1 - \\text{Specificity}$\n",
    "- **y-axis**: True Positive Rate (TPR) $= \\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "The curve is generated by varying the classification threshold from 0 to 1.\n",
    "\n",
    "**AUC (Area Under the ROC Curve)**:\n",
    "- AUC = 1.0: perfect classifier\n",
    "- AUC = 0.5: random guessing (diagonal line)\n",
    "- AUC < 0.5: worse than random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b-\", linewidth=2, label=f\"Logistic Regression (AUC = {auc_score:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random (AUC = 0.5)\")\n",
    "plt.fill_between(fpr, tpr, alpha=0.1, color=\"blue\")\n",
    "plt.xlabel(\"False Positive Rate (FPR)\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate (TPR / Recall)\", fontsize=12)\n",
    "plt.title(\"ROC Curve\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC = {auc_score:.4f}\")\n",
    "print(\"Interpretation: the probability that a randomly chosen positive\")\n",
    "print(\"instance is ranked higher than a randomly chosen negative instance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Precision-Recall Curve and PR-AUC\n",
    "\n",
    "The **Precision-Recall (PR)** curve plots:\n",
    "- **x-axis**: Recall\n",
    "- **y-axis**: Precision\n",
    "\n",
    "**Why PR curves?** For imbalanced datasets, the ROC curve can be overly optimistic because TN (which are abundant) inflate the FPR denominator. The PR curve focuses only on the positive class and is more informative when positives are rare.\n",
    "\n",
    "**PR-AUC (Average Precision)**: area under the PR curve. Higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PR curve\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_vals, precision_vals, \"r-\", linewidth=2,\n",
    "         label=f\"Logistic Regression (PR-AUC = {pr_auc:.4f})\")\n",
    "# Baseline: fraction of positives\n",
    "baseline = y_test.mean()\n",
    "plt.axhline(y=baseline, color=\"k\", linestyle=\"--\", linewidth=1,\n",
    "            label=f\"Baseline (prevalence = {baseline:.3f})\")\n",
    "plt.fill_between(recall_vals, precision_vals, alpha=0.1, color=\"red\")\n",
    "plt.xlabel(\"Recall\", fontsize=12)\n",
    "plt.ylabel(\"Precision\", fontsize=12)\n",
    "plt.title(\"Precision-Recall Curve\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (PR-AUC) = {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC\n",
    "axes[0].plot(fpr, tpr, \"b-\", linewidth=2, label=f\"AUC = {auc_score:.4f}\")\n",
    "axes[0].plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "axes[0].set_xlabel(\"FPR\", fontsize=12)\n",
    "axes[0].set_ylabel(\"TPR\", fontsize=12)\n",
    "axes[0].set_title(\"ROC Curve\", fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR\n",
    "axes[1].plot(recall_vals, precision_vals, \"r-\", linewidth=2,\n",
    "             label=f\"PR-AUC = {pr_auc:.4f}\")\n",
    "axes[1].axhline(y=baseline, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "axes[1].set_xlabel(\"Recall\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Precision\", fontsize=12)\n",
    "axes[1].set_title(\"Precision-Recall Curve\", fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. When Accuracy is Misleading\n",
    "\n",
    "Consider a fraud detection dataset with 99% legitimate transactions and 1% fraud. A model that always predicts \"legitimate\" achieves **99% accuracy** but catches **zero fraud** (recall = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the accuracy trap with imbalanced data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "y_imb = np.array([0] * 990 + [1] * 10)  # 99% class 0, 1% class 1\n",
    "\n",
    "# \"Model\" that always predicts majority class\n",
    "y_pred_naive = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "print(\"Naive model (always predicts class 0):\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_imb, y_pred_naive):.4f}  (looks great!)\")\n",
    "print(f\"  Precision: {precision_score(y_imb, y_pred_naive, zero_division=0):.4f}  (undefined/0)\")\n",
    "print(f\"  Recall:    {recall_score(y_imb, y_pred_naive):.4f}  (catches nothing!)\")\n",
    "print(f\"  F1 Score:  {f1_score(y_imb, y_pred_naive):.4f}  (reveals the problem)\")\n",
    "print()\n",
    "print(\"Takeaway: for imbalanced data, ALWAYS look beyond accuracy.\")\n",
    "print(\"Use F1, PR-AUC, or the metric most relevant to your business problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Metric to Use?\n",
    "\n",
    "| Scenario | Recommended Metric | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| Balanced classes | Accuracy or F1 | Both are reliable |\n",
    "| Imbalanced classes | F1, PR-AUC | Accuracy is misleading |\n",
    "| Cost of FP is high (spam filter) | Precision | Minimize false alarms |\n",
    "| Cost of FN is high (cancer detection) | Recall | Minimize missed cases |\n",
    "| Ranking / probability quality | ROC-AUC | Threshold-independent |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Common Mistakes\n",
    "\n",
    "1. **Relying solely on accuracy**: Always compute precision, recall, and F1 -- especially for imbalanced data.\n",
    "\n",
    "2. **Confusing precision and recall**: Precision answers \"of my positive predictions, how many are right?\" Recall answers \"of all actual positives, how many did I find?\"\n",
    "\n",
    "3. **Using ROC-AUC for highly imbalanced data**: ROC can look good even when the model is poor. Use PR-AUC instead.\n",
    "\n",
    "4. **Ignoring the threshold**: All metrics except AUC depend on the chosen classification threshold. The default 0.5 is not always optimal.\n",
    "\n",
    "5. **Comparing AUC across different datasets**: AUC values are only comparable on the same test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Exercise: Compute Metrics on a Different Dataset\n",
    "\n",
    "**Task**: Use `sklearn.datasets.make_classification` to generate a binary dataset and compute all metrics.\n",
    "\n",
    "1. Generate 500 samples, 10 features, `weights=[0.7, 0.3]`, `random_state=42`.\n",
    "2. Split 70/30 with stratification.\n",
    "3. Scale with `StandardScaler`, fit `LogisticRegression`.\n",
    "4. Print the classification report.\n",
    "5. Plot both the ROC curve and PR curve side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ------------------\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Generate data\n",
    "X_ex, y_ex = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=5, n_redundant=2,\n",
    "    weights=[0.7, 0.3], random_state=42\n",
    ")\n",
    "print(f\"Class distribution: {np.bincount(y_ex)}\")\n",
    "\n",
    "# Step 2: Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ex, y_ex, test_size=0.3, random_state=42, stratify=y_ex\n",
    ")\n",
    "\n",
    "# Step 3: Scale and fit\n",
    "sc = StandardScaler()\n",
    "X_tr_s = sc.fit_transform(X_tr)\n",
    "X_te_s = sc.transform(X_te)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(X_tr_s, y_tr)\n",
    "\n",
    "y_p = clf.predict(X_te_s)\n",
    "y_prob = clf.predict_proba(X_te_s)[:, 1]\n",
    "\n",
    "# Step 4: Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_te, y_p))\n",
    "\n",
    "# Step 5: Plot ROC and PR curves\n",
    "fpr_ex, tpr_ex, _ = roc_curve(y_te, y_prob)\n",
    "auc_ex = roc_auc_score(y_te, y_prob)\n",
    "prec_ex, rec_ex, _ = precision_recall_curve(y_te, y_prob)\n",
    "pr_auc_ex = average_precision_score(y_te, y_prob)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(fpr_ex, tpr_ex, \"b-\", linewidth=2, label=f\"AUC = {auc_ex:.4f}\")\n",
    "axes[0].plot([0, 1], [0, 1], \"k--\")\n",
    "axes[0].set_xlabel(\"FPR\")\n",
    "axes[0].set_ylabel(\"TPR\")\n",
    "axes[0].set_title(\"ROC Curve\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(rec_ex, prec_ex, \"r-\", linewidth=2, label=f\"PR-AUC = {pr_auc_ex:.4f}\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Precision-Recall Curve\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}