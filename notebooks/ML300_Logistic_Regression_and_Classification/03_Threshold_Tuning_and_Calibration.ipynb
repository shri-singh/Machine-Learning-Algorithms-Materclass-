{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Tuning and Probability Calibration\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain why the default threshold of 0.5 is not always optimal\n",
    "2. Frame threshold selection as a business cost problem\n",
    "3. Plot precision, recall, and F1 as functions of the threshold\n",
    "4. Find the optimal threshold for different objectives (max F1, target recall)\n",
    "5. Understand what calibrated probabilities mean\n",
    "6. Create and interpret reliability diagrams (calibration curves)\n",
    "7. Apply Platt scaling and Isotonic calibration with `CalibratedClassifierCV`\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Logistic regression (Notebook 01)\n",
    "- Classification metrics: precision, recall, F1, ROC, PR (Notebook 02)\n",
    "- Python, NumPy, Matplotlib fundamentals\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Tune the Threshold?](#1)\n",
    "2. [Business Cost Framing](#2)\n",
    "3. [Metrics vs Threshold Plots](#3)\n",
    "4. [Finding the Optimal Threshold](#4)\n",
    "5. [Probability Calibration](#5)\n",
    "6. [Reliability Diagrams](#6)\n",
    "7. [Platt Scaling and Isotonic Calibration](#7)\n",
    "8. [Common Mistakes](#8)\n",
    "9. [Exercise](#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    precision_recall_curve, roc_curve, classification_report\n",
    ")\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train_s, y_train)\n",
    "\n",
    "y_proba = model.predict_proba(X_test_s)[:, 1]\n",
    "print(f\"Samples: train={len(y_train)}, test={len(y_test)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Why Tune the Threshold?\n",
    "\n",
    "By default, classifiers use a threshold of 0.5:\n",
    "- If $P(y=1|x) \\geq 0.5$, predict class 1\n",
    "- If $P(y=1|x) < 0.5$, predict class 0\n",
    "\n",
    "But 0.5 is arbitrary. The optimal threshold depends on:\n",
    "- **Class imbalance**: if positives are rare, a lower threshold catches more\n",
    "- **Business costs**: is a false positive or false negative more expensive?\n",
    "- **Target metric**: do you want to maximize F1, precision, or recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how different thresholds produce different predictions\n",
    "thresholds_demo = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Predicted+':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for t in thresholds_demo:\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "    p = precision_score(y_test, y_pred_t, zero_division=0)\n",
    "    r = recall_score(y_test, y_pred_t)\n",
    "    f = f1_score(y_test, y_pred_t)\n",
    "    n_pos = y_pred_t.sum()\n",
    "    print(f\"{t:<12.1f} {p:<12.4f} {r:<12.4f} {f:<12.4f} {n_pos:>12}\")\n",
    "\n",
    "print(\"\\nLower threshold => more positives predicted => higher recall, lower precision\")\n",
    "print(\"Higher threshold => fewer positives predicted => lower recall, higher precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Business Cost Framing\n",
    "\n",
    "Choosing a threshold is fundamentally a **business decision**:\n",
    "\n",
    "| Scenario | Cost of FP | Cost of FN | Strategy |\n",
    "|----------|-----------|-----------|----------|\n",
    "| **Spam filter** | High (lose important email) | Low (see a spam) | Raise threshold (favor precision) |\n",
    "| **Fraud detection** | Low (flag legitimate transaction) | Very high (miss fraud) | Lower threshold (favor recall) |\n",
    "| **Cancer screening** | Moderate (unnecessary biopsy) | Very high (miss cancer) | Lower threshold (favor recall) |\n",
    "| **Product recommendation** | Very low (irrelevant suggestion) | Low (missed sale) | Use default or optimize for engagement |\n",
    "\n",
    "The key question: **What costs more -- a false positive or a false negative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Metrics vs Threshold Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision, recall, and F1 as functions of threshold\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "    precisions.append(precision_score(y_test, y_pred_t, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred_t, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_t, zero_division=0))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, \"b-\", linewidth=2, label=\"Precision\")\n",
    "plt.plot(thresholds, recalls, \"r-\", linewidth=2, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, \"g-\", linewidth=2, label=\"F1 Score\")\n",
    "plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Default threshold (0.5)\")\n",
    "plt.xlabel(\"Threshold\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.title(\"Precision, Recall, and F1 vs Classification Threshold\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.show()\n",
    "\n",
    "print(\"As threshold increases: precision goes up, recall goes down.\")\n",
    "print(\"F1 peaks at the point that best balances precision and recall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Finding the Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective 1: Maximize F1 score\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds[best_f1_idx]\n",
    "best_f1 = f1_scores[best_f1_idx]\n",
    "\n",
    "print(\"=== Objective: Maximize F1 ===\")\n",
    "print(f\"  Optimal threshold: {best_f1_threshold:.2f}\")\n",
    "print(f\"  F1 at optimal:     {best_f1:.4f}\")\n",
    "print(f\"  Precision:         {precisions[best_f1_idx]:.4f}\")\n",
    "print(f\"  Recall:            {recalls[best_f1_idx]:.4f}\")\n",
    "\n",
    "# Objective 2: Target recall >= 0.95 with best precision\n",
    "target_recall = 0.95\n",
    "valid_indices = [i for i, r in enumerate(recalls) if r >= target_recall]\n",
    "if valid_indices:\n",
    "    # Among thresholds with recall >= target, pick highest threshold (best precision)\n",
    "    best_idx = max(valid_indices, key=lambda i: thresholds[i])\n",
    "    print(f\"\\n=== Objective: Recall >= {target_recall} ===\")\n",
    "    print(f\"  Optimal threshold: {thresholds[best_idx]:.2f}\")\n",
    "    print(f\"  Precision:         {precisions[best_idx]:.4f}\")\n",
    "    print(f\"  Recall:            {recalls[best_idx]:.4f}\")\n",
    "    print(f\"  F1:                {f1_scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold selection\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, \"g-\", linewidth=2, label=\"F1 Score\")\n",
    "plt.axvline(x=best_f1_threshold, color=\"green\", linestyle=\"--\", alpha=0.7,\n",
    "            label=f\"Best F1 threshold = {best_f1_threshold:.2f}\")\n",
    "plt.axvline(x=0.5, color=\"gray\", linestyle=\":\", alpha=0.7, label=\"Default (0.5)\")\n",
    "plt.scatter([best_f1_threshold], [best_f1], color=\"green\", s=100, zorder=5)\n",
    "plt.xlabel(\"Threshold\", fontsize=12)\n",
    "plt.ylabel(\"F1 Score\", fontsize=12)\n",
    "plt.title(\"Finding Optimal Threshold for Maximum F1\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Probability Calibration\n",
    "\n",
    "A classifier is **well-calibrated** if its predicted probabilities match observed frequencies:\n",
    "- Among all samples where the model predicts $P = 0.8$, approximately 80% should actually be positive.\n",
    "\n",
    "**Why does calibration matter?**\n",
    "- If you use probabilities for decision-making (e.g., \"treat if risk > 30%\"), they must be accurate.\n",
    "- If you only care about ranking (AUC), calibration is less critical.\n",
    "\n",
    "Logistic regression is generally well-calibrated by default (because it directly models probabilities). Other models (e.g., Random Forest, SVM) often need post-hoc calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Reliability Diagrams (Calibration Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reliability diagram\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, \"bo-\", linewidth=2, markersize=8,\n",
    "         label=\"Logistic Regression\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Probability\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives (Observed)\", fontsize=12)\n",
    "plt.title(\"Reliability Diagram (Calibration Curve)\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.show()\n",
    "\n",
    "print(\"If the curve closely follows the diagonal, the model is well-calibrated.\")\n",
    "print(\"Points above the diagonal: model is underconfident (actual prob > predicted).\")\n",
    "print(\"Points below the diagonal: model is overconfident (actual prob < predicted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also show the distribution of predicted probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration curve\n",
    "axes[0].plot(prob_pred, prob_true, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[0].plot([0, 1], [0, 1], \"k--\")\n",
    "axes[0].set_xlabel(\"Mean Predicted Probability\")\n",
    "axes[0].set_ylabel(\"Fraction of Positives\")\n",
    "axes[0].set_title(\"Calibration Curve\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "axes[1].hist(y_proba[y_test == 0], bins=20, alpha=0.6, label=\"Class 0\", color=\"blue\")\n",
    "axes[1].hist(y_proba[y_test == 1], bins=20, alpha=0.6, label=\"Class 1\", color=\"red\")\n",
    "axes[1].set_xlabel(\"Predicted Probability\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Distribution of Predicted Probabilities\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Platt Scaling and Isotonic Calibration\n",
    "\n",
    "Two common post-hoc calibration methods:\n",
    "\n",
    "### Platt Scaling (Sigmoid)\n",
    "- Fits a sigmoid function to map raw scores to calibrated probabilities.\n",
    "- Works well with small datasets. Assumes the calibration curve is S-shaped.\n",
    "- Use: `CalibratedClassifierCV(method='sigmoid')`\n",
    "\n",
    "### Isotonic Regression\n",
    "- Fits a non-parametric, monotone increasing function.\n",
    "- More flexible but needs more data to avoid overfitting.\n",
    "- Use: `CalibratedClassifierCV(method='isotonic')`\n",
    "\n",
    "Both use cross-validation internally to avoid overfitting to the calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uncalibrated vs calibrated models\n",
    "# Use a model that benefits more from calibration for demonstration\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train a GBM (often less calibrated than logistic regression)\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "gbm.fit(X_train_s, y_train)\n",
    "y_proba_gbm = gbm.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "# Calibrate with Platt scaling (sigmoid)\n",
    "gbm_sigmoid = CalibratedClassifierCV(gbm, method=\"sigmoid\", cv=5)\n",
    "gbm_sigmoid.fit(X_train_s, y_train)\n",
    "y_proba_sigmoid = gbm_sigmoid.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "# Calibrate with Isotonic regression\n",
    "gbm_isotonic = CalibratedClassifierCV(gbm, method=\"isotonic\", cv=5)\n",
    "gbm_isotonic.fit(X_train_s, y_train)\n",
    "y_proba_isotonic = gbm_isotonic.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "# Plot all three calibration curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name, proba, color in [\n",
    "    (\"GBM (uncalibrated)\", y_proba_gbm, \"blue\"),\n",
    "    (\"GBM + Platt (sigmoid)\", y_proba_sigmoid, \"green\"),\n",
    "    (\"GBM + Isotonic\", y_proba_isotonic, \"orange\"),\n",
    "    (\"Logistic Regression\", y_proba, \"purple\"),\n",
    "]:\n",
    "    prob_true_i, prob_pred_i = calibration_curve(y_test, proba, n_bins=10)\n",
    "    ax.plot(prob_pred_i, prob_true_i, \"o-\", label=name, color=color, linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "ax.set_xlabel(\"Mean Predicted Probability\", fontsize=12)\n",
    "ax.set_ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "ax.set_title(\"Calibration Comparison\", fontsize=14)\n",
    "ax.legend(fontsize=10, loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Logistic regression is typically well-calibrated out of the box.\")\n",
    "print(\"Tree-based models (GBM, RF) often benefit from post-hoc calibration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Using threshold 0.5 blindly**: The default threshold is rarely optimal. Always analyze your precision-recall tradeoff and business constraints.\n",
    "\n",
    "2. **Trusting uncalibrated probabilities**: Many models (SVMs, Random Forests, boosted trees) produce scores that are not true probabilities. Always check the calibration curve before using predicted probabilities in downstream decisions.\n",
    "\n",
    "3. **Tuning threshold on the test set**: The threshold should be selected on a validation set, not the test set. Otherwise you leak information and get optimistic estimates.\n",
    "\n",
    "4. **Ignoring class imbalance when setting threshold**: With 99% negatives, threshold 0.5 may predict almost everything as negative.\n",
    "\n",
    "5. **Calibrating with too little data**: Isotonic calibration in particular needs enough samples. With small datasets, prefer Platt scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Exercise\n",
    "\n",
    "**Task**: Work with a synthetic imbalanced dataset to practice threshold tuning.\n",
    "\n",
    "1. Generate data: `make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)`.\n",
    "2. Split, scale, and fit a logistic regression model.\n",
    "3. Plot precision, recall, and F1 vs threshold.\n",
    "4. Find the threshold that maximizes F1.\n",
    "5. Compare the classification report at threshold=0.5 vs your optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Generate imbalanced data\n",
    "X_ex, y_ex = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, n_redundant=2,\n",
    "    weights=[0.9, 0.1], random_state=42\n",
    ")\n",
    "print(f\"Class distribution: {np.bincount(y_ex)}\")\n",
    "\n",
    "# Step 2: Split, scale, fit\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_ex, y_ex, test_size=0.3,\n",
    "                                           random_state=42, stratify=y_ex)\n",
    "sc = StandardScaler()\n",
    "X_tr_s = sc.fit_transform(X_tr)\n",
    "X_te_s = sc.transform(X_te)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(X_tr_s, y_tr)\n",
    "y_prob_ex = clf.predict_proba(X_te_s)[:, 1]\n",
    "\n",
    "# Step 3: Plot metrics vs threshold\n",
    "ts = np.arange(0.0, 1.01, 0.01)\n",
    "p_list, r_list, f_list = [], [], []\n",
    "for t in ts:\n",
    "    yp = (y_prob_ex >= t).astype(int)\n",
    "    p_list.append(precision_score(y_te, yp, zero_division=0))\n",
    "    r_list.append(recall_score(y_te, yp, zero_division=0))\n",
    "    f_list.append(f1_score(y_te, yp, zero_division=0))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ts, p_list, \"b-\", linewidth=2, label=\"Precision\")\n",
    "plt.plot(ts, r_list, \"r-\", linewidth=2, label=\"Recall\")\n",
    "plt.plot(ts, f_list, \"g-\", linewidth=2, label=\"F1\")\n",
    "plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", label=\"Default (0.5)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Metrics vs Threshold (Imbalanced Data)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Find optimal F1 threshold\n",
    "best_idx = np.argmax(f_list)\n",
    "opt_t = ts[best_idx]\n",
    "print(f\"\\nOptimal F1 threshold: {opt_t:.2f} (F1={f_list[best_idx]:.4f})\")\n",
    "\n",
    "# Step 5: Compare reports\n",
    "print(\"\\n=== Classification Report at threshold=0.5 ===\")\n",
    "print(classification_report(y_te, (y_prob_ex >= 0.5).astype(int)))\n",
    "\n",
    "print(f\"=== Classification Report at threshold={opt_t:.2f} ===\")\n",
    "print(classification_report(y_te, (y_prob_ex >= opt_t).astype(int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}