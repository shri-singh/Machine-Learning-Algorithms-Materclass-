{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Classification: SMOTE and Class Weights\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain what class imbalance is and why it causes problems\n",
    "2. Demonstrate why accuracy fails for imbalanced datasets\n",
    "3. Apply `class_weight='balanced'` to handle imbalance in logistic regression\n",
    "4. Implement random undersampling and oversampling\n",
    "5. Understand and apply SMOTE (Synthetic Minority Oversampling Technique)\n",
    "6. Use stratified cross-validation and appropriate metrics for evaluation\n",
    "7. Avoid data leakage when resampling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Logistic regression (Notebook 01)\n",
    "- Classification metrics: precision, recall, F1, PR-AUC (Notebook 02)\n",
    "- Threshold tuning concepts (Notebook 03)\n",
    "- Python, NumPy, sklearn fundamentals\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is Class Imbalance?](#1)\n",
    "2. [Why Accuracy Fails](#2)\n",
    "3. [Strategy 1: Class Weights](#3)\n",
    "4. [Strategy 2: Random Undersampling](#4)\n",
    "5. [Strategy 3: Random Oversampling](#5)\n",
    "6. [Strategy 4: SMOTE](#6)\n",
    "7. [Comparing All Strategies](#7)\n",
    "8. [Evaluation Best Practices](#8)\n",
    "9. [Common Mistakes](#9)\n",
    "10. [Exercise](#10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, precision_recall_curve,\n",
    "    average_precision_score, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. What is Class Imbalance?\n",
    "\n",
    "**Class imbalance** occurs when one class significantly outnumbers the other(s). This is common in real-world problems:\n",
    "\n",
    "| Domain | Positive Class | Typical Prevalence |\n",
    "|--------|---------------|--------------------|\n",
    "| Fraud detection | Fraudulent transaction | 0.1% - 1% |\n",
    "| Medical diagnosis | Disease positive | 1% - 5% |\n",
    "| Spam detection | Spam email | 10% - 30% |\n",
    "| Churn prediction | Customer churns | 5% - 15% |\n",
    "| Click-through rate | User clicks ad | 0.5% - 3% |\n",
    "\n",
    "**Why is it a problem?** Standard classifiers optimize overall accuracy, so they learn to predict the majority class most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset with 10:1 ratio\n",
    "X, y = make_classification(\n",
    "    n_samples=2200, n_features=10, n_informative=5, n_redundant=2,\n",
    "    weights=[0.909, 0.091],  # ~10:1 ratio\n",
    "    flip_y=0.05, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Class 0 (majority): {np.sum(y == 0)} ({np.mean(y == 0):.1%})\")\n",
    "print(f\"Class 1 (minority): {np.sum(y == 1)} ({np.mean(y == 1):.1%})\")\n",
    "print(f\"Imbalance ratio: {np.sum(y == 0) / np.sum(y == 1):.1f}:1\")\n",
    "\n",
    "# Split data BEFORE any resampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTrain: {len(y_train)} samples (class 1: {np.sum(y_train == 1)})\")\n",
    "print(f\"Test:  {len(y_test)} samples (class 1: {np.sum(y_test == 1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "counts = np.bincount(y)\n",
    "axes[0].bar([\"Class 0 (Majority)\", \"Class 1 (Minority)\"], counts,\n",
    "            color=[\"steelblue\", \"coral\"])\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Class Distribution\")\n",
    "for i, v in enumerate(counts):\n",
    "    axes[0].text(i, v + 10, str(v), ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "# Show first 2 features\n",
    "axes[1].scatter(X[y == 0, 0], X[y == 0, 1], alpha=0.3, label=\"Class 0\", s=10)\n",
    "axes[1].scatter(X[y == 1, 0], X[y == 1, 1], alpha=0.7, label=\"Class 1\", s=30,\n",
    "                edgecolors=\"k\", linewidths=0.5)\n",
    "axes[1].set_xlabel(\"Feature 0\")\n",
    "axes[1].set_ylabel(\"Feature 1\")\n",
    "axes[1].set_title(\"Feature Space (first 2 features)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Why Accuracy Fails\n",
    "\n",
    "With a 10:1 imbalance ratio, a model that always predicts class 0 achieves ~91% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: always predict majority class\n",
    "y_pred_naive = np.zeros_like(y_test)\n",
    "naive_accuracy = np.mean(y_pred_naive == y_test)\n",
    "\n",
    "print(\"=== Naive Model (always predict class 0) ===\")\n",
    "print(f\"Accuracy: {naive_accuracy:.4f}  (looks decent!)\")\n",
    "print(f\"F1 (minority): {f1_score(y_test, y_pred_naive):.4f}  (zero -- useless!)\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_naive, zero_division=0))\n",
    "\n",
    "# Unweighted logistic regression\n",
    "model_default = LogisticRegression(max_iter=300, random_state=42)\n",
    "model_default.fit(X_train_s, y_train)\n",
    "y_pred_default = model_default.predict(X_test_s)\n",
    "\n",
    "print(\"\\n=== Default Logistic Regression (no class weights) ===\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, model_default.predict_proba(X_test_s)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Strategy 1: Class Weights\n",
    "\n",
    "Setting `class_weight='balanced'` adjusts the loss function to penalize misclassifications of the minority class more heavily.\n",
    "\n",
    "The weight for class $c$ is computed as:\n",
    "\n",
    "$$w_c = \\frac{n}{k \\cdot n_c}$$\n",
    "\n",
    "where $n$ = total samples, $k$ = number of classes, $n_c$ = samples in class $c$.\n",
    "\n",
    "This is the **simplest and most commonly used** approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with balanced class weights\n",
    "model_balanced = LogisticRegression(\n",
    "    class_weight=\"balanced\", max_iter=300, random_state=42\n",
    ")\n",
    "model_balanced.fit(X_train_s, y_train)\n",
    "y_pred_balanced = model_balanced.predict(X_test_s)\n",
    "\n",
    "print(\"=== Logistic Regression with class_weight='balanced' ===\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, model_balanced.predict_proba(X_test_s)[:, 1]):.4f}\")\n",
    "\n",
    "# Show the effective class weights\n",
    "n = len(y_train)\n",
    "k = 2\n",
    "for c in [0, 1]:\n",
    "    nc = np.sum(y_train == c)\n",
    "    w = n / (k * nc)\n",
    "    print(f\"\\nClass {c}: n_c={nc}, weight={w:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Strategy 2: Random Undersampling\n",
    "\n",
    "Randomly remove samples from the majority class until both classes are balanced.\n",
    "\n",
    "**Pros**: Fast, reduces training time.\n",
    "\n",
    "**Cons**: Discards potentially useful majority-class data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersample(X, y, random_state=42):\n",
    "    \"\"\"Undersample majority class to match minority class size.\"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        c_indices = np.where(y == c)[0]\n",
    "        sampled = rng.choice(c_indices, size=min_count, replace=False)\n",
    "        indices.extend(sampled)\n",
    "\n",
    "    indices = np.array(indices)\n",
    "    rng.shuffle(indices)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "X_under, y_under = random_undersample(X_train_s, y_train)\n",
    "print(f\"After undersampling: {len(y_under)} samples\")\n",
    "print(f\"Class distribution: {np.bincount(y_under)}\")\n",
    "\n",
    "model_under = LogisticRegression(max_iter=300, random_state=42)\n",
    "model_under.fit(X_under, y_under)\n",
    "y_pred_under = model_under.predict(X_test_s)\n",
    "\n",
    "print(\"\\n=== Logistic Regression with Undersampling ===\")\n",
    "print(classification_report(y_test, y_pred_under))\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, model_under.predict_proba(X_test_s)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Strategy 3: Random Oversampling\n",
    "\n",
    "Randomly duplicate samples from the minority class until both classes are balanced.\n",
    "\n",
    "**Pros**: No information loss from majority class.\n",
    "\n",
    "**Cons**: Exact duplicates can cause overfitting. Increases training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_oversample(X, y, random_state=42):\n",
    "    \"\"\"Oversample minority class to match majority class size.\"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    max_count = counts.max()\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for c in classes:\n",
    "        c_indices = np.where(y == c)[0]\n",
    "        if len(c_indices) < max_count:\n",
    "            # Oversample: sample with replacement\n",
    "            extra = rng.choice(c_indices, size=max_count - len(c_indices), replace=True)\n",
    "            all_indices = np.concatenate([c_indices, extra])\n",
    "        else:\n",
    "            all_indices = c_indices\n",
    "        X_list.append(X[all_indices])\n",
    "        y_list.append(y[all_indices])\n",
    "\n",
    "    X_res = np.vstack(X_list)\n",
    "    y_res = np.concatenate(y_list)\n",
    "\n",
    "    # Shuffle\n",
    "    shuffle_idx = rng.permutation(len(y_res))\n",
    "    return X_res[shuffle_idx], y_res[shuffle_idx]\n",
    "\n",
    "X_over, y_over = random_oversample(X_train_s, y_train)\n",
    "print(f\"After oversampling: {len(y_over)} samples\")\n",
    "print(f\"Class distribution: {np.bincount(y_over)}\")\n",
    "\n",
    "model_over = LogisticRegression(max_iter=300, random_state=42)\n",
    "model_over.fit(X_over, y_over)\n",
    "y_pred_over = model_over.predict(X_test_s)\n",
    "\n",
    "print(\"\\n=== Logistic Regression with Oversampling ===\")\n",
    "print(classification_report(y_test, y_pred_over))\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, model_over.predict_proba(X_test_s)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Strategy 4: SMOTE\n",
    "\n",
    "**SMOTE (Synthetic Minority Oversampling Technique)** creates new synthetic samples instead of duplicating existing ones:\n",
    "\n",
    "1. For each minority sample, find its k nearest neighbors (in the minority class).\n",
    "2. Randomly pick one neighbor.\n",
    "3. Create a new sample along the line segment between the original and the neighbor.\n",
    "\n",
    "This produces more diverse synthetic samples than random duplication.\n",
    "\n",
    "SMOTE requires the `imbalanced-learn` (imblearn) library. We try to import it; if unavailable, we demonstrate the concept with manual oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use SMOTE from imblearn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "    print(\"imblearn is available. Using SMOTE.\")\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    print(\"imblearn not installed. Will use manual oversampling as fallback.\")\n",
    "    print(\"To install: pip install imbalanced-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMBLEARN_AVAILABLE:\n",
    "    # SMOTE with imblearn\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_train_s, y_train)\n",
    "\n",
    "    print(f\"After SMOTE: {len(y_smote)} samples\")\n",
    "    print(f\"Class distribution: {np.bincount(y_smote)}\")\n",
    "\n",
    "    model_smote = LogisticRegression(max_iter=300, random_state=42)\n",
    "    model_smote.fit(X_smote, y_smote)\n",
    "    y_pred_smote = model_smote.predict(X_test_s)\n",
    "\n",
    "    print(\"\\n=== Logistic Regression with SMOTE ===\")\n",
    "    print(classification_report(y_test, y_pred_smote))\n",
    "    print(f\"PR-AUC: {average_precision_score(y_test, model_smote.predict_proba(X_test_s)[:, 1]):.4f}\")\n",
    "else:\n",
    "    # Fallback: manual oversampling with noise (poor man's SMOTE)\n",
    "    print(\"Using manual oversampling with added noise as SMOTE substitute.\")\n",
    "    rng = np.random.RandomState(42)\n",
    "    minority_idx = np.where(y_train == 1)[0]\n",
    "    majority_count = np.sum(y_train == 0)\n",
    "    minority_count = len(minority_idx)\n",
    "    n_synthetic = majority_count - minority_count\n",
    "\n",
    "    # Duplicate with small Gaussian noise\n",
    "    base_indices = rng.choice(minority_idx, size=n_synthetic, replace=True)\n",
    "    X_synthetic = X_train_s[base_indices] + rng.normal(0, 0.1, size=(n_synthetic, X_train_s.shape[1]))\n",
    "    y_synthetic = np.ones(n_synthetic, dtype=int)\n",
    "\n",
    "    X_smote = np.vstack([X_train_s, X_synthetic])\n",
    "    y_smote = np.concatenate([y_train, y_synthetic])\n",
    "\n",
    "    print(f\"After manual oversampling with noise: {len(y_smote)} samples\")\n",
    "    print(f\"Class distribution: {np.bincount(y_smote)}\")\n",
    "\n",
    "    model_smote = LogisticRegression(max_iter=300, random_state=42)\n",
    "    model_smote.fit(X_smote, y_smote)\n",
    "    y_pred_smote = model_smote.predict(X_test_s)\n",
    "\n",
    "    print(\"\\n=== Logistic Regression with Manual Oversampling + Noise ===\")\n",
    "    print(classification_report(y_test, y_pred_smote))\n",
    "    print(f\"PR-AUC: {average_precision_score(y_test, model_smote.predict_proba(X_test_s)[:, 1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If imblearn available, show the Pipeline approach (best practice)\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"classifier\", LogisticRegression(max_iter=300, random_state=42)),\n",
    "    ])\n",
    "\n",
    "    # Use stratified cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(pipeline, X_train_s, y_train, cv=cv, scoring=\"f1\")\n",
    "\n",
    "    print(\"SMOTE + LogisticRegression Pipeline (5-fold stratified CV):\")\n",
    "    print(f\"  F1 scores: {scores.round(4)}\")\n",
    "    print(f\"  Mean F1:   {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "    print(\"\\nNote: SMOTE is applied inside each CV fold, avoiding data leakage.\")\n",
    "else:\n",
    "    print(\"Skipping Pipeline demo (imblearn not available).\")\n",
    "    print(\"The key idea: always resample INSIDE the cross-validation loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Comparing All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect results for comparison\nfrom sklearn.metrics import precision_score, recall_score\n\nmodels = {\n    \"Default (no weights)\": model_default,\n    \"Class weight=balanced\": model_balanced,\n    \"Undersampling\": model_under,\n    \"Oversampling\": model_over,\n    \"SMOTE / Manual+Noise\": model_smote,\n}\n\nresults = []\nfor name, m in models.items():\n    yp = m.predict(X_test_s)\n    yprob = m.predict_proba(X_test_s)[:, 1]\n    results.append({\n        \"Method\": name,\n        \"Accuracy\": np.mean(yp == y_test),\n        \"F1 (minority)\": f1_score(y_test, yp),\n        \"Precision (minority)\": precision_score(y_test, yp),\n        \"Recall (minority)\": recall_score(y_test, yp),\n        \"PR-AUC\": average_precision_score(y_test, yprob),\n    })\n\ndf_results = pd.DataFrame(results)\nprint(df_results.to_string(index=False, float_format=\"{:.4f}\".format))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar chart of key metrics\n",
    "metrics_to_plot = [\"F1 (minority)\", \"PR-AUC\"]\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, df_results[\"F1 (minority)\"], width, label=\"F1 (minority)\", color=\"steelblue\")\n",
    "axes[0].bar(x + width/2, df_results[\"PR-AUC\"], width, label=\"PR-AUC\", color=\"coral\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_results[\"Method\"], rotation=30, ha=\"right\", fontsize=9)\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"F1 and PR-AUC Comparison\")\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "\n",
    "# PR curves for all models\n",
    "for name, m in models.items():\n",
    "    yprob = m.predict_proba(X_test_s)[:, 1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, yprob)\n",
    "    ap = average_precision_score(y_test, yprob)\n",
    "    axes[1].plot(rec, prec, linewidth=2, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Precision-Recall Curves\")\n",
    "axes[1].legend(fontsize=8, loc=\"lower left\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Evaluation Best Practices\n",
    "\n",
    "When working with imbalanced data:\n",
    "\n",
    "1. **Always use stratified cross-validation** (`StratifiedKFold`) to ensure each fold has the same class ratio.\n",
    "2. **Focus on F1 or PR-AUC**, not accuracy.\n",
    "3. **Report the full classification report** so both classes are visible.\n",
    "4. **Use stratified train/test splits** (`stratify=y` in `train_test_split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, m_class in [(\"Default\", LogisticRegression(max_iter=300, random_state=42)),\n",
    "                       (\"Balanced\", LogisticRegression(class_weight=\"balanced\", max_iter=300, random_state=42))]:\n",
    "    f1_scores_cv = cross_val_score(m_class, X_train_s, y_train, cv=cv, scoring=\"f1\")\n",
    "    print(f\"{name:12s} - 5-fold F1: {f1_scores_cv.mean():.4f} +/- {f1_scores_cv.std():.4f}  {f1_scores_cv.round(3)}\")\n",
    "\n",
    "print(\"\\nStratified CV ensures each fold preserves the class ratio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Common Mistakes\n",
    "\n",
    "1. **Oversampling before splitting (DATA LEAKAGE!)**: If you oversample or SMOTE the entire dataset before the train/test split, synthetic copies of test-set minority samples will appear in the training set. This leads to over-optimistic results. **Always split first, then resample only the training set.**\n",
    "\n",
    "2. **Using accuracy for imbalanced data**: A 99% accuracy on a 99:1 dataset means nothing. Always use F1, PR-AUC, or business-specific metrics.\n",
    "\n",
    "3. **Not using stratified splits**: Without stratification, a random split may put very few minority samples in the test set, giving unreliable metric estimates.\n",
    "\n",
    "4. **Applying SMOTE outside the CV loop**: When doing cross-validation, SMOTE must be applied inside each fold (use `imblearn.pipeline.Pipeline`). Otherwise you leak information across folds.\n",
    "\n",
    "5. **Ignoring the simplest solution**: `class_weight='balanced'` often works just as well as resampling with much less complexity. Try it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the data leakage problem\n",
    "print(\"=== Data Leakage Demo ===\")\n",
    "print(\"\\nWRONG approach: oversample entire dataset, then split\")\n",
    "X_over_all, y_over_all = random_oversample(X, y, random_state=42)\n",
    "X_tr_bad, X_te_bad, y_tr_bad, y_te_bad = train_test_split(\n",
    "    X_over_all, y_over_all, test_size=0.3, random_state=42\n",
    ")\n",
    "sc_bad = StandardScaler()\n",
    "X_tr_bad_s = sc_bad.fit_transform(X_tr_bad)\n",
    "X_te_bad_s = sc_bad.transform(X_te_bad)\n",
    "m_bad = LogisticRegression(max_iter=300, random_state=42).fit(X_tr_bad_s, y_tr_bad)\n",
    "print(f\"  F1 (looks great but LEAKED): {f1_score(y_te_bad, m_bad.predict(X_te_bad_s)):.4f}\")\n",
    "\n",
    "print(\"\\nCORRECT approach: split first, then oversample only training data\")\n",
    "print(f\"  F1 (honest): {f1_score(y_test, model_over.predict(X_test_s)):.4f}\")\n",
    "print(\"\\nThe leaked F1 is inflated because test data was used to create training samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Exercise\n",
    "\n",
    "**Task**: Handle a severely imbalanced dataset (20:1 ratio).\n",
    "\n",
    "1. Generate data: `make_classification(n_samples=2100, weights=[0.952, 0.048], n_features=8, n_informative=4, random_state=42)`.\n",
    "2. Split 70/30 with stratification.\n",
    "3. Scale features.\n",
    "4. Train three models: default, `class_weight='balanced'`, and manual oversampling.\n",
    "5. Compare F1 (minority class) and PR-AUC for all three.\n",
    "6. Which method works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Generate data\n",
    "X_ex, y_ex = make_classification(\n",
    "    n_samples=2100, weights=[0.952, 0.048], n_features=8,\n",
    "    n_informative=4, n_redundant=2, random_state=42\n",
    ")\n",
    "print(f\"Class distribution: {np.bincount(y_ex)}\")\n",
    "print(f\"Ratio: {np.sum(y_ex == 0) / np.sum(y_ex == 1):.1f}:1\")\n",
    "\n",
    "# Step 2: Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_ex, y_ex, test_size=0.3, random_state=42, stratify=y_ex\n",
    ")\n",
    "\n",
    "# Step 3: Scale\n",
    "sc = StandardScaler()\n",
    "X_tr_s = sc.fit_transform(X_tr)\n",
    "X_te_s = sc.transform(X_te)\n",
    "\n",
    "# Step 4: Three models\n",
    "# (a) Default\n",
    "m1 = LogisticRegression(max_iter=300, random_state=42).fit(X_tr_s, y_tr)\n",
    "\n",
    "# (b) Balanced weights\n",
    "m2 = LogisticRegression(class_weight=\"balanced\", max_iter=300, random_state=42).fit(X_tr_s, y_tr)\n",
    "\n",
    "# (c) Manual oversampling\n",
    "X_over_ex, y_over_ex = random_oversample(X_tr_s, y_tr, random_state=42)\n",
    "m3 = LogisticRegression(max_iter=300, random_state=42).fit(X_over_ex, y_over_ex)\n",
    "\n",
    "# Step 5: Compare\n",
    "print(f\"\\n{'Method':<25} {'F1 (minority)':>15} {'PR-AUC':>10}\")\n",
    "print(\"-\" * 52)\n",
    "for name, m in [(\"Default\", m1), (\"Balanced weights\", m2), (\"Oversampling\", m3)]:\n",
    "    yp = m.predict(X_te_s)\n",
    "    yprob = m.predict_proba(X_te_s)[:, 1]\n",
    "    f1 = f1_score(y_te, yp)\n",
    "    ap = average_precision_score(y_te, yprob)\n",
    "    print(f\"{name:<25} {f1:>15.4f} {ap:>10.4f}\")\n",
    "\n",
    "# Step 6: Conclusion\n",
    "print(\"\\nConclusion: class_weight='balanced' is often the simplest and most\")\n",
    "print(\"effective first approach for imbalanced classification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}