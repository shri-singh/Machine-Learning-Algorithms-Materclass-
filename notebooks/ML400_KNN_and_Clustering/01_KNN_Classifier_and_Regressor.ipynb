{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors: Classifier and Regressor\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain how KNN works as an instance-based (lazy) learner\n",
    "2. Understand distance metrics (Euclidean, Manhattan) and why feature scaling is critical\n",
    "3. Train a `KNeighborsClassifier` and `KNeighborsRegressor` using scikit-learn\n",
    "4. Visualize decision boundaries and analyze the bias-variance tradeoff with different `k` values\n",
    "5. Choose an optimal `k` using accuracy-vs-k analysis\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python fundamentals (loops, functions, basic data structures)\n",
    "- NumPy and Pandas basics\n",
    "- Familiarity with train/test splitting and basic classification concepts (ML100, ML300)\n",
    "- Matplotlib for plotting\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Theory: Instance-Based Learning](#1-theory-instance-based-learning)\n",
    "2. [Distance Metrics](#2-distance-metrics)\n",
    "3. [Why Scaling Is Critical](#3-why-scaling-is-critical)\n",
    "4. [KNN Classifier on the Iris Dataset](#4-knn-classifier-on-the-iris-dataset)\n",
    "5. [Decision Boundary Visualization](#5-decision-boundary-visualization)\n",
    "6. [Bias-Variance Tradeoff and Choosing k](#6-bias-variance-tradeoff-and-choosing-k)\n",
    "7. [KNN Regressor](#7-knn-regressor)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theory: Instance-Based Learning\n",
    "\n",
    "K-Nearest Neighbors (KNN) is an **instance-based** (or **lazy**) learning algorithm. Unlike models such as logistic regression or decision trees, KNN does **not** learn an explicit mapping from features to labels during training. Instead, it stores the entire training set and defers computation until prediction time.\n",
    "\n",
    "**How it works:**\n",
    "1. Store all training examples.\n",
    "2. For a new query point, compute the distance to every training example.\n",
    "3. Select the `k` closest neighbors.\n",
    "4. **Classification:** take a majority vote among the neighbors' labels.\n",
    "5. **Regression:** take the mean (or weighted mean) of the neighbors' target values.\n",
    "\n",
    "Because there is no explicit training phase, KNN is called a **lazy learner** -- all the work happens at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Distance Metrics\n",
    "\n",
    "KNN relies on a notion of \"closeness\". The two most common metrics are:\n",
    "\n",
    "**Euclidean distance** (L2):\n",
    "\n",
    "$$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "**Manhattan distance** (L1):\n",
    "\n",
    "$$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "Euclidean distance is the default in scikit-learn. Manhattan distance can be more robust when features have very different distributions or when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration of distance metrics\n",
    "a = np.array([1, 2])\n",
    "b = np.array([4, 6])\n",
    "\n",
    "euclidean = np.sqrt(np.sum((a - b) ** 2))\n",
    "manhattan = np.sum(np.abs(a - b))\n",
    "\n",
    "print(f\"Point A: {a}\")\n",
    "print(f\"Point B: {b}\")\n",
    "print(f\"Euclidean distance: {euclidean:.4f}\")\n",
    "print(f\"Manhattan distance: {manhattan:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Why Scaling Is Critical\n",
    "\n",
    "KNN uses distances to determine neighbors. If one feature has a much larger range than another, it will **dominate** the distance calculation. Feature scaling ensures all features contribute equally.\n",
    "\n",
    "Below we demonstrate KNN on the Iris dataset **with and without** scaling, using two features that have different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris and pick two features with different scales\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [0, 3]]  # sepal length (cm) vs petal width (cm)\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Feature 0 (sepal length) range: {X[:, 0].min():.1f} - {X[:, 0].max():.1f}\")\n",
    "print(f\"Feature 1 (petal width)  range: {X[:, 1].min():.1f} - {X[:, 1].max():.1f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without scaling\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Accuracy WITHOUT scaling: {acc_unscaled:.4f}\")\n",
    "print(f\"Accuracy WITH scaling:    {acc_scaled:.4f}\")\n",
    "print(\"\\nScaling can improve results by preventing features with larger ranges from dominating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. KNN Classifier on the Iris Dataset\n",
    "\n",
    "Key parameters of `KNeighborsClassifier`:\n",
    "- **`n_neighbors`** (default=5): number of neighbors to consider\n",
    "- **`metric`** (default='minkowski' with p=2, i.e., Euclidean): distance function\n",
    "- **`weights`** (default='uniform'): 'uniform' gives equal weight to all neighbors; 'distance' weights by inverse distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Iris dataset with all 4 features\n",
    "X_full = iris.data\n",
    "y_full = iris.target\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "scaler_f = StandardScaler()\n",
    "X_train_f_s = scaler_f.fit_transform(X_train_f)\n",
    "X_test_f_s = scaler_f.transform(X_test_f)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\", weights=\"uniform\")\n",
    "knn.fit(X_train_f_s, y_train_f)\n",
    "\n",
    "y_pred = knn.predict(X_test_f_s)\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_f, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_f, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Decision Boundary Visualization\n",
    "\n",
    "To visualize the decision boundary, we use only **2 features** so we can plot in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use petal length and petal width (features 2, 3) for cleaner separation\n",
    "X_2d = iris.data[:, 2:4]\n",
    "y_2d = iris.target\n",
    "\n",
    "scaler_2d = StandardScaler()\n",
    "X_2d_s = scaler_2d.fit_transform(X_2d)\n",
    "\n",
    "def plot_decision_boundary(X, y, model, title, ax):\n",
    "    \"\"\"Plot the decision boundary for a 2D feature space.\"\"\"\n",
    "    h = 0.05\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu,\n",
    "                         edgecolors=\"k\", s=30)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Petal length (scaled)\")\n",
    "    ax.set_ylabel(\"Petal width (scaled)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, k in enumerate([1, 5, 25]):\n",
    "    knn_viz = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_viz.fit(X_2d_s, y_2d)\n",
    "    plot_decision_boundary(X_2d_s, y_2d, knn_viz, f\"k = {k}\", axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice: k=1 is very jagged (overfitting), k=25 is very smooth (underfitting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Bias-Variance Tradeoff and Choosing k\n",
    "\n",
    "- **Small k** (e.g., k=1): low bias, high variance -- the model is sensitive to noise and overfits.\n",
    "- **Large k** (e.g., k=50): high bias, low variance -- the model is too smooth and underfits.\n",
    "\n",
    "We plot accuracy vs. k on both training and test sets to find the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d_s, y_2d, test_size=0.3, random_state=42, stratify=y_2d\n",
    ")\n",
    "\n",
    "k_values = range(1, 31)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_k.fit(X_train_2d, y_train_2d)\n",
    "    train_accs.append(accuracy_score(y_train_2d, knn_k.predict(X_train_2d)))\n",
    "    test_accs.append(accuracy_score(y_test_2d, knn_k.predict(X_test_2d)))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, train_accs, \"o-\", label=\"Train accuracy\")\n",
    "plt.plot(k_values, test_accs, \"s-\", label=\"Test accuracy\")\n",
    "plt.xlabel(\"k (number of neighbors)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. k -- Bias-Variance Tradeoff\")\n",
    "plt.xticks(list(k_values))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(test_accs)]\n",
    "print(f\"Best k on test set: {best_k} with accuracy {max(test_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. KNN Regressor\n",
    "\n",
    "`KNeighborsRegressor` predicts the **mean** of the k nearest neighbors' target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, k in enumerate([1, 5, 20]):\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_reg.fit(X_reg, y_reg)\n",
    "    X_plot = np.linspace(0, 5, 300).reshape(-1, 1)\n",
    "    y_plot = knn_reg.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X_reg, y_reg, color=\"darkorange\", s=20, label=\"data\")\n",
    "    axes[i].plot(X_plot, y_plot, color=\"navy\", linewidth=2, label=\"prediction\")\n",
    "    axes[i].set_title(f\"KNN Regressor (k={k})\")\n",
    "    axes[i].set_xlabel(\"X\")\n",
    "    axes[i].set_ylabel(\"y\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"k=1 overfits (jagged), k=20 underfits (too smooth), k=5 is a good middle ground.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Matters |\n",
    "|---|---|\n",
    "| **Not scaling features** | Features with larger ranges dominate the distance calculation. Always use `StandardScaler` or `MinMaxScaler`. |\n",
    "| **Using even k for binary classification** | Even k can lead to ties in majority voting. Use odd k for binary problems. |\n",
    "| **Applying KNN to high-dimensional data** | In high dimensions, distances become less meaningful (the \"curse of dimensionality\"). Consider dimensionality reduction first. |\n",
    "| **Choosing k without evaluation** | Always evaluate multiple k values on a validation set or with cross-validation. |\n",
    "| **Forgetting KNN is slow at prediction** | KNN must compute distances to all training points at inference. For large datasets, consider approximate methods or other algorithms. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Load the wine dataset (`sklearn.datasets.load_wine`). Split into train/test (70/30, `random_state=42`). Scale the features. Train a KNN classifier for k = 1, 3, 5, 7, 9, 11. Plot test accuracy vs. k and report the best k.\n",
    "\n",
    "Bonus: Try `weights='distance'` and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# 1. Load data\n",
    "# 2. Split into train/test\n",
    "# 3. Scale features\n",
    "# 4. Loop over k values and record test accuracy\n",
    "# 5. Plot accuracy vs. k\n",
    "# 6. Print best k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}