{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering and Cluster Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain unsupervised learning and the K-Means algorithm step by step\n",
    "2. Use `KMeans` from scikit-learn with key parameters (`n_clusters`, `init`, `n_init`, `max_iter`)\n",
    "3. Evaluate clustering quality using **inertia** and the **Elbow method**\n",
    "4. Compute and interpret the **Silhouette score**\n",
    "5. Recognize the limitations of K-Means\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python fundamentals and NumPy/Pandas basics\n",
    "- Matplotlib for plotting\n",
    "- Conceptual understanding of supervised learning (to contrast with unsupervised)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Theory: Unsupervised Learning and Clustering](#1-theory-unsupervised-learning-and-clustering)\n",
    "2. [The K-Means Algorithm](#2-the-k-means-algorithm)\n",
    "3. [K-Means in scikit-learn](#3-k-means-in-scikit-learn)\n",
    "4. [Inertia and the Elbow Method](#4-inertia-and-the-elbow-method)\n",
    "5. [Silhouette Score](#5-silhouette-score)\n",
    "6. [Silhouette Plots for Different k](#6-silhouette-plots-for-different-k)\n",
    "7. [Limitations of K-Means](#7-limitations-of-k-means)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theory: Unsupervised Learning and Clustering\n",
    "\n",
    "In **supervised learning**, we have labeled data (features + target). In **unsupervised learning**, we have only features -- no labels. The goal is to discover hidden structure in the data.\n",
    "\n",
    "**Clustering** is a key unsupervised task: group data points so that points in the same cluster are more similar to each other than to points in other clusters.\n",
    "\n",
    "Applications include:\n",
    "- Customer segmentation\n",
    "- Document grouping\n",
    "- Image compression\n",
    "- Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The K-Means Algorithm\n",
    "\n",
    "K-Means partitions data into **k** clusters by minimizing the within-cluster sum of squares (inertia).\n",
    "\n",
    "**Algorithm steps:**\n",
    "1. Choose k (number of clusters).\n",
    "2. Randomly initialize k **centroids**.\n",
    "3. **Assign** each point to the nearest centroid.\n",
    "4. **Update** each centroid to be the mean of all points assigned to it.\n",
    "5. Repeat steps 3-4 until centroids stop moving (or `max_iter` is reached).\n",
    "\n",
    "**Objective function (inertia):**\n",
    "\n",
    "$$J = \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in C_i} \\|\\mathbf{x} - \\boldsymbol{\\mu}_i\\|^2$$\n",
    "\n",
    "where $C_i$ is cluster $i$ and $\\boldsymbol{\\mu}_i$ is its centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. K-Means in scikit-learn\n",
    "\n",
    "Key parameters:\n",
    "- **`n_clusters`**: number of clusters (k)\n",
    "- **`init`**: initialization method -- `'k-means++'` (default, smart init) or `'random'`\n",
    "- **`n_init`**: number of times to run with different seeds (default=10); best result is kept\n",
    "- **`max_iter`**: max iterations per run (default=300)\n",
    "- **`random_state`**: for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic blob data\n",
    "X_blobs, y_true = make_blobs(\n",
    "    n_samples=300, centers=4, cluster_std=0.8, random_state=42\n",
    ")\n",
    "\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "plt.title(\"Synthetic Blobs (true labels)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans\n",
    "kmeans = KMeans(n_clusters=4, init=\"k-means++\", n_init=10, max_iter=300, random_state=42)\n",
    "labels = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[0].set_title(\"True Labels\")\n",
    "\n",
    "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                c=\"red\", marker=\"X\", s=200, edgecolors=\"k\", label=\"Centroids\")\n",
    "axes[1].set_title(\"KMeans Labels (k=4)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Inertia and the Elbow Method\n",
    "\n",
    "**Inertia** (within-cluster sum of squares) always decreases as k increases. The **Elbow method** looks for the k where adding more clusters gives diminishing returns -- the \"elbow\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_blobs)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertias, \"bo-\", linewidth=2)\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Inertia (within-cluster sum of squares)\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The 'elbow' is at k=4, matching the true number of clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Silhouette Score\n",
    "\n",
    "The **silhouette score** measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "For each sample $i$:\n",
    "- $a(i)$: mean distance to other points in the same cluster\n",
    "- $b(i)$: mean distance to points in the nearest neighboring cluster\n",
    "\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n",
    "\n",
    "- $s(i) \\approx 1$: well-clustered\n",
    "- $s(i) \\approx 0$: on the boundary\n",
    "- $s(i) < 0$: possibly assigned to the wrong cluster\n",
    "\n",
    "The overall score is the mean across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = []\n",
    "k_range_sil = range(2, 11)  # silhouette needs at least 2 clusters\n",
    "\n",
    "for k in k_range_sil:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_k = km.fit_predict(X_blobs)\n",
    "    score = silhouette_score(X_blobs, labels_k)\n",
    "    sil_scores.append(score)\n",
    "    print(f\"k={k}: Silhouette Score = {score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(k_range_sil), sil_scores, \"go-\", linewidth=2)\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs. k\")\n",
    "plt.xticks(list(k_range_sil))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Silhouette Plots for Different k\n",
    "\n",
    "A silhouette plot shows the silhouette coefficient for each sample, grouped by cluster. Well-formed clusters have uniformly thick \"knives\" above the average score line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, k in enumerate([3, 4, 5]):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_k = km.fit_predict(X_blobs)\n",
    "    sil_vals = silhouette_samples(X_blobs, labels_k)\n",
    "    avg_score = silhouette_score(X_blobs, labels_k)\n",
    "\n",
    "    ax = axes[idx]\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        cluster_sil = np.sort(sil_vals[labels_k == i])\n",
    "        y_upper = y_lower + len(cluster_sil)\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil,\n",
    "                         facecolor=color, alpha=0.7)\n",
    "        ax.text(-0.05, y_lower + 0.5 * len(cluster_sil), str(i))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax.axvline(x=avg_score, color=\"red\", linestyle=\"--\", label=f\"Avg: {avg_score:.3f}\")\n",
    "    ax.set_title(f\"Silhouette Plot (k={k})\")\n",
    "    ax.set_xlabel(\"Silhouette Coefficient\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"k=4 shows the most uniform silhouette widths, confirming it as the best choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Limitations of K-Means\n",
    "\n",
    "K-Means has several important limitations:\n",
    "\n",
    "1. **Assumes spherical (globular) clusters** -- struggles with elongated or irregular shapes.\n",
    "2. **Sensitive to initialization** -- `k-means++` helps but does not guarantee the global optimum.\n",
    "3. **Must specify k in advance** -- the elbow method helps, but it is not always clear-cut.\n",
    "4. **Sensitive to outliers** -- a single outlier can pull a centroid significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: KMeans fails on non-globular data\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "km_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_moons = km_moons.fit_predict(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[0].set_title(\"True Labels (moons)\")\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[1].set_title(\"KMeans Labels (k=2) -- FAILS on moons\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"KMeans cannot separate the two moons because they are not spherical clusters.\")\n",
    "print(\"We will see that DBSCAN handles this correctly in Notebook 04.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Matters |\n",
    "|---|---|\n",
    "| **Not scaling features** | Features with larger ranges dominate the Euclidean distance used by KMeans. Always scale first. |\n",
    "| **Choosing k without evaluation** | Always use the Elbow method and/or Silhouette score. Do not guess k. |\n",
    "| **Applying KMeans to non-globular data** | KMeans assumes spherical clusters. Use DBSCAN or hierarchical clustering for non-convex shapes. |\n",
    "| **Ignoring `n_init`** | Running KMeans only once may converge to a poor local minimum. Use `n_init >= 10`. |\n",
    "| **Interpreting cluster labels as meaningful** | KMeans labels (0, 1, 2...) are arbitrary -- they do not correspond to any ordering or ground truth. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Generate a dataset using `make_blobs` with `n_samples=500, centers=5, cluster_std=1.0, random_state=42`. Scale the data with `StandardScaler`. Run KMeans for k = 2 to 10. Plot both the Elbow curve (inertia) and the Silhouette score curve side by side. Identify the best k.\n",
    "\n",
    "Bonus: Create a silhouette plot for the best k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# 1. Generate blobs\n",
    "# 2. Scale data\n",
    "# 3. Loop over k = 2..10, compute inertia and silhouette score\n",
    "# 4. Plot both curves side by side\n",
    "# 5. Print the best k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}