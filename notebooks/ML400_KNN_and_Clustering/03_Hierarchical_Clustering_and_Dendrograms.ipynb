{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering and Dendrograms\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain agglomerative (bottom-up) and divisive (top-down) hierarchical clustering\n",
    "2. Understand linkage types: single, complete, average, and Ward\n",
    "3. Create and interpret dendrograms using `scipy.cluster.hierarchy`\n",
    "4. Use `AgglomerativeClustering` from scikit-learn\n",
    "5. Compare hierarchical clustering results with K-Means on the same data\n",
    "6. Decide when to use hierarchical clustering vs. K-Means\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Notebook 02 (K-Means and Cluster Evaluation)\n",
    "- Familiarity with distance metrics (Notebook 01)\n",
    "- NumPy, Matplotlib basics\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Theory: Agglomerative vs. Divisive](#1-theory-agglomerative-vs-divisive)\n",
    "2. [Linkage Types](#2-linkage-types)\n",
    "3. [Dendrograms](#3-dendrograms)\n",
    "4. [Cutting the Dendrogram](#4-cutting-the-dendrogram)\n",
    "5. [AgglomerativeClustering in scikit-learn](#5-agglomerativeclustering-in-scikit-learn)\n",
    "6. [Comparison with K-Means](#6-comparison-with-k-means)\n",
    "7. [When to Use Hierarchical vs. K-Means](#7-when-to-use-hierarchical-vs-k-means)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theory: Agglomerative vs. Divisive\n",
    "\n",
    "Hierarchical clustering builds a **tree of clusters** (dendrogram) rather than producing a flat partition.\n",
    "\n",
    "**Agglomerative (bottom-up):**\n",
    "1. Start with each point as its own cluster.\n",
    "2. Merge the two closest clusters.\n",
    "3. Repeat until all points are in one cluster.\n",
    "\n",
    "**Divisive (top-down):**\n",
    "1. Start with all points in one cluster.\n",
    "2. Split a cluster into two.\n",
    "3. Repeat until each point is its own cluster.\n",
    "\n",
    "Agglomerative is far more common and is the default in scikit-learn. Divisive is computationally more expensive and rarely used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y_true = make_blobs(n_samples=200, centers=4, cluster_std=0.6, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "plt.title(\"Synthetic Blobs (true labels)\")\n",
    "plt.xlabel(\"Feature 1 (scaled)\")\n",
    "plt.ylabel(\"Feature 2 (scaled)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Linkage Types\n",
    "\n",
    "When merging two clusters, the \"distance\" between them depends on the **linkage** criterion:\n",
    "\n",
    "| Linkage | Definition | Behavior |\n",
    "|---|---|---|\n",
    "| **Single** | Min distance between any two points in the two clusters | Can produce elongated, \"chaining\" clusters |\n",
    "| **Complete** | Max distance between any two points in the two clusters | Produces compact, equally-sized clusters |\n",
    "| **Average** | Mean distance between all pairs across clusters | Compromise between single and complete |\n",
    "| **Ward** | Minimizes increase in total within-cluster variance | Produces compact, similarly-sized clusters (most popular) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrices for each method\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "linkage_matrices = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    linkage_matrices[method] = linkage(X_scaled, method=method)\n",
    "\n",
    "print(\"Linkage matrices computed for:\", linkage_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dendrograms\n",
    "\n",
    "A **dendrogram** is a tree diagram showing the order of merges. The y-axis represents the distance (or dissimilarity) at which clusters merge. Taller vertical lines indicate bigger jumps in distance.\n",
    "\n",
    "**How to read a dendrogram:**\n",
    "- Leaf nodes at the bottom are individual data points.\n",
    "- The height at which two branches merge indicates how dissimilar those clusters are.\n",
    "- A large vertical gap suggests a natural number of clusters (cut there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for ax, method in zip(axes.ravel(), linkage_methods):\n",
    "    dendrogram(\n",
    "        linkage_matrices[method],\n",
    "        truncate_mode=\"lastp\",\n",
    "        p=30,\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=8,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"Dendrogram ({method} linkage)\")\n",
    "    ax.set_xlabel(\"Sample index (or cluster size)\")\n",
    "    ax.set_ylabel(\"Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Ward linkage typically produces the clearest separation for globular clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Cutting the Dendrogram\n",
    "\n",
    "To obtain a flat clustering, we \"cut\" the dendrogram at a chosen height (distance threshold) or specify the number of clusters. Below we demonstrate both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dendrogram with Ward linkage, showing a horizontal cut line\n",
    "Z_ward = linkage_matrices[\"ward\"]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram(Z_ward, truncate_mode=\"lastp\", p=30, leaf_rotation=90, leaf_font_size=8)\n",
    "plt.axhline(y=5.0, color=\"r\", linestyle=\"--\", linewidth=2, label=\"Cut at distance = 5.0\")\n",
    "plt.title(\"Ward Dendrogram with Cut Line\")\n",
    "plt.xlabel(\"Sample index (or cluster size)\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut by distance threshold\n",
    "labels_cut_dist = fcluster(Z_ward, t=5.0, criterion=\"distance\")\n",
    "print(f\"Cutting at distance=5.0 produces {len(np.unique(labels_cut_dist))} clusters\")\n",
    "\n",
    "# Cut by number of clusters\n",
    "labels_cut_k = fcluster(Z_ward, t=4, criterion=\"maxclust\")\n",
    "print(f\"Cutting at k=4 produces {len(np.unique(labels_cut_k))} clusters\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_cut_dist, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[0].set_title(f\"Cut by distance=5.0 ({len(np.unique(labels_cut_dist))} clusters)\")\n",
    "\n",
    "axes[1].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_cut_k, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[1].set_title(\"Cut by maxclust=4\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. AgglomerativeClustering in scikit-learn\n",
    "\n",
    "scikit-learn provides `AgglomerativeClustering` with a familiar `.fit_predict()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "labels_agg = agg.fit_predict(X_scaled)\n",
    "\n",
    "sil = silhouette_score(X_scaled, labels_agg)\n",
    "print(f\"AgglomerativeClustering (ward, k=4) Silhouette Score: {sil:.4f}\")\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_agg, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "plt.title(\"AgglomerativeClustering (Ward, k=4)\")\n",
    "plt.xlabel(\"Feature 1 (scaled)\")\n",
    "plt.ylabel(\"Feature 2 (scaled)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different linkage types with sklearn\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for ax, link in zip(axes, [\"single\", \"complete\", \"average\", \"ward\"]):\n",
    "    agg_link = AgglomerativeClustering(n_clusters=4, linkage=link)\n",
    "    labels_link = agg_link.fit_predict(X_scaled)\n",
    "    sil_link = silhouette_score(X_scaled, labels_link)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_link, cmap=\"viridis\", s=20, edgecolors=\"k\")\n",
    "    ax.set_title(f\"{link} (sil={sil_link:.3f})\")\n",
    "\n",
    "plt.suptitle(\"AgglomerativeClustering: Linkage Comparison\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comparison with K-Means\n",
    "\n",
    "Let us run both KMeans and AgglomerativeClustering (Ward) on the same data and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels_km = km.fit_predict(X_scaled)\n",
    "sil_km = silhouette_score(X_scaled, labels_km)\n",
    "\n",
    "agg_ward = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "labels_ward = agg_ward.fit_predict(X_scaled)\n",
    "sil_ward = silhouette_score(X_scaled, labels_ward)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[0].set_title(\"True Labels\")\n",
    "\n",
    "axes[1].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_km, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[1].set_title(f\"KMeans (sil={sil_km:.3f})\")\n",
    "\n",
    "axes[2].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_ward, cmap=\"viridis\", s=30, edgecolors=\"k\")\n",
    "axes[2].set_title(f\"Agglomerative Ward (sil={sil_ward:.3f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"For globular blobs, both methods perform similarly well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. When to Use Hierarchical vs. K-Means\n",
    "\n",
    "| Criterion | K-Means | Hierarchical |\n",
    "|---|---|---|\n",
    "| **Speed** | Fast (O(nk) per iteration) | Slow (O(n^2) or O(n^3)) |\n",
    "| **Scalability** | Handles large datasets well | Best for small to medium datasets |\n",
    "| **Number of clusters** | Must specify k in advance | Can explore different k via dendrogram |\n",
    "| **Cluster shape** | Assumes spherical | Ward assumes spherical; single linkage finds elongated shapes |\n",
    "| **Interpretability** | Centroids are easy to understand | Dendrogram shows hierarchy of relationships |\n",
    "| **Determinism** | Non-deterministic (depends on init) | Deterministic |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Matters |\n",
    "|---|---|\n",
    "| **Using single linkage on noisy data** | Single linkage is prone to the \"chaining effect\" -- noise can bridge two separate clusters into one. Use Ward or complete linkage instead. |\n",
    "| **Ignoring the dendrogram scale** | The y-axis of a dendrogram shows distances. Large jumps indicate natural cluster boundaries. Always inspect the scale. |\n",
    "| **Not scaling features** | Like K-Means, hierarchical clustering relies on distances. Always scale features first. |\n",
    "| **Running on very large datasets** | Hierarchical clustering is O(n^2) in memory and O(n^3) in time. For large n, use K-Means or mini-batch KMeans. |\n",
    "| **Choosing linkage blindly** | Different linkage types produce very different results. Always compare at least Ward vs. complete vs. average. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Generate data using `make_blobs(n_samples=150, centers=3, cluster_std=1.0, random_state=42)`. Scale the data. Create a Ward dendrogram and visually identify the best number of clusters. Then run `AgglomerativeClustering` with that k, compute the silhouette score, and visualize the result.\n",
    "\n",
    "Bonus: Compare the silhouette scores of single, complete, average, and Ward linkage for the same k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# 1. Generate blobs and scale\n",
    "# 2. Compute Ward linkage and plot dendrogram\n",
    "# 3. Choose k from the dendrogram\n",
    "# 4. Run AgglomerativeClustering with that k\n",
    "# 5. Compute silhouette score and visualize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}