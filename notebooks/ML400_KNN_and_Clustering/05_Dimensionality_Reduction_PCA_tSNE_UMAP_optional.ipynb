{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: PCA, t-SNE, and UMAP\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain PCA (Principal Component Analysis) and its role in dimensionality reduction\n",
    "2. Compute PCA, interpret explained variance, and create scree plots\n",
    "3. Use PCA as a preprocessing step before clustering\n",
    "4. *(Optional)* Understand the basics of t-SNE and UMAP for visualization\n",
    "5. Know when to use PCA vs. t-SNE vs. UMAP\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Notebooks 01-04 (KNN and clustering fundamentals)\n",
    "- Linear algebra intuition (eigenvectors, covariance -- explained below)\n",
    "- NumPy, Matplotlib, scikit-learn basics\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [PCA Theory: Variance Maximization](#1-pca-theory-variance-maximization)\n",
    "2. [PCA on the Iris Dataset](#2-pca-on-the-iris-dataset)\n",
    "3. [Scree Plot: Explained Variance](#3-scree-plot-explained-variance)\n",
    "4. [PCA as Preprocessing for Clustering](#4-pca-as-preprocessing-for-clustering)\n",
    "5. [t-SNE (Optional)](#5-t-sne-optional)\n",
    "6. [UMAP (Optional)](#6-umap-optional)\n",
    "7. [When to Use PCA vs. t-SNE vs. UMAP](#7-when-to-use-pca-vs-t-sne-vs-umap)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercise](#9-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PCA Theory: Variance Maximization\n",
    "\n",
    "**PCA** finds new axes (principal components) that capture the maximum variance in the data. It is a **linear** transformation that projects data from a high-dimensional space to a lower-dimensional one while preserving as much information as possible.\n",
    "\n",
    "### The Math\n",
    "\n",
    "Given a centered data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ (n samples, p features), PCA proceeds as:\n",
    "\n",
    "1. **Compute the covariance matrix:**\n",
    "\n",
    "$$\\mathbf{C} = \\frac{1}{n-1} \\mathbf{X}^T \\mathbf{X}$$\n",
    "\n",
    "2. **Eigendecomposition:**\n",
    "\n",
    "$$\\mathbf{C} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "\n",
    "where $\\mathbf{v}_i$ are the eigenvectors (principal component directions) and $\\lambda_i$ are the eigenvalues (variance captured by each component).\n",
    "\n",
    "3. **Project:** select the top $d$ eigenvectors (sorted by $\\lambda_i$ descending) and project:\n",
    "\n",
    "$$\\mathbf{X}_{\\text{reduced}} = \\mathbf{X} \\mathbf{V}_d$$\n",
    "\n",
    "where $\\mathbf{V}_d \\in \\mathbb{R}^{p \\times d}$ contains the top $d$ eigenvectors.\n",
    "\n",
    "### Explained Variance Ratio\n",
    "\n",
    "The fraction of total variance captured by the $i$-th component is:\n",
    "\n",
    "$$\\text{EVR}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. PCA on the Iris Dataset\n",
    "\n",
    "The Iris dataset has 4 features. We will reduce it to 2 dimensions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and scale\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Reduced shape: {X_pca.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance captured: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D scatter plot colored by true class\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i, name in enumerate(target_names):\n",
    "    mask = y == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=name, s=50, edgecolors=\"k\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "plt.title(\"PCA of Iris Dataset (2D)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Scree Plot: Explained Variance\n",
    "\n",
    "A **scree plot** shows how much variance each principal component captures. It helps decide how many components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components to see the full variance breakdown\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "evr = pca_full.explained_variance_ratio_\n",
    "cumulative_evr = np.cumsum(evr)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0].bar(range(1, len(evr) + 1), evr, color=\"steelblue\", edgecolor=\"k\")\n",
    "axes[0].set_xlabel(\"Principal Component\")\n",
    "axes[0].set_ylabel(\"Explained Variance Ratio\")\n",
    "axes[0].set_title(\"Scree Plot\")\n",
    "axes[0].set_xticks(range(1, len(evr) + 1))\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_evr) + 1), cumulative_evr, \"bo-\", linewidth=2)\n",
    "axes[1].axhline(y=0.95, color=\"r\", linestyle=\"--\", label=\"95% threshold\")\n",
    "axes[1].set_xlabel(\"Number of Components\")\n",
    "axes[1].set_ylabel(\"Cumulative Explained Variance\")\n",
    "axes[1].set_title(\"Cumulative Explained Variance\")\n",
    "axes[1].set_xticks(range(1, len(cumulative_evr) + 1))\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i, (ev, cev) in enumerate(zip(evr, cumulative_evr), 1):\n",
    "    print(f\"PC{i}: {ev:.4f} (cumulative: {cev:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. PCA as Preprocessing for Clustering\n",
    "\n",
    "When data has many features, PCA can reduce dimensionality **before** clustering. This helps:\n",
    "- Remove noise from less informative features\n",
    "- Speed up clustering\n",
    "- Mitigate the curse of dimensionality\n",
    "\n",
    "Below we use the digits dataset (64 features) and compare K-Means on the original data vs. PCA-reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset (8x8 images, 64 features)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "scaler_d = StandardScaler()\n",
    "X_digits_scaled = scaler_d.fit_transform(X_digits)\n",
    "\n",
    "print(f\"Digits dataset shape: {X_digits_scaled.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on original 64 features\n",
    "km_orig = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "labels_orig = km_orig.fit_predict(X_digits_scaled)\n",
    "sil_orig = silhouette_score(X_digits_scaled, labels_orig)\n",
    "\n",
    "# PCA to reduce to components that capture 95% variance\n",
    "pca_digits = PCA(n_components=0.95, random_state=42)\n",
    "X_digits_pca = pca_digits.fit_transform(X_digits_scaled)\n",
    "print(f\"PCA reduced 64 features to {X_digits_pca.shape[1]} components (95% variance)\")\n",
    "\n",
    "# K-Means on PCA-reduced features\n",
    "km_pca = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "labels_pca = km_pca.fit_predict(X_digits_pca)\n",
    "sil_pca = silhouette_score(X_digits_pca, labels_pca)\n",
    "\n",
    "print(f\"\\nSilhouette score (original 64 features): {sil_orig:.4f}\")\n",
    "print(f\"Silhouette score (PCA-reduced):            {sil_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA-reduced digits in 2D\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_digits_2d = pca_2d.fit_transform(X_digits_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_digits_2d[:, 0], X_digits_2d[:, 1],\n",
    "                      c=y_digits, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Digits Dataset: PCA 2D Projection\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. t-SNE (Optional)\n",
    "\n",
    "> **This section is OPTIONAL.** t-SNE is a visualization technique, not a general dimensionality reduction method.\n",
    "\n",
    "**t-SNE** (t-distributed Stochastic Neighbor Embedding) is a **non-linear** method that excels at creating 2D/3D visualizations of high-dimensional data.\n",
    "\n",
    "Key concepts:\n",
    "- Converts pairwise distances into probabilities (Gaussian in high-D, Student-t in low-D)\n",
    "- Minimizes the KL divergence between the two distributions\n",
    "- **Perplexity** (typical range: 5-50) controls the balance between local and global structure\n",
    "\n",
    "**Important caveats:**\n",
    "- t-SNE is **stochastic** -- different runs produce different results\n",
    "- **Distances between clusters are NOT meaningful** -- only within-cluster structure is reliable\n",
    "- Computationally expensive for large datasets\n",
    "- Should NOT be used as input for downstream ML models (use PCA instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: t-SNE on the digits dataset\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_digits_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                      c=y_digits, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.title(\"Digits Dataset: t-SNE 2D (perplexity=30)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"t-SNE often produces better-separated visual clusters than PCA.\")\n",
    "print(\"BUT: distances between clusters are NOT meaningful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Compare different perplexity values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, perp in zip(axes, [5, 30, 50]):\n",
    "    tsne_p = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "    X_tsne_p = tsne_p.fit_transform(X_digits_scaled)\n",
    "    ax.scatter(X_tsne_p[:, 0], X_tsne_p[:, 1], c=y_digits, cmap=\"tab10\", s=8, alpha=0.7)\n",
    "    ax.set_title(f\"t-SNE (perplexity={perp})\")\n",
    "\n",
    "plt.suptitle(\"Effect of Perplexity on t-SNE\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Lower perplexity emphasizes local structure; higher perplexity captures more global structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. UMAP (Optional)\n",
    "\n",
    "> **This section is OPTIONAL.** UMAP requires the `umap-learn` package.\n",
    "\n",
    "**UMAP** (Uniform Manifold Approximation and Projection) is a more recent non-linear technique that:\n",
    "- Is generally **faster** than t-SNE\n",
    "- Better preserves **global structure** (relative cluster positions)\n",
    "- Can be used for both visualization and as a general-purpose dimensionality reduction\n",
    "\n",
    "Key parameter: `n_neighbors` (similar to perplexity in t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: UMAP on the digits dataset\n",
    "try:\n",
    "    import umap\n",
    "    \n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=15, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_digits_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1],\n",
    "                          c=y_digits, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Digit\")\n",
    "    plt.xlabel(\"UMAP 1\")\n",
    "    plt.ylabel(\"UMAP 2\")\n",
    "    plt.title(\"Digits Dataset: UMAP 2D\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"UMAP tends to produce tighter, better-separated clusters than t-SNE.\")\n",
    "except ImportError:\n",
    "    print(\"UMAP is not installed. Install with: pip install umap-learn\")\n",
    "    print(\"Skipping UMAP demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. When to Use PCA vs. t-SNE vs. UMAP\n",
    "\n",
    "| Method | Type | Best For | Preserves | Speed |\n",
    "|---|---|---|---|---|\n",
    "| **PCA** | Linear | Preprocessing, feature reduction, denoising | Global structure, variance | Fast |\n",
    "| **t-SNE** | Non-linear | 2D/3D visualization | Local structure | Slow for large n |\n",
    "| **UMAP** | Non-linear | Visualization + general reduction | Local + some global | Faster than t-SNE |\n",
    "\n",
    "**Rules of thumb:**\n",
    "- Use **PCA** when you need a preprocessor for downstream models (regression, clustering, etc.)\n",
    "- Use **t-SNE** or **UMAP** when you want to visualize high-dimensional data in 2D\n",
    "- Use **UMAP** if you also want to preserve global structure or need speed\n",
    "- Do NOT use t-SNE/UMAP output as features for supervised learning (use PCA instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Matters |\n",
    "|---|---|\n",
    "| **Interpreting t-SNE distances as meaningful** | t-SNE does NOT preserve global distances. Two clusters far apart in the plot may or may not be far apart in the original space. |\n",
    "| **Not scaling before PCA** | PCA finds directions of maximum variance. If features are on different scales, the largest-scale feature dominates. Always standardize first. |\n",
    "| **Using t-SNE/UMAP for preprocessing** | These are primarily visualization tools (especially t-SNE). Use PCA for feature reduction before training models. |\n",
    "| **Keeping too few PCA components** | Check the cumulative explained variance. A common threshold is 95%. Keeping too few components loses important information. |\n",
    "| **Running t-SNE with default perplexity on all data** | Perplexity should scale with dataset size. Always try a few values (5, 30, 50) and compare. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Load the wine dataset (`sklearn.datasets.load_wine`). Scale the features. Apply PCA and create a scree plot to determine how many components capture 90% of the variance. Reduce to that number of components, then run K-Means (k=3) on the reduced data. Visualize the first 2 PCs colored by K-Means labels.\n",
    "\n",
    "Bonus: If t-SNE is available, create a 2D t-SNE visualization of the wine data colored by the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# 1. Load wine data and scale\n",
    "# 2. Fit PCA with all components, create scree plot\n",
    "# 3. Determine number of components for 90% variance\n",
    "# 4. Reduce dimensions, run KMeans(k=3)\n",
    "# 5. Visualize first 2 PCs colored by KMeans labels\n",
    "# 6. (Bonus) t-SNE visualization colored by true labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}