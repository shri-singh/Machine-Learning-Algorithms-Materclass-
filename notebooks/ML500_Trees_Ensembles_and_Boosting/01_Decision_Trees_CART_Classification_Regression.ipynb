{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML501: Decision Trees - CART for Classification & Regression\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the CART algorithm and recursive binary splitting\n",
    "2. Understand Gini impurity and entropy as classification split criteria\n",
    "3. Understand MSE as a regression split criterion\n",
    "4. Train and tune `DecisionTreeClassifier` and `DecisionTreeRegressor` in scikit-learn\n",
    "5. Visualize decision trees and interpret feature importances\n",
    "6. Identify and mitigate overfitting in decision trees\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python fundamentals (loops, functions, classes)\n",
    "- NumPy and pandas basics\n",
    "- Basic understanding of supervised learning (classification vs regression)\n",
    "- Familiarity with train/test splitting and model evaluation\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [CART Algorithm Theory](#1-cart-algorithm-theory)\n",
    "2. [Classification Split Criteria](#2-classification-split-criteria)\n",
    "3. [Regression Split Criteria](#3-regression-split-criteria)\n",
    "4. [Classification Demo: Iris Dataset](#4-classification-demo-iris-dataset)\n",
    "5. [Tree Visualization and Feature Importance](#5-tree-visualization-and-feature-importance)\n",
    "6. [Overfitting: Deep vs Pruned Trees](#6-overfitting-deep-vs-pruned-trees)\n",
    "7. [Regression Demo: Synthetic Data](#7-regression-demo-synthetic-data)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercises](#9-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CART Algorithm Theory\n",
    "\n",
    "**CART (Classification and Regression Trees)** builds a binary tree by recursively splitting the data.\n",
    "\n",
    "### Recursive Binary Splitting\n",
    "\n",
    "At each node, the algorithm:\n",
    "1. Considers every feature and every possible split point\n",
    "2. Selects the split that minimizes a cost function (impurity for classification, MSE for regression)\n",
    "3. Partitions the data into two child nodes\n",
    "4. Repeats until a stopping criterion is met (max depth, min samples, pure node, etc.)\n",
    "\n",
    "This is a **greedy** algorithm -- it picks the locally optimal split at each step without looking ahead.\n",
    "\n",
    "### Key Properties of Decision Trees\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| Non-parametric | No assumptions about data distribution |\n",
    "| Interpretable | Easy to visualize and explain |\n",
    "| No scaling needed | Features do not require normalization |\n",
    "| Handles mixed types | Works with numerical and categorical features |\n",
    "| Prone to overfitting | Can memorize training data if unconstrained |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Split Criteria\n",
    "\n",
    "For classification, CART uses impurity measures to decide the best split.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "$$G = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "where $p_k$ is the proportion of class $k$ in the node. Gini ranges from 0 (pure node) to $1 - 1/K$ (maximally impure).\n",
    "\n",
    "### Entropy (Information Gain)\n",
    "\n",
    "$$H = -\\sum_{k=1}^{K} p_k \\log_2 p_k$$\n",
    "\n",
    "Entropy ranges from 0 (pure) to $\\log_2 K$ (maximally impure).\n",
    "\n",
    "In practice, Gini and entropy produce very similar trees. Gini is the default in scikit-learn because it is slightly faster to compute (no logarithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gini vs Entropy for a binary classification problem\n",
    "p = np.linspace(0.001, 0.999, 200)\n",
    "gini = 1 - p**2 - (1 - p)**2\n",
    "entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(p, gini, label='Gini Impurity', linewidth=2)\n",
    "ax.plot(p, entropy, label='Entropy', linewidth=2)\n",
    "ax.set_xlabel('Proportion of Class 1 ($p$)')\n",
    "ax.set_ylabel('Impurity')\n",
    "ax.set_title('Gini Impurity vs Entropy (Binary Classification)')\n",
    "ax.legend()\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Max impurity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Split Criteria\n",
    "\n",
    "For regression trees, the split criterion is **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
    "\n",
    "At each node, the algorithm finds the split that minimizes the weighted sum of MSE in the two child nodes. The prediction at each leaf is the mean of the target values in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Demo: Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train a decision tree with limited depth for interpretability\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test accuracy:     {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters of `DecisionTreeClassifier`\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `criterion` | Split quality measure | `'gini'` (default), `'entropy'` |\n",
    "| `max_depth` | Maximum depth of the tree | 3-10 or `None` |\n",
    "| `min_samples_split` | Min samples to split a node | 2-20 |\n",
    "| `min_samples_leaf` | Min samples in a leaf node | 1-10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tree Visualization and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "plot_tree(\n",
    "    clf,\n",
    "    feature_names=feature_names,\n",
    "    class_names=target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree on Iris Dataset (max_depth=3)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "feature_imp = pd.Series(importances, index=feature_names).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "feature_imp.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Feature Importance (Gini)')\n",
    "ax.set_title('Feature Importance from Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for name, imp in zip(feature_names, importances):\n",
    "    print(f\"  {name}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting: Deep vs Pruned Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across different max_depth values\n",
    "depths = range(1, 16)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for d in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, tree.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, tree.predict(X_test)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(depths, train_scores, 'o-', label='Training Accuracy', linewidth=2)\n",
    "ax.plot(depths, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "ax.set_xlabel('max_depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Overfitting: Training vs Test Accuracy by Tree Depth')\n",
    "ax.legend()\n",
    "ax.set_xticks(list(depths))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: training accuracy reaches 1.0 quickly,\")\n",
    "print(\"but test accuracy plateaus or even decreases -- classic overfitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep tree vs pruned tree\n",
    "deep_tree = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "deep_tree.fit(X_train, y_train)\n",
    "\n",
    "pruned_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, random_state=42)\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Deep Tree (no constraints):\")\n",
    "print(f\"  Depth: {deep_tree.get_depth()}, Leaves: {deep_tree.get_n_leaves()}\")\n",
    "print(f\"  Train acc: {accuracy_score(y_train, deep_tree.predict(X_train)):.4f}\")\n",
    "print(f\"  Test acc:  {accuracy_score(y_test, deep_tree.predict(X_test)):.4f}\")\n",
    "print()\n",
    "print(\"Pruned Tree (max_depth=3, min_samples_leaf=5):\")\n",
    "print(f\"  Depth: {pruned_tree.get_depth()}, Leaves: {pruned_tree.get_n_leaves()}\")\n",
    "print(f\"  Train acc: {accuracy_score(y_train, pruned_tree.predict(X_train)):.4f}\")\n",
    "print(f\"  Test acc:  {accuracy_score(y_test, pruned_tree.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Demo: Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic non-linear data\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(5 * np.random.rand(200, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + 0.3 * np.random.randn(200)\n",
    "\n",
    "# Fit regression trees with different depths\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, depth in zip(axes, [2, 5, None]):\n",
    "    reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    reg.fit(X_reg, y_reg)\n",
    "    \n",
    "    X_test_reg = np.linspace(0, 5, 500).reshape(-1, 1)\n",
    "    y_pred = reg.predict(X_test_reg)\n",
    "    \n",
    "    ax.scatter(X_reg, y_reg, s=15, alpha=0.5, label='Data')\n",
    "    ax.plot(X_test_reg, y_pred, color='red', linewidth=2, label='Prediction')\n",
    "    depth_str = str(depth) if depth else 'None'\n",
    "    ax.set_title(f'max_depth={depth_str}')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('DecisionTreeRegressor: Effect of max_depth', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: underfitting (too shallow), Middle: good fit, Right: overfitting (too deep)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that decision trees cannot extrapolate\n",
    "np.random.seed(42)\n",
    "X_extrap = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "y_extrap = 2 * X_extrap.ravel() + 1 + 0.5 * np.random.randn(100)\n",
    "\n",
    "reg_extrap = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "reg_extrap.fit(X_extrap, y_extrap)\n",
    "\n",
    "# Predict beyond training range\n",
    "X_full = np.linspace(-1, 8, 300).reshape(-1, 1)\n",
    "y_pred_full = reg_extrap.predict(X_full)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.scatter(X_extrap, y_extrap, s=20, alpha=0.6, label='Training data')\n",
    "ax.plot(X_full, y_pred_full, color='red', linewidth=2, label='Tree prediction')\n",
    "ax.plot(X_full, 2 * X_full.ravel() + 1, 'g--', linewidth=1.5, label='True linear trend')\n",
    "ax.axvspan(-1, 0, alpha=0.1, color='red', label='Extrapolation zone')\n",
    "ax.axvspan(5, 8, alpha=0.1, color='red')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Decision Trees Cannot Extrapolate')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Outside the training range, the tree predicts a constant (the nearest leaf value).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Not limiting tree depth**: An unconstrained tree will memorize the training data, leading to poor generalization. Always set `max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "\n",
    "2. **Ignoring overfitting signals**: If training accuracy is much higher than test accuracy, the tree is too complex. Use cross-validation to select hyperparameters.\n",
    "\n",
    "3. **Using trees for extrapolation**: Decision trees predict constant values in each leaf. They cannot extrapolate beyond the range of training data -- predictions outside that range are simply the nearest leaf's mean.\n",
    "\n",
    "4. **Assuming feature importance equals causation**: A feature being \"important\" in the tree means it is useful for splitting, not that it causes the outcome.\n",
    "\n",
    "5. **Ignoring class imbalance**: Decision trees can be biased toward the majority class. Use `class_weight='balanced'` or resample the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "### Exercise 1: Entropy vs Gini\n",
    "Train two `DecisionTreeClassifier` models on the Iris dataset -- one with `criterion='gini'` and one with `criterion='entropy'`. Compare their test accuracy and tree structure (depth, number of leaves). Are the results meaningfully different?\n",
    "\n",
    "### Exercise 2: Hyperparameter Tuning\n",
    "Use `GridSearchCV` to find the best combination of `max_depth` (1-10), `min_samples_split` (2, 5, 10), and `min_samples_leaf` (1, 3, 5) for a `DecisionTreeClassifier` on the Iris dataset. Report the best parameters and CV score.\n",
    "\n",
    "### Exercise 3: Regression Tree\n",
    "Generate a noisy quadratic dataset: `y = 2*x^2 - 3*x + 1 + noise`. Fit `DecisionTreeRegressor` models with depths 2, 4, 6, and None. Plot the predictions and discuss which depth gives the best bias-variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter code\n",
    "# clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=42)\n",
    "# clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)\n",
    "# ... fit, predict, compare ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}