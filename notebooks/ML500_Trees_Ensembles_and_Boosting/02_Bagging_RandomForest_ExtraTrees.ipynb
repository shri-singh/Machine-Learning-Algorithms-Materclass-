{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML502: Bagging, Random Forest & Extra Trees\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain ensemble methods and the bagging (bootstrap aggregating) strategy\n",
    "2. Understand how Random Forest combines bagging with random feature subsets\n",
    "3. Train and tune `RandomForestClassifier` with key hyperparameters\n",
    "4. Interpret out-of-bag (OOB) scores and feature importances\n",
    "5. Compare single decision trees, Random Forests, and Extra Trees\n",
    "6. Know when Random Forest is a strong default choice\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Decision tree fundamentals (Notebook 01)\n",
    "- Understanding of overfitting and variance in models\n",
    "- NumPy, pandas, and matplotlib basics\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Ensemble Methods and Bagging Theory](#1-ensemble-methods-and-bagging-theory)\n",
    "2. [Random Forest: Bagging + Random Features](#2-random-forest-bagging--random-features)\n",
    "3. [Key Hyperparameters](#3-key-hyperparameters)\n",
    "4. [OOB Score Explanation](#4-oob-score-explanation)\n",
    "5. [Demo: Single Tree vs Random Forest](#5-demo-single-tree-vs-random-forest)\n",
    "6. [Feature Importance from Random Forest](#6-feature-importance-from-random-forest)\n",
    "7. [Extra Trees: Even More Randomness](#7-extra-trees-even-more-randomness)\n",
    "8. [When to Use Random Forest](#8-when-to-use-random-forest)\n",
    "9. [Common Mistakes](#9-common-mistakes)\n",
    "10. [Exercises](#10-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ensemble Methods and Bagging Theory\n",
    "\n",
    "**Ensemble methods** combine multiple models to produce a better predictor than any single model.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging reduces **variance** by:\n",
    "1. Drawing $B$ bootstrap samples (random samples with replacement) from the training data\n",
    "2. Training one model (usually a decision tree) on each bootstrap sample\n",
    "3. Aggregating predictions: **majority vote** for classification, **mean** for regression\n",
    "\n",
    "**Why it works:** Individual trees are high-variance models. By averaging many trees trained on different subsets, the variance decreases while bias stays roughly the same.\n",
    "\n",
    "$$\\text{Var}\\left(\\frac{1}{B}\\sum_{b=1}^B T_b(x)\\right) \\approx \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$$\n",
    "\n",
    "where $\\rho$ is the average correlation between trees and $\\sigma^2$ is the variance of a single tree. The lower the correlation $\\rho$, the more variance reduction we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest: Bagging + Random Features\n",
    "\n",
    "Random Forest goes one step beyond bagging by also **randomizing the features** considered at each split:\n",
    "\n",
    "1. For each tree, draw a bootstrap sample of the training data\n",
    "2. At each split, randomly select `max_features` features (not all features)\n",
    "3. Find the best split among those selected features\n",
    "4. Aggregate predictions across all trees\n",
    "\n",
    "This additional randomness **decorrelates the trees** (reduces $\\rho$), which further reduces ensemble variance.\n",
    "\n",
    "**Default `max_features`:**\n",
    "- Classification: $\\sqrt{p}$ (square root of total features)\n",
    "- Regression: $p / 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `n_estimators` | Number of trees in the forest | 100-1000 |\n",
    "| `max_features` | Features to consider at each split | `'sqrt'`, `'log2'`, or float |\n",
    "| `max_depth` | Maximum depth of each tree | `None` or 10-30 |\n",
    "| `min_samples_split` | Min samples to split a node | 2-10 |\n",
    "| `min_samples_leaf` | Min samples in a leaf | 1-5 |\n",
    "| `oob_score` | Whether to use OOB samples for validation | `True` / `False` |\n",
    "| `n_jobs` | Parallel workers (-1 = all cores) | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OOB Score Explanation\n",
    "\n",
    "Each bootstrap sample leaves out roughly **37%** of the data (out-of-bag samples). The OOB score evaluates each sample using only the trees that did **not** include it in their training set.\n",
    "\n",
    "This provides a **free validation estimate** without needing a separate validation set, similar to leave-one-out cross-validation but much cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo: Single Tree vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"Classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_train_acc = accuracy_score(y_train, dt.predict(X_train))\n",
    "dt_test_acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "rf_test_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "print(\"Single Decision Tree:\")\n",
    "print(f\"  Train accuracy: {dt_train_acc:.4f}\")\n",
    "print(f\"  Test accuracy:  {dt_test_acc:.4f}\")\n",
    "print()\n",
    "print(\"Random Forest (100 trees):\")\n",
    "print(f\"  Train accuracy: {rf_train_acc:.4f}\")\n",
    "print(f\"  Test accuracy:  {rf_test_acc:.4f}\")\n",
    "print(f\"  OOB score:      {rf.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison\n",
    "dt_cv = cross_val_score(DecisionTreeClassifier(random_state=42), X, y, cv=5, scoring='accuracy')\n",
    "rf_cv = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    X, y, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Decision Tree CV: {dt_cv.mean():.4f} +/- {dt_cv.std():.4f}\")\n",
    "print(f\"Random Forest CV: {rf_cv.mean():.4f} +/- {rf_cv.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how test accuracy improves with number of trees\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 200, 300, 500]\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=n, oob_score=True, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "    oob_scores.append(rf_temp.oob_score_)\n",
    "    test_scores.append(accuracy_score(y_test, rf_temp.predict(X_test)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(n_estimators_range, oob_scores, 'o-', label='OOB Score', linewidth=2)\n",
    "ax.plot(n_estimators_range, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "ax.set_xlabel('Number of Trees (n_estimators)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Random Forest: Performance vs Number of Trees')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance saturates after enough trees -- more trees never hurt, but returns diminish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (mean decrease in impurity)\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# Sort by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Show top 15 features\n",
    "top_n = 15\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(\n",
    "    range(top_n),\n",
    "    importances[indices[:top_n]][::-1],\n",
    "    xerr=std[indices[:top_n]][::-1],\n",
    "    align='center',\n",
    "    color='steelblue',\n",
    "    alpha=0.8\n",
    ")\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(feature_names[indices[:top_n]][::-1])\n",
    "ax.set_xlabel('Feature Importance (Mean Decrease in Impurity)')\n",
    "ax.set_title('Top 15 Feature Importances from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extra Trees: Even More Randomness\n",
    "\n",
    "`ExtraTreesClassifier` (Extremely Randomized Trees) differs from Random Forest in two ways:\n",
    "\n",
    "1. **No bootstrapping** -- each tree uses the full training set\n",
    "2. **Random split thresholds** -- instead of finding the best split, it picks a random threshold for each feature\n",
    "\n",
    "This adds even more randomness, which can further reduce variance (at the cost of slightly more bias). Extra Trees is also **faster** because it skips the optimization of split thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Random Forest vs Extra Trees\n",
    "et = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "et_train_acc = accuracy_score(y_train, et.predict(X_train))\n",
    "et_test_acc = accuracy_score(y_test, et.predict(X_test))\n",
    "\n",
    "et_cv = cross_val_score(\n",
    "    ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    X, y, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Summary comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'Extra Trees'],\n",
    "    'Train Acc': [dt_train_acc, rf_train_acc, et_train_acc],\n",
    "    'Test Acc': [dt_test_acc, rf_test_acc, et_test_acc],\n",
    "    'CV Mean': [dt_cv.mean(), rf_cv.mean(), et_cv.mean()],\n",
    "    'CV Std': [dt_cv.std(), rf_cv.std(), et_cv.std()],\n",
    "})\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. When to Use Random Forest\n",
    "\n",
    "Random Forest is an excellent **default model** because:\n",
    "\n",
    "- **Robust out of the box** -- works well without extensive tuning\n",
    "- **Handles mixed feature types** -- numerical and categorical (with encoding)\n",
    "- **No feature scaling needed** -- tree-based splitting is invariant to monotone transformations\n",
    "- **Built-in feature importance** -- useful for feature selection\n",
    "- **Parallelizable** -- trees are independent, so training scales with `n_jobs=-1`\n",
    "- **Resistant to overfitting** -- more trees do not increase overfitting (unlike boosting)\n",
    "\n",
    "**Limitations:**\n",
    "- Cannot extrapolate beyond training data range\n",
    "- Less interpretable than a single tree\n",
    "- Can be slow for very large datasets (many trees, many features)\n",
    "- Often outperformed by gradient boosting on structured/tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Common Mistakes\n",
    "\n",
    "1. **Too few trees**: Using `n_estimators=10` is almost always too few. Start with 100-300; more trees only cost computation, not accuracy.\n",
    "\n",
    "2. **Ignoring feature importance**: Random Forest provides feature importances for free. Use them for feature selection and understanding your data.\n",
    "\n",
    "3. **Overfitting with deep trees**: While Random Forest is resistant to overfitting, individual trees with no depth limit on small datasets can still memorize noise. Consider setting `max_depth` or `min_samples_leaf` for small datasets.\n",
    "\n",
    "4. **Not using OOB score**: The OOB score is a free validation estimate. Set `oob_score=True` to monitor generalization without a separate validation split.\n",
    "\n",
    "5. **Expecting extrapolation**: Like single decision trees, Random Forest cannot predict values outside the training range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Effect of `max_features`\n",
    "Train Random Forest models with `max_features` set to `'sqrt'`, `'log2'`, `0.3`, `0.5`, and `1.0` on the breast cancer dataset. Compare their OOB scores and test accuracies. Which setting works best?\n",
    "\n",
    "### Exercise 2: Bagging vs Random Forest\n",
    "Use `BaggingClassifier` with a `DecisionTreeClassifier` base estimator (with all features at each split) and compare it to `RandomForestClassifier`. This isolates the effect of random feature selection.\n",
    "\n",
    "### Exercise 3: Feature Selection with RF\n",
    "Train a Random Forest and identify the top 10 features by importance. Re-train the model using only those 10 features. Does performance degrade significantly? What does this tell you about the other features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter code\n",
    "# max_features_options = ['sqrt', 'log2', 0.3, 0.5, 1.0]\n",
    "# for mf in max_features_options:\n",
    "#     rf_temp = RandomForestClassifier(\n",
    "#         n_estimators=200, max_features=mf, oob_score=True, random_state=42\n",
    "#     )\n",
    "#     rf_temp.fit(X_train, y_train)\n",
    "#     print(f\"max_features={mf}: OOB={rf_temp.oob_score_:.4f}, \"\n",
    "#           f\"Test={accuracy_score(y_test, rf_temp.predict(X_test)):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}