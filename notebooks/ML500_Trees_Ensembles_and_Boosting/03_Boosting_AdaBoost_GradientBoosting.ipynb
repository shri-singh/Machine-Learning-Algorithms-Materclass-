{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML503: Boosting -- AdaBoost & Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the boosting paradigm: sequential learners correcting predecessor errors\n",
    "2. Understand AdaBoost's sample reweighting mechanism\n",
    "3. Understand Gradient Boosting's residual-fitting approach\n",
    "4. Train and tune `AdaBoostClassifier` and `GradientBoostingClassifier`\n",
    "5. Compare boosting methods against Random Forest on the same dataset\n",
    "6. Detect overfitting via training/validation error curves\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Decision tree fundamentals (Notebook 01)\n",
    "- Ensemble and bagging concepts (Notebook 02)\n",
    "- Understanding of bias-variance trade-off\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Boosting Theory](#1-boosting-theory)\n",
    "2. [AdaBoost](#2-adaboost)\n",
    "3. [Gradient Boosting](#3-gradient-boosting)\n",
    "4. [Scikit-Learn Implementation](#4-scikit-learn-implementation)\n",
    "5. [Comparison: AdaBoost vs GradientBoosting vs RandomForest](#5-comparison-adaboost-vs-gradientboosting-vs-randomforest)\n",
    "6. [Overfitting Detection](#6-overfitting-detection)\n",
    "7. [Common Mistakes](#7-common-mistakes)\n",
    "8. [Exercises](#8-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boosting Theory\n",
    "\n",
    "**Boosting** is a family of ensemble methods where models are trained **sequentially**, and each new model focuses on correcting the errors of the previous ones.\n",
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "| Aspect | Bagging (e.g., RF) | Boosting (e.g., GBM) |\n",
    "|--------|-------------------|---------------------|\n",
    "| Training | Parallel (independent) | Sequential (dependent) |\n",
    "| Reduces | Variance | Bias (and variance) |\n",
    "| Base learners | Full-depth trees | Shallow trees (stumps/weak learners) |\n",
    "| Overfitting risk | Low | Higher (must tune carefully) |\n",
    "| Weighting | Equal vote | Weighted contributions |\n",
    "\n",
    "### General Boosting Update\n",
    "\n",
    "The ensemble prediction after $m$ stages:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "where $\\eta$ is the learning rate and $h_m(x)$ is the new weak learner added at stage $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AdaBoost\n",
    "\n",
    "**Adaptive Boosting (AdaBoost)** works by reweighting samples:\n",
    "\n",
    "1. Start with equal sample weights: $w_i = 1/n$\n",
    "2. Train a weak learner (e.g., decision stump) on the weighted data\n",
    "3. Compute the weighted error rate $\\epsilon_m$\n",
    "4. Compute the learner weight: $\\alpha_m = \\frac{1}{2} \\ln\\frac{1 - \\epsilon_m}{\\epsilon_m}$\n",
    "5. Update sample weights -- increase weight of misclassified samples:\n",
    "   $$w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_m \\cdot y_i \\cdot h_m(x_i))$$\n",
    "6. Normalize weights and repeat\n",
    "\n",
    "**Final prediction:** $F(x) = \\text{sign}\\left(\\sum_{m=1}^{M} \\alpha_m \\cdot h_m(x)\\right)$\n",
    "\n",
    "**Key insight:** Misclassified samples get higher weights, so the next learner focuses more on the hard cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** generalizes boosting by fitting new learners to the **negative gradient of the loss function** (which, for squared error, equals the residuals).\n",
    "\n",
    "### Algorithm (simplified for regression with MSE):\n",
    "\n",
    "1. Initialize $F_0(x) = \\bar{y}$ (mean of targets)\n",
    "2. For $m = 1, 2, \\ldots, M$:\n",
    "   - Compute residuals: $r_i = y_i - F_{m-1}(x_i)$\n",
    "   - Fit a regression tree $h_m$ to the residuals\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `n_estimators` | Number of boosting stages | 100-1000 |\n",
    "| `learning_rate` | Shrinkage per step | 0.01-0.3 |\n",
    "| `max_depth` | Depth of each tree (shallow!) | 3-8 |\n",
    "| `subsample` | Fraction of samples per tree | 0.5-1.0 |\n",
    "\n",
    "**Important trade-off:** Lower `learning_rate` requires more `n_estimators` but usually gives better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # decision stumps\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "print(\"AdaBoost (200 stumps, lr=0.5):\")\n",
    "print(f\"  Train accuracy: {accuracy_score(y_train, ada.predict(X_train)):.4f}\")\n",
    "print(f\"  Test accuracy:  {accuracy_score(y_test, ada.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,           # shallow trees for boosting\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting (200 trees, lr=0.1, depth=3):\")\n",
    "print(f\"  Train accuracy: {accuracy_score(y_train, gb.predict(X_train)):.4f}\")\n",
    "print(f\"  Test accuracy:  {accuracy_score(y_test, gb.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: AdaBoost vs GradientBoosting vs RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest for comparison\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation comparison\n",
    "models = {\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=200, learning_rate=0.5, random_state=42\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    ),\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    model.fit(X_train, y_train)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': accuracy_score(y_train, model.predict(X_train)),\n",
    "        'Test Acc': accuracy_score(y_test, model.predict(X_test)),\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.3\n",
    "\n",
    "ax.bar(x_pos - width/2, results_df['Train Acc'], width, label='Train Acc', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, results_df['Test Acc'], width, label='Test Acc', alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(results_df['Model'])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison on Breast Cancer Dataset')\n",
    "ax.legend()\n",
    "ax.set_ylim(0.9, 1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting Detection\n",
    "\n",
    "Boosting can overfit if too many estimators are used or the learning rate is too high. We can monitor training vs validation loss across boosting stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting: staged prediction for train/test error curves\n",
    "gb_monitor = GradientBoostingClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    ")\n",
    "gb_monitor.fit(X_train, y_train)\n",
    "\n",
    "# Compute error at each stage using staged_predict\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for y_pred_train in gb_monitor.staged_predict(X_train):\n",
    "    train_errors.append(1 - accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "for y_pred_test in gb_monitor.staged_predict(X_test):\n",
    "    test_errors.append(1 - accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "stages = range(1, len(train_errors) + 1)\n",
    "ax.plot(stages, train_errors, label='Training Error', linewidth=2)\n",
    "ax.plot(stages, test_errors, label='Test Error', linewidth=2)\n",
    "ax.set_xlabel('Number of Boosting Stages')\n",
    "ax.set_ylabel('Error Rate')\n",
    "ax.set_title('Gradient Boosting: Training vs Test Error over Stages')\n",
    "ax.legend()\n",
    "\n",
    "# Mark approximate best number of estimators\n",
    "best_stage = np.argmin(test_errors) + 1\n",
    "ax.axvline(x=best_stage, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.annotate(f'Best stage: {best_stage}', xy=(best_stage, test_errors[best_stage - 1]),\n",
    "            xytext=(best_stage + 50, test_errors[best_stage - 1] + 0.01),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best test error at stage {best_stage}: {test_errors[best_stage - 1]:.4f}\")\n",
    "print(\"After this point, additional stages may overfit (test error increases).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost staged prediction\n",
    "ada_monitor = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "ada_monitor.fit(X_train, y_train)\n",
    "\n",
    "ada_train_errors = [\n",
    "    1 - accuracy_score(y_train, y_pred)\n",
    "    for y_pred in ada_monitor.staged_predict(X_train)\n",
    "]\n",
    "ada_test_errors = [\n",
    "    1 - accuracy_score(y_test, y_pred)\n",
    "    for y_pred in ada_monitor.staged_predict(X_test)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "stages = range(1, len(ada_train_errors) + 1)\n",
    "ax.plot(stages, ada_train_errors, label='Training Error', linewidth=2)\n",
    "ax.plot(stages, ada_test_errors, label='Test Error', linewidth=2)\n",
    "ax.set_xlabel('Number of Boosting Stages')\n",
    "ax.set_ylabel('Error Rate')\n",
    "ax.set_title('AdaBoost: Training vs Test Error over Stages')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Common Mistakes\n",
    "\n",
    "1. **High learning rate + many estimators**: A large learning rate (e.g., 1.0) with many estimators causes overfitting. Use a small learning rate (0.01-0.1) and increase `n_estimators` accordingly.\n",
    "\n",
    "2. **Not tuning `max_depth` for boosting**: Boosting uses **shallow trees** (depth 3-5), not deep trees like Random Forest. Deep trees in boosting lead to severe overfitting.\n",
    "\n",
    "3. **Treating boosting like bagging**: In bagging, more trees never hurt. In boosting, too many stages can overfit. Always monitor validation error.\n",
    "\n",
    "4. **Ignoring the learning rate / n_estimators trade-off**: These two parameters must be tuned together. A smaller learning rate generally needs more estimators but produces better results.\n",
    "\n",
    "5. **Not using early stopping**: For Gradient Boosting, use `n_iter_no_change` and `validation_fraction` to automatically stop when validation performance plateaus. This prevents overfitting and saves compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1: Learning Rate Experiment\n",
    "Train `GradientBoostingClassifier` with learning rates of 0.01, 0.05, 0.1, 0.5, and 1.0, all with `n_estimators=300`. Plot the test error curves for each. Which learning rate gives the best final test accuracy? Which converges fastest?\n",
    "\n",
    "### Exercise 2: AdaBoost Depth Experiment\n",
    "Train `AdaBoostClassifier` with base estimators of depth 1 (stump), 2, 3, and 5. Does increasing the base learner complexity help AdaBoost? At what depth does overfitting become visible?\n",
    "\n",
    "### Exercise 3: Early Stopping\n",
    "Train a `GradientBoostingClassifier` with `n_estimators=1000` and `n_iter_no_change=10`. Compare the final number of estimators used (access `n_estimators_` after fit) with the full 1000. How much compute did early stopping save?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter code\n",
    "# learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# for lr in learning_rates:\n",
    "#     gb_exp = GradientBoostingClassifier(\n",
    "#         n_estimators=300, learning_rate=lr, max_depth=3, random_state=42\n",
    "#     )\n",
    "#     gb_exp.fit(X_train, y_train)\n",
    "#     test_errors = [\n",
    "#         1 - accuracy_score(y_test, y_pred)\n",
    "#         for y_pred in gb_exp.staged_predict(X_test)\n",
    "#     ]\n",
    "#     ax.plot(range(1, 301), test_errors, label=f'lr={lr}')\n",
    "# ax.set_xlabel('Boosting Stages')\n",
    "# ax.set_ylabel('Test Error')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}