{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML504: XGBoost / HistGradientBoosting -- Practical Guide\n",
    "\n",
    "> **Note:** The original spec mentioned \"Chegg\" which appears to be a typo. This notebook covers XGBoost/HistGradientBoosting-style gradient boosting.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Recall how CART decision trees perform recursive splitting\n",
    "2. Use `HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` from scikit-learn\n",
    "3. Understand and tune key parameters: `max_iter`, `learning_rate`, `max_depth`, `l2_regularization`\n",
    "4. Apply early stopping to prevent overfitting and save compute\n",
    "5. Optionally use `xgboost.XGBClassifier` if the library is installed\n",
    "6. Compare HistGBDT vs standard GradientBoosting in speed and performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Decision trees and CART (Notebook 01)\n",
    "- Gradient Boosting fundamentals (Notebook 03)\n",
    "- Familiarity with scikit-learn model APIs\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [CART Recap](#1-cart-recap)\n",
    "2. [HistGradientBoosting Overview](#2-histgradientboosting-overview)\n",
    "3. [Key Parameters](#3-key-parameters)\n",
    "4. [HistGradientBoosting Demo with Early Stopping](#4-histgradientboosting-demo-with-early-stopping)\n",
    "5. [Optional: XGBoost](#5-optional-xgboost)\n",
    "6. [Speed and Performance Comparison](#6-speed-and-performance-comparison)\n",
    "7. [Practical Tips](#7-practical-tips)\n",
    "8. [Common Mistakes](#8-common-mistakes)\n",
    "9. [Exercises](#9-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CART Recap\n",
    "\n",
    "Before diving into advanced boosting, let us recall how a single CART decision tree works.\n",
    "\n",
    "**CART (Classification and Regression Trees):**\n",
    "- Builds a binary tree via **recursive binary splitting**\n",
    "- At each node, selects the feature and threshold that minimizes a cost function\n",
    "  - Classification: Gini impurity $G = 1 - \\sum p_k^2$\n",
    "  - Regression: MSE $= \\frac{1}{n}\\sum(y_i - \\bar{y})^2$\n",
    "- Continues splitting until a stopping criterion is met\n",
    "- Predictions: majority class (classification) or mean value (regression) at each leaf\n",
    "\n",
    "**In boosting**, each tree is typically **shallow** (depth 3-6) and fits the residuals (or pseudo-residuals) of the ensemble built so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HistGradientBoosting Overview\n",
    "\n",
    "`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` are scikit-learn's fast gradient boosting implementations, inspired by LightGBM.\n",
    "\n",
    "### Key advantages over standard `GradientBoostingClassifier`:\n",
    "\n",
    "| Feature | Standard GB | HistGradientBoosting |\n",
    "|---------|------------|---------------------|\n",
    "| Speed | Slow on large data | Much faster (histogram-based) |\n",
    "| Missing values | Requires imputation | Native support |\n",
    "| Categorical features | Requires encoding | Native support (experimental) |\n",
    "| Early stopping | Manual (`n_iter_no_change`) | Built-in (`early_stopping='auto'`) |\n",
    "| Scalability | Poor for >10k samples | Good for millions of samples |\n",
    "\n",
    "### How histogram-based splitting works:\n",
    "Instead of evaluating all possible thresholds for each feature, the algorithm **bins** continuous features into ~256 discrete bins. This dramatically reduces the number of candidate splits and makes computation much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Parameters\n",
    "\n",
    "| Parameter | Description | Default | Typical Range |\n",
    "|-----------|-------------|---------|---------------|\n",
    "| `max_iter` | Number of boosting iterations | 100 | 100-1000 |\n",
    "| `learning_rate` | Shrinkage per step | 0.1 | 0.01-0.3 |\n",
    "| `max_depth` | Max depth of each tree | None | 3-8 |\n",
    "| `max_leaf_nodes` | Max leaves per tree | 31 | 15-63 |\n",
    "| `min_samples_leaf` | Min samples in a leaf | 20 | 5-50 |\n",
    "| `l2_regularization` | L2 penalty on leaf values | 0 | 0-10 |\n",
    "| `max_bins` | Number of histogram bins | 255 | 63-255 |\n",
    "| `early_stopping` | When to use early stopping | `'auto'` | `True`, `False`, `'auto'` |\n",
    "| `n_iter_no_change` | Iterations without improvement before stopping | 10 | 5-20 |\n",
    "| `validation_fraction` | Fraction of data for early stopping | 0.1 | 0.1-0.2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HistGradientBoosting Demo with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistGradientBoosting with early stopping\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_iter=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    l2_regularization=1.0,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=15,\n",
    "    validation_fraction=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Requested max_iter: 500\")\n",
    "print(f\"Actual iterations used: {hgb.n_iter_}\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, hgb.predict(X_train)):.4f}\")\n",
    "print(f\"Test accuracy:  {accuracy_score(y_test, hgb.predict(X_test)):.4f}\")\n",
    "print(f\"\\nEarly stopping saved {500 - hgb.n_iter_} iterations of unnecessary computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistGradientBoosting for Regression demo\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000, n_features=10, n_informative=5, noise=20, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "hgb_reg = HistGradientBoostingRegressor(\n",
    "    max_iter=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42,\n",
    ")\n",
    "hgb_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "y_pred_reg = hgb_reg.predict(X_reg_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_pred_reg))\n",
    "print(f\"HistGradientBoostingRegressor:\")\n",
    "print(f\"  Iterations used: {hgb_reg.n_iter_}\")\n",
    "print(f\"  Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: XGBoost\n",
    "\n",
    "XGBoost is a popular external library for gradient boosting. It provides additional features like:\n",
    "- More regularization options (L1 + L2)\n",
    "- GPU acceleration\n",
    "- Built-in cross-validation\n",
    "\n",
    "The cell below attempts to import XGBoost. If it is not installed, it will skip gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        reg_lambda=1.0,       # L2 regularization\n",
    "        reg_alpha=0.0,        # L1 regularization\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        early_stopping_rounds=15,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    xgb_clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    xgb_train_acc = accuracy_score(y_train, xgb_clf.predict(X_train))\n",
    "    xgb_test_acc = accuracy_score(y_test, xgb_clf.predict(X_test))\n",
    "    \n",
    "    print(f\"\\nXGBoost Results:\")\n",
    "    print(f\"  Best iteration: {xgb_clf.best_iteration}\")\n",
    "    print(f\"  Train accuracy: {xgb_train_acc:.4f}\")\n",
    "    print(f\"  Test accuracy:  {xgb_test_acc:.4f}\")\n",
    "    \n",
    "    HAS_XGBOOST = True\n",
    "\n",
    "except ImportError:\n",
    "    print(\"XGBoost is not installed. Skipping this section.\")\n",
    "    print(\"To install: pip install xgboost\")\n",
    "    print(\"\\nThe HistGradientBoosting models above provide similar functionality\")\n",
    "    print(\"without requiring any additional packages.\")\n",
    "    HAS_XGBOOST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speed and Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger dataset for meaningful speed comparison\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=10000, n_features=30, n_informative=15,\n",
    "    n_redundant=5, random_state=42\n",
    ")\n",
    "X_lg_train, X_lg_test, y_lg_train, y_lg_test = train_test_split(\n",
    "    X_large, y_large, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Large dataset: {X_large.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard GradientBoosting\n",
    "gb_std = GradientBoostingClassifier(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "start = time.time()\n",
    "gb_std.fit(X_lg_train, y_lg_train)\n",
    "gb_time = time.time() - start\n",
    "gb_acc = accuracy_score(y_lg_test, gb_std.predict(X_lg_test))\n",
    "\n",
    "# HistGradientBoosting\n",
    "hgb_fast = HistGradientBoostingClassifier(\n",
    "    max_iter=200, learning_rate=0.1, max_depth=5,\n",
    "    early_stopping=False, random_state=42\n",
    ")\n",
    "start = time.time()\n",
    "hgb_fast.fit(X_lg_train, y_lg_train)\n",
    "hgb_time = time.time() - start\n",
    "hgb_acc = accuracy_score(y_lg_test, hgb_fast.predict(X_lg_test))\n",
    "\n",
    "# Results\n",
    "print(f\"{'Model':<30} {'Time (s)':<12} {'Test Acc':<10}\")\n",
    "print(f\"{'-'*52}\")\n",
    "print(f\"{'GradientBoosting':<30} {gb_time:<12.3f} {gb_acc:<10.4f}\")\n",
    "print(f\"{'HistGradientBoosting':<30} {hgb_time:<12.3f} {hgb_acc:<10.4f}\")\n",
    "print(f\"\\nHistGradientBoosting speedup: {gb_time / hgb_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "models_compared = ['GradientBoosting', 'HistGradientBoosting']\n",
    "times = [gb_time, hgb_time]\n",
    "accs = [gb_acc, hgb_acc]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(models_compared, times, color=['steelblue', 'coral'])\n",
    "axes[0].set_ylabel('Training Time (seconds)')\n",
    "axes[0].set_title('Training Speed Comparison')\n",
    "\n",
    "axes[1].bar(models_compared, accs, color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].set_ylim(min(accs) - 0.02, max(accs) + 0.02)\n",
    "\n",
    "plt.suptitle('Standard vs Histogram-Based Gradient Boosting (10k samples)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Tips\n",
    "\n",
    "1. **Always use early stopping**: Set `early_stopping=True` (HistGBDT) or `early_stopping_rounds` (XGBoost). This prevents overfitting and saves compute.\n",
    "\n",
    "2. **Tune `learning_rate` and `max_iter` together**: Start with `learning_rate=0.1` and `max_iter=500` with early stopping. If performance is good, try lowering the learning rate and increasing max_iter.\n",
    "\n",
    "3. **Keep trees shallow**: `max_depth=3-6` is usually sufficient for boosting. Unlike Random Forest, deep trees in boosting cause overfitting.\n",
    "\n",
    "4. **Use HistGradientBoosting for large datasets**: It is much faster than standard GradientBoosting and handles missing values natively.\n",
    "\n",
    "5. **Start simple, add complexity**: Begin with default parameters, evaluate, then tune. Do not over-tune from the start.\n",
    "\n",
    "6. **Regularization matters**: Use `l2_regularization` (HistGBDT) or `reg_lambda`/`reg_alpha` (XGBoost) to control leaf values and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Not using early stopping**: Training for a fixed number of iterations without monitoring validation loss is the most common source of overfitting in gradient boosting.\n",
    "\n",
    "2. **Ignoring overfitting signs**: If training accuracy is much higher than validation accuracy, reduce `max_iter`, increase `l2_regularization`, or lower `learning_rate`.\n",
    "\n",
    "3. **Over-tuning hyperparameters**: Gradient boosting has many knobs. Excessive grid search over all parameters leads to overfitting the validation set. Focus on `learning_rate`, `max_iter`, and `max_depth` first.\n",
    "\n",
    "4. **Using standard GradientBoosting on large data**: For datasets with more than ~10k samples, `HistGradientBoostingClassifier` is dramatically faster with similar or better accuracy.\n",
    "\n",
    "5. **Forgetting that boosting cannot extrapolate**: Like all tree-based methods, gradient boosting predicts constants outside the training data range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "### Exercise 1: Early Stopping Experiment\n",
    "Train `HistGradientBoostingClassifier` with `max_iter=1000` and different values of `n_iter_no_change` (5, 10, 20, 50). How does the patience parameter affect the number of iterations used and final test accuracy?\n",
    "\n",
    "### Exercise 2: Regularization\n",
    "Train models with `l2_regularization` values of 0, 0.1, 1.0, 10.0, and 100.0. Plot test accuracy vs regularization strength. At what point does regularization start to hurt performance?\n",
    "\n",
    "### Exercise 3: HistGBDT vs XGBoost\n",
    "If XGBoost is installed, compare `HistGradientBoostingClassifier` and `XGBClassifier` with similar parameters on the breast cancer dataset. Report training time, train accuracy, and test accuracy. Are the results comparable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter code\n",
    "# patience_values = [5, 10, 20, 50]\n",
    "# for patience in patience_values:\n",
    "#     hgb_exp = HistGradientBoostingClassifier(\n",
    "#         max_iter=1000, learning_rate=0.1, max_depth=5,\n",
    "#         early_stopping=True, n_iter_no_change=patience, random_state=42\n",
    "#     )\n",
    "#     hgb_exp.fit(X_train, y_train)\n",
    "#     print(f\"patience={patience:2d}: iters={hgb_exp.n_iter_:3d}, \"\n",
    "#           f\"test_acc={accuracy_score(y_test, hgb_exp.predict(X_test)):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}