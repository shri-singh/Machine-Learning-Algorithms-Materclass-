{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML505: Model Interpretability -- SHAP & Permutation Importance (OPTIONAL)\n",
    "\n",
    "---\n",
    "\n",
    "> **This entire notebook is OPTIONAL.** It covers advanced interpretability techniques that go beyond core tree/ensemble modeling. These methods are valuable in practice but are not required for understanding the fundamentals of tree-based models.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Compute and interpret permutation importance using scikit-learn\n",
    "2. Compare permutation importance with built-in (impurity-based) feature importance\n",
    "3. Create and interpret partial dependence plots\n",
    "4. Understand the concept of SHAP values for model explanation\n",
    "5. Choose the right interpretability method for your use case\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Random Forest and ensemble methods (Notebooks 01-02)\n",
    "- Familiarity with feature importance concepts\n",
    "- scikit-learn basics\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Permutation Importance](#1-permutation-importance)\n",
    "2. [Permutation vs Built-in Importance](#2-permutation-vs-built-in-importance)\n",
    "3. [Partial Dependence Plots](#3-partial-dependence-plots)\n",
    "4. [SHAP Values](#4-shap-values)\n",
    "5. [When to Use Which Method](#5-when-to-use-which-method)\n",
    "6. [Common Mistakes](#6-common-mistakes)\n",
    "7. [Exercises](#7-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and train a Random Forest for all interpretability demos\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, rf.predict(X_train)):.4f}\")\n",
    "print(f\"Test accuracy:  {accuracy_score(y_test, rf.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Permutation Importance\n",
    "\n",
    "> **(OPTIONAL)** This section covers permutation importance as an alternative to built-in feature importance.\n",
    "\n",
    "### Concept\n",
    "\n",
    "Permutation importance measures how much the model's performance **drops** when a single feature is randomly shuffled:\n",
    "\n",
    "1. Compute baseline score on the dataset\n",
    "2. For each feature:\n",
    "   - Randomly shuffle that feature's values\n",
    "   - Re-compute the score\n",
    "   - Importance = baseline score - shuffled score\n",
    "3. Repeat multiple times for stability\n",
    "\n",
    "**Advantages over built-in importance:**\n",
    "- Model-agnostic (works with any model)\n",
    "- Not biased toward high-cardinality features\n",
    "- Can be computed on test data (measures real generalization impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance on the TEST set\n",
    "perm_imp = permutation_importance(\n",
    "    rf, X_test, y_test,\n",
    "    n_repeats=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Sort by mean importance\n",
    "sorted_idx = perm_imp.importances_mean.argsort()[::-1]\n",
    "\n",
    "# Display top 15\n",
    "top_n = 15\n",
    "print(f\"Top {top_n} features by permutation importance (on test set):\")\n",
    "print(f\"{'Feature':<30} {'Mean':>8} {'Std':>8}\")\n",
    "print(f\"{'-'*46}\")\n",
    "for idx in sorted_idx[:top_n]:\n",
    "    print(f\"{feature_names[idx]:<30} {perm_imp.importances_mean[idx]:>8.4f} \"\n",
    "          f\"{perm_imp.importances_std[idx]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importance (top 15)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "top_indices = sorted_idx[:top_n][::-1]  # reverse for horizontal bar plot\n",
    "ax.barh(\n",
    "    range(top_n),\n",
    "    perm_imp.importances_mean[top_indices],\n",
    "    xerr=perm_imp.importances_std[top_indices],\n",
    "    color='steelblue',\n",
    "    alpha=0.8\n",
    ")\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(feature_names[top_indices])\n",
    "ax.set_xlabel('Mean Accuracy Decrease')\n",
    "ax.set_title('Permutation Importance (computed on test set)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Permutation vs Built-in Importance\n",
    "\n",
    "> **(OPTIONAL)** Comparing the two importance methods reveals important differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare built-in (Gini/impurity) importance with permutation importance\n",
    "builtin_imp = rf.feature_importances_\n",
    "perm_imp_mean = perm_imp.importances_mean\n",
    "\n",
    "# Normalize both to [0, 1] for comparison\n",
    "builtin_norm = builtin_imp / builtin_imp.max()\n",
    "perm_norm = perm_imp_mean / perm_imp_mean.max() if perm_imp_mean.max() > 0 else perm_imp_mean\n",
    "\n",
    "# Select top features by either method\n",
    "top_by_builtin = np.argsort(builtin_imp)[::-1][:10]\n",
    "top_by_perm = np.argsort(perm_imp_mean)[::-1][:10]\n",
    "top_features = list(set(top_by_builtin) | set(top_by_perm))\n",
    "top_features.sort(key=lambda i: -builtin_imp[i])\n",
    "\n",
    "# Plot side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Built-in importance\n",
    "idx_sorted = np.argsort(builtin_imp[top_features])\n",
    "axes[0].barh(\n",
    "    range(len(top_features)),\n",
    "    builtin_imp[np.array(top_features)[idx_sorted]],\n",
    "    color='steelblue', alpha=0.8\n",
    ")\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels(feature_names[np.array(top_features)[idx_sorted]])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Built-in (Impurity) Importance')\n",
    "\n",
    "# Permutation importance\n",
    "idx_sorted_p = np.argsort(perm_imp_mean[top_features])\n",
    "axes[1].barh(\n",
    "    range(len(top_features)),\n",
    "    perm_imp_mean[np.array(top_features)[idx_sorted_p]],\n",
    "    color='coral', alpha=0.8\n",
    ")\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels(feature_names[np.array(top_features)[idx_sorted_p]])\n",
    "axes[1].set_xlabel('Mean Accuracy Decrease')\n",
    "axes[1].set_title('Permutation Importance (test set)')\n",
    "\n",
    "plt.suptitle('Built-in vs Permutation Feature Importance', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: The two methods may rank features differently.\")\n",
    "print(\"Built-in importance can be biased toward high-cardinality numerical features.\")\n",
    "print(\"Permutation importance measures actual predictive value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partial Dependence Plots\n",
    "\n",
    "> **(OPTIONAL)** Partial dependence shows the marginal effect of a feature on the prediction.\n",
    "\n",
    "A **partial dependence plot (PDP)** shows how the predicted outcome changes as one feature varies, while averaging over all other features. It reveals the shape of the relationship between a feature and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top 4 features by permutation importance for PDP\n",
    "top4_idx = perm_imp.importances_mean.argsort()[::-1][:4]\n",
    "top4_names = feature_names[top4_idx]\n",
    "print(f\"Top 4 features for partial dependence plots: {list(top4_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Dependence Plots for top features\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf,\n",
    "    X_train,\n",
    "    features=list(top4_idx),\n",
    "    feature_names=feature_names,\n",
    "    kind='both',  # show individual conditional expectation (ICE) + average\n",
    "    subsample=100,\n",
    "    n_jobs=-1,\n",
    "    ax=ax,\n",
    "    random_state=42,\n",
    ")\n",
    "fig.suptitle('Partial Dependence Plots (Top 4 Features)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The yellow line shows the average partial dependence.\")\n",
    "print(\"The thin blue lines show Individual Conditional Expectation (ICE) curves.\")\n",
    "print(\"Large variation in ICE curves suggests feature interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Partial Dependence Plot (interaction between top 2 features)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "display_2d = PartialDependenceDisplay.from_estimator(\n",
    "    rf,\n",
    "    X_train,\n",
    "    features=[(top4_idx[0], top4_idx[1])],\n",
    "    feature_names=feature_names,\n",
    "    kind='average',\n",
    "    n_jobs=-1,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.suptitle(\n",
    "    f'2D Partial Dependence: {feature_names[top4_idx[0]]} vs {feature_names[top4_idx[1]]}',\n",
    "    fontsize=13\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SHAP Values\n",
    "\n",
    "> **(OPTIONAL)** SHAP (SHapley Additive exPlanations) provides theoretically grounded feature attributions.\n",
    "\n",
    "### Concept\n",
    "\n",
    "SHAP values are based on **Shapley values** from cooperative game theory. For each prediction, SHAP assigns a value to each feature that represents its contribution to moving the prediction away from the average.\n",
    "\n",
    "**Properties of SHAP values:**\n",
    "- **Local accuracy**: SHAP values for a prediction sum to the difference between the prediction and the average prediction\n",
    "- **Consistency**: If a model changes so that a feature has a larger impact, its SHAP value will not decrease\n",
    "- **Missingness**: Features that are missing have SHAP value of 0\n",
    "\n",
    "For tree-based models, `TreeExplainer` computes exact SHAP values efficiently in polynomial time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "    print(f\"SHAP version: {shap.__version__}\")\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Install with: pip install shap\")\n",
    "    print(\"Proceeding with a manual conceptual example instead.\")\n",
    "    HAS_SHAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SHAP:\n",
    "    # TreeExplainer for efficient SHAP computation on tree-based models\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Summary plot: global feature importance via SHAP\n",
    "    print(\"SHAP Summary Plot (class 1 = malignant):\")\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, :, 1] if shap_values.ndim == 3 else shap_values[1],\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        max_display=15,\n",
    "        show=True\n",
    "    )\n",
    "else:\n",
    "    print(\"SHAP library not available. Showing conceptual example.\")\n",
    "    print()\n",
    "    print(\"=== Manual SHAP-like Explanation ===\")\n",
    "    print()\n",
    "    print(\"For a single prediction, SHAP values explain how each feature\")\n",
    "    print(\"contributed to the prediction relative to the average.\")\n",
    "    print()\n",
    "    \n",
    "    # Manual demonstration using permutation-based approximation\n",
    "    sample_idx = 0\n",
    "    sample = X_test[sample_idx:sample_idx + 1]\n",
    "    base_pred = rf.predict_proba(X_test).mean(axis=0)\n",
    "    sample_pred = rf.predict_proba(sample)[0]\n",
    "    \n",
    "    print(f\"Average prediction (baseline): {base_pred}\")\n",
    "    print(f\"This sample's prediction:      {sample_pred}\")\n",
    "    print(f\"Difference to explain:         {sample_pred - base_pred}\")\n",
    "    print()\n",
    "    \n",
    "    # Approximate feature contributions by permuting one feature at a time\n",
    "    approx_contributions = []\n",
    "    for i in range(X_test.shape[1]):\n",
    "        X_permuted = sample.copy()\n",
    "        X_permuted[0, i] = X_train[:, i].mean()  # replace with mean\n",
    "        permuted_pred = rf.predict_proba(X_permuted)[0]\n",
    "        contribution = sample_pred[1] - permuted_pred[1]\n",
    "        approx_contributions.append(contribution)\n",
    "    \n",
    "    approx_contributions = np.array(approx_contributions)\n",
    "    top_contrib_idx = np.argsort(np.abs(approx_contributions))[::-1][:10]\n",
    "    \n",
    "    print(\"Approximate feature contributions (top 10):\")\n",
    "    for idx in top_contrib_idx:\n",
    "        direction = \"+\" if approx_contributions[idx] > 0 else \"-\"\n",
    "        print(f\"  {feature_names[idx]:<30} {direction}{abs(approx_contributions[idx]):.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Note: This is a simplified approximation. True SHAP values account\")\n",
    "    print(\"for all possible feature coalitions, not just single-feature replacement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SHAP:\n",
    "    # Force plot for a single prediction\n",
    "    print(\"SHAP Force Plot for sample 0:\")\n",
    "    shap.initjs()\n",
    "    \n",
    "    sv = shap_values[:, :, 1] if shap_values.ndim == 3 else shap_values[1]\n",
    "    ev = explainer.expected_value[1] if hasattr(explainer.expected_value, '__len__') else explainer.expected_value\n",
    "    \n",
    "    shap.force_plot(\n",
    "        ev,\n",
    "        sv[0],\n",
    "        X_test[0],\n",
    "        feature_names=feature_names,\n",
    "        matplotlib=True\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP not available -- force plot requires the shap library.\")\n",
    "    print(\"Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SHAP:\n",
    "    # SHAP bar plot: mean absolute SHAP values (global importance)\n",
    "    print(\"SHAP Bar Plot (mean |SHAP value|):\")\n",
    "    shap.summary_plot(\n",
    "        shap_values[:, :, 1] if shap_values.ndim == 3 else shap_values[1],\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        plot_type='bar',\n",
    "        max_display=15,\n",
    "        show=True\n",
    "    )\n",
    "else:\n",
    "    print(\"SHAP not available. See permutation importance above for global feature ranking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When to Use Which Method\n",
    "\n",
    "> **(OPTIONAL)** Choosing the right interpretability tool for your situation.\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Built-in importance** | Quick feature ranking for tree models | Fast, no extra computation | Biased toward high-cardinality features; training-data only |\n",
    "| **Permutation importance** | Model-agnostic feature ranking | Unbiased; can use test data; simple concept | Slow for many features; affected by correlated features |\n",
    "| **Partial dependence** | Understanding feature-response shape | Shows non-linear effects; intuitive plots | Assumes feature independence; can be misleading with correlations |\n",
    "| **SHAP values** | Individual prediction explanations | Theoretically sound; local + global; handles interactions | Slower; requires library; complex to explain to non-technical audiences |\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **Start with permutation importance** for a reliable global feature ranking\n",
    "- **Use partial dependence** to understand the shape of important feature effects\n",
    "- **Use SHAP** when you need to explain individual predictions (e.g., \"why did the model predict X for this patient?\")\n",
    "- **Avoid relying solely on built-in importance** for final conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Mistakes\n",
    "\n",
    "1. **Confusing importance with causation**: A feature being important means it is useful for prediction, not that it causes the outcome. Correlation and predictive power are not causation.\n",
    "\n",
    "2. **Using training data for permutation importance**: Permutation importance should be computed on the **test set** (or held-out data). On training data, it may reflect overfitting patterns rather than true predictive value.\n",
    "\n",
    "3. **Ignoring feature correlations**: When features are correlated, permutation importance can underestimate both features' importance (since permuting one leaves the correlated feature intact). Consider grouping correlated features or using SHAP.\n",
    "\n",
    "4. **Over-interpreting partial dependence with correlated features**: PDP assumes features are independent. If two features are highly correlated, the PDP may show unrealistic data points that do not occur in practice.\n",
    "\n",
    "5. **Treating SHAP values as definitive explanations**: SHAP values explain the model, not the underlying reality. A model can learn spurious correlations, and SHAP will faithfully explain those spurious patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "### Exercise 1: Permutation Importance on Train vs Test\n",
    "Compute permutation importance on both the training set and the test set for the Random Forest. Compare the rankings. Are there features that appear important on training data but not on test data? What does this suggest?\n",
    "\n",
    "### Exercise 2: Partial Dependence with Interactions\n",
    "Create 2D partial dependence plots for the top 2-3 pairs of features. Do any pairs show interaction effects (non-additive patterns in the 2D PDP)?\n",
    "\n",
    "### Exercise 3: Compare Interpretability Across Models\n",
    "Train a `GradientBoostingClassifier` alongside the Random Forest. Compute permutation importance for both models. Do the two models agree on the most important features? What might explain any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter code\n",
    "# perm_imp_train = permutation_importance(\n",
    "#     rf, X_train, y_train, n_repeats=20, random_state=42, n_jobs=-1\n",
    "# )\n",
    "# perm_imp_test = permutation_importance(\n",
    "#     rf, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1\n",
    "# )\n",
    "# # Compare: perm_imp_train.importances_mean vs perm_imp_test.importances_mean\n",
    "# # Look for features with high train importance but low test importance (overfitting signal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}