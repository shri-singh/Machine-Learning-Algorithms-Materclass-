{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions, Cost Functions, and the Bias-Variance Tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Distinguish between a **loss function** (single sample) and a **cost function** (average over the dataset)\n",
    "- Write and interpret common loss functions: MSE, MAE, Binary Cross-Entropy, Hinge Loss\n",
    "- Explain the **Bias-Variance Tradeoff** and its relationship to underfitting and overfitting\n",
    "- Use polynomial regression to visually demonstrate the tradeoff\n",
    "- Read and interpret **learning curves** to diagnose model problems\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python and NumPy\n",
    "- Familiarity with linear regression concepts (ML200)\n",
    "- Understanding of train/test splits (ML100)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Loss Function vs Cost Function](#1)\n",
    "2. [Common Loss Functions](#2)\n",
    "3. [The Bias-Variance Tradeoff](#3)\n",
    "4. [Demonstrating Bias-Variance with Polynomial Regression](#4)\n",
    "5. [Learning Curves](#5)\n",
    "6. [Common Mistakes](#6)\n",
    "7. [Exercise](#7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import learning_curve, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Loss Function vs Cost Function\n",
    "\n",
    "These two terms are often used interchangeably, but they have a precise distinction:\n",
    "\n",
    "| Term | Scope | Definition |\n",
    "|------|-------|------------|\n",
    "| **Loss function** $L$ | Single sample | Measures the error for **one** data point |\n",
    "| **Cost function** $J$ | Entire dataset | The **average** (or sum) of the loss over all samples |\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L\\bigl(y^{(i)},\\; \\hat{y}^{(i)}\\bigr)$$\n",
    "\n",
    "- **Loss** tells you how wrong the model is on a single prediction.\n",
    "- **Cost** aggregates that information so you can optimize over the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Common Loss Functions\n",
    "\n",
    "### 2.1 Mean Squared Error (MSE) -- Regression\n",
    "\n",
    "$$L = (y - \\hat{y})^2 \\qquad\\qquad J = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "- Penalizes large errors heavily (quadratic)\n",
    "- Differentiable everywhere -- convenient for gradient-based optimization\n",
    "\n",
    "### 2.2 Mean Absolute Error (MAE) -- Regression\n",
    "\n",
    "$$L = |y - \\hat{y}| \\qquad\\qquad J = \\frac{1}{n}\\sum_{i=1}^{n}|y^{(i)} - \\hat{y}^{(i)}|$$\n",
    "\n",
    "- Less sensitive to outliers than MSE\n",
    "- Not differentiable at zero -- requires sub-gradient methods\n",
    "\n",
    "### 2.3 Binary Cross-Entropy -- Classification\n",
    "\n",
    "$$L = -\\bigl[y\\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})\\bigr]$$\n",
    "\n",
    "- Used in logistic regression and neural networks\n",
    "- Heavily penalizes confident wrong predictions\n",
    "\n",
    "### 2.4 Hinge Loss -- Classification (SVM)\n",
    "\n",
    "$$L = \\max(0,\\; 1 - y\\hat{y})$$\n",
    "\n",
    "- Used by Support Vector Machines\n",
    "- $y \\in \\{-1, +1\\}$; zero loss when the prediction is correct and confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize MSE vs MAE loss for a single sample ---\n",
    "errors = np.linspace(-4, 4, 200)\n",
    "\n",
    "mse_loss = errors ** 2\n",
    "mae_loss = np.abs(errors)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(errors, mse_loss, color='steelblue', linewidth=2)\n",
    "axes[0].set_title('MSE Loss: $L = (y - \\hat{y})^2$')\n",
    "axes[0].set_xlabel('Error $(y - \\hat{y})$')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(errors, mae_loss, color='darkorange', linewidth=2)\n",
    "axes[1].set_title('MAE Loss: $L = |y - \\hat{y}|$')\n",
    "axes[1].set_xlabel('Error $(y - \\hat{y})$')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Key takeaway: MSE penalizes large errors much more than MAE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Binary Cross-Entropy ---\n",
    "p_hat = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "loss_y1 = -np.log(p_hat)        # true label y=1\n",
    "loss_y0 = -np.log(1 - p_hat)    # true label y=0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(p_hat, loss_y1, color='crimson', linewidth=2)\n",
    "axes[0].set_title('Cross-Entropy when $y = 1$')\n",
    "axes[0].set_xlabel('Predicted probability $\\hat{p}$')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(p_hat, loss_y0, color='teal', linewidth=2)\n",
    "axes[1].set_title('Cross-Entropy when $y = 0$')\n",
    "axes[1].set_xlabel('Predicted probability $\\hat{p}$')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Key takeaway: loss explodes when the model is confident AND wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. The Bias-Variance Tradeoff\n",
    "\n",
    "Every model's expected prediction error can be decomposed as:\n",
    "\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "| Component | Meaning | Symptom |\n",
    "|-----------|---------|----------|\n",
    "| **Bias** | How far the average prediction is from the true value | **Underfitting** -- model is too simple |\n",
    "| **Variance** | How much predictions change across different training sets | **Overfitting** -- model is too complex |\n",
    "| **Irreducible Noise** | Random noise inherent in the data | Cannot be reduced by any model |\n",
    "\n",
    "**The tradeoff:**\n",
    "- Increasing model complexity decreases bias but increases variance.\n",
    "- Decreasing model complexity increases bias but decreases variance.\n",
    "- The sweet spot is where total error (bias$^2$ + variance) is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conceptual plot: model complexity vs error ---\n",
    "complexity = np.linspace(0.5, 10, 200)\n",
    "\n",
    "bias_sq = 8 / complexity ** 1.5\n",
    "variance = 0.05 * complexity ** 2\n",
    "noise = np.full_like(complexity, 0.5)\n",
    "total_error = bias_sq + variance + noise\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(complexity, bias_sq, '--', label='Bias$^2$', color='steelblue', linewidth=2)\n",
    "ax.plot(complexity, variance, '--', label='Variance', color='darkorange', linewidth=2)\n",
    "ax.plot(complexity, noise, ':', label='Irreducible Noise', color='gray', linewidth=2)\n",
    "ax.plot(complexity, total_error, '-', label='Total Error', color='crimson', linewidth=2.5)\n",
    "\n",
    "# Mark the optimal point\n",
    "opt_idx = np.argmin(total_error)\n",
    "ax.axvline(complexity[opt_idx], color='green', linestyle='-.', alpha=0.6, label='Optimal Complexity')\n",
    "\n",
    "ax.set_xlabel('Model Complexity')\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_title('Bias-Variance Tradeoff')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate regions\n",
    "ax.annotate('Underfitting\\n(High Bias)', xy=(1.5, 5), fontsize=13, color='steelblue', fontweight='bold')\n",
    "ax.annotate('Overfitting\\n(High Variance)', xy=(7.5, 5), fontsize=13, color='darkorange', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Demonstrating Bias-Variance with Polynomial Regression\n",
    "\n",
    "We will fit polynomials of degree 1 (linear), 3 (moderate), and 15 (very flexible) to noisy sinusoidal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate noisy sinusoidal data ---\n",
    "np.random.seed(42)\n",
    "n_samples = 30\n",
    "\n",
    "X = np.sort(np.random.uniform(0, 1, n_samples))\n",
    "y_true = np.sin(2 * np.pi * X)\n",
    "y = y_true + np.random.normal(0, 0.25, n_samples)\n",
    "\n",
    "X_plot = np.linspace(0, 1, 300)\n",
    "y_plot_true = np.sin(2 * np.pi * X_plot)\n",
    "\n",
    "print(f\"Generated {n_samples} noisy samples from sin(2*pi*x).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit polynomials of degree 1, 3, and 15 ---\n",
    "degrees = [1, 3, 15]\n",
    "labels = ['Degree 1 (High Bias)', 'Degree 3 (Good Fit)', 'Degree 15 (High Variance)']\n",
    "colors = ['steelblue', 'green', 'crimson']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, degree, label, color in zip(axes, degrees, labels, colors):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X.reshape(-1, 1), y)\n",
    "    y_pred = model.predict(X_plot.reshape(-1, 1))\n",
    "\n",
    "    ax.scatter(X, y, color='black', s=20, zorder=5, label='Training data')\n",
    "    ax.plot(X_plot, y_plot_true, 'k--', alpha=0.4, label='True function')\n",
    "    ax.plot(X_plot, y_pred, color=color, linewidth=2, label=f'Poly degree {degree}')\n",
    "    ax.set_title(label, fontsize=13)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 1:  Too simple -- misses the curve (HIGH BIAS).\")\n",
    "print(\"Degree 3:  Captures the shape without chasing noise (GOOD BALANCE).\")\n",
    "print(\"Degree 15: Fits noise exactly -- wild swings on unseen data (HIGH VARIANCE).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train/test error vs polynomial degree ---\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.reshape(-1, 1), y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "max_degree = 15\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for d in range(1, max_degree + 1):\n",
    "    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    train_errors.append(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    test_errors.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(range(1, max_degree + 1), train_errors, 'o-', label='Training Error', color='steelblue')\n",
    "ax.plot(range(1, max_degree + 1), test_errors, 's-', label='Test Error', color='crimson')\n",
    "ax.set_xlabel('Polynomial Degree')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Model Complexity vs Training/Test Error')\n",
    "ax.set_ylim(0, min(2, max(test_errors) * 1.1))\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The classic U-shaped test curve:\")\n",
    "print(\"  - Left side: high bias (underfitting)\")\n",
    "print(\"  - Right side: high variance (overfitting)\")\n",
    "print(\"  - Minimum: the sweet spot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Learning Curves\n",
    "\n",
    "A **learning curve** plots training and validation scores as a function of the **number of training samples**.\n",
    "\n",
    "### How to read them:\n",
    "\n",
    "| Pattern | Diagnosis | Action |\n",
    "|---------|-----------|--------|\n",
    "| Both scores low, close together | **High bias** (underfitting) | Increase model complexity, add features |\n",
    "| Training score high, validation low, large gap | **High variance** (overfitting) | More data, regularization, simpler model |\n",
    "| Both scores high, close together | **Good fit** | You are done |\n",
    "\n",
    "Let's use `sklearn.model_selection.learning_curve` to generate these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate more data for learning curve analysis ---\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X_lc = np.sort(np.random.uniform(0, 1, n)).reshape(-1, 1)\n",
    "y_lc = np.sin(2 * np.pi * X_lc.ravel()) + np.random.normal(0, 0.25, n)\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ax):\n",
    "    \"\"\"Plot a learning curve using sklearn.\"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        random_state=42\n",
    "    )\n",
    "    train_mean = -train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = -val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "\n",
    "    ax.plot(train_sizes, train_mean, 'o-', color='steelblue', label='Training Error')\n",
    "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                    alpha=0.15, color='steelblue')\n",
    "    ax.plot(train_sizes, val_mean, 's-', color='crimson', label='Validation Error')\n",
    "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
    "                    alpha=0.15, color='crimson')\n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# High bias model\n",
    "model_bias = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "plot_learning_curve(model_bias, 'Degree 1 -- High Bias', X_lc, y_lc, axes[0])\n",
    "\n",
    "# Good fit model\n",
    "model_good = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "plot_learning_curve(model_good, 'Degree 3 -- Good Fit', X_lc, y_lc, axes[1])\n",
    "\n",
    "# High variance model\n",
    "model_var = make_pipeline(PolynomialFeatures(15), LinearRegression())\n",
    "plot_learning_curve(model_var, 'Degree 15 -- High Variance', X_lc, y_lc, axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 1:  Both curves converge high  --> HIGH BIAS (more data won't help).\")\n",
    "print(\"Degree 3:  Both curves converge low    --> GOOD FIT.\")\n",
    "print(\"Degree 15: Large gap between curves     --> HIGH VARIANCE (more data may help).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Common Mistakes\n",
    "\n",
    "1. **Confusing bias and variance**\n",
    "   - High **bias** = underfitting (model too simple, poor on training AND test)\n",
    "   - High **variance** = overfitting (model too complex, great on training, poor on test)\n",
    "\n",
    "2. **Not looking at learning curves**\n",
    "   - A single train/test split score does not tell you whether to get more data, add features, or simplify the model\n",
    "   - Always plot learning curves before deciding your next step\n",
    "\n",
    "3. **Confusing loss and cost**\n",
    "   - Loss = one sample, cost = average over all samples\n",
    "   - Optimization minimizes the **cost** function, not individual losses\n",
    "\n",
    "4. **Using the wrong loss for the task**\n",
    "   - MSE/MAE for regression, cross-entropy for classification\n",
    "   - Using MSE for classification can work but converges poorly\n",
    "\n",
    "5. **Assuming more data always helps**\n",
    "   - If your model has high bias, more data will **not** help\n",
    "   - Learning curves make this obvious: if both curves have already converged, more data is useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Exercise\n",
    "\n",
    "**Task:** Use the California Housing dataset to explore the bias-variance tradeoff.\n",
    "\n",
    "1. Load the dataset with `sklearn.datasets.fetch_california_housing()`\n",
    "2. Use only the first feature (`MedInc`) for simplicity\n",
    "3. Fit polynomial regression with degrees 1, 3, 5, 10\n",
    "4. Plot the **learning curve** for each degree (use the `plot_learning_curve` helper above)\n",
    "5. Answer:\n",
    "   - Which degree shows high bias?\n",
    "   - Which degree shows high variance?\n",
    "   - Which degree gives the best balance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# data = fetch_california_housing()\n",
    "# X_ex = data.data[:, [0]]  # MedInc only\n",
    "# y_ex = data.target\n",
    "#\n",
    "# ... fit models and plot learning curves ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}