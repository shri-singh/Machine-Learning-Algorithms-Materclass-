{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Optimization Intuition\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain **optimization** as the process of finding parameters that minimize a cost function\n",
    "- Describe the **gradient descent** algorithm and its intuition (ball rolling downhill)\n",
    "- Write and interpret the parameter update rule\n",
    "- Explain the effect of the **learning rate** on convergence\n",
    "- Distinguish between **batch**, **stochastic**, and **mini-batch** gradient descent\n",
    "- Implement gradient descent from scratch for simple linear regression\n",
    "- Visualize the optimization path on a contour/surface plot\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of loss and cost functions (Notebook 01)\n",
    "- Basic linear regression concepts (ML200)\n",
    "- Basic calculus intuition (what a derivative/gradient means)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What Is Optimization?](#1)\n",
    "2. [Gradient Descent Intuition](#2)\n",
    "3. [The Update Rule](#3)\n",
    "4. [Learning Rate Effects](#4)\n",
    "5. [Batch vs Stochastic vs Mini-Batch](#5)\n",
    "6. [Convex vs Non-Convex](#6)\n",
    "7. [Gradient Descent from Scratch](#7)\n",
    "8. [Visualizing the Optimization Path](#8)\n",
    "9. [Common Mistakes](#9)\n",
    "10. [Exercise](#10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. What Is Optimization?\n",
    "\n",
    "In machine learning, **optimization** means:\n",
    "\n",
    "> Find the parameters $\\theta$ that **minimize** the cost function $J(\\theta)$.\n",
    "\n",
    "$$\\theta^* = \\arg\\min_{\\theta} J(\\theta)$$\n",
    "\n",
    "For linear regression with MSE:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - (wx^{(i)} + b))^2$$\n",
    "\n",
    "We need to find the values of $w$ and $b$ that make $J$ as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Gradient Descent Intuition\n",
    "\n",
    "Imagine you are standing on a hilly surface **blindfolded** and want to reach the lowest point:\n",
    "\n",
    "1. **Feel the slope** beneath your feet (compute the gradient)\n",
    "2. **Take a step downhill** in the steepest direction (update parameters)\n",
    "3. **Repeat** until the ground feels flat (gradient near zero)\n",
    "\n",
    "The **gradient** $\\nabla J$ tells you:\n",
    "- The **direction** of steepest ascent\n",
    "- The **magnitude** of the slope\n",
    "\n",
    "We move in the **opposite** direction (steepest descent) to minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. The Update Rule\n",
    "\n",
    "The gradient descent update rule for a parameter $w$:\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = **learning rate** (step size)\n",
    "- $\\frac{\\partial J}{\\partial w}$ = partial derivative of the cost with respect to $w$\n",
    "\n",
    "For simple linear regression $\\hat{y} = wx + b$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = \\frac{-2}{n}\\sum_{i=1}^{n} x^{(i)}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{-2}{n}\\sum_{i=1}^{n} (y^{(i)} - \\hat{y}^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Learning Rate Effects\n",
    "\n",
    "| Learning Rate | Behavior |\n",
    "|---------------|----------|\n",
    "| Too small ($\\alpha \\ll$) | Converges, but extremely slowly |\n",
    "| Just right | Smooth convergence to the minimum |\n",
    "| Too large ($\\alpha \\gg$) | Oscillates wildly, may **diverge** |\n",
    "\n",
    "Let's visualize this with a simple 1D quadratic cost function $J(w) = (w - 3)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demonstrate learning rate effect on a simple 1D cost function ---\n",
    "def cost_1d(w):\n",
    "    return (w - 3) ** 2\n",
    "\n",
    "def grad_1d(w):\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "learning_rates = [0.01, 0.3, 0.95]\n",
    "titles = ['Too Small ($\\\\alpha=0.01$)', 'Just Right ($\\\\alpha=0.3$)', 'Too Large ($\\\\alpha=0.95$)']\n",
    "colors = ['steelblue', 'green', 'crimson']\n",
    "n_steps = 30\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "w_range = np.linspace(-2, 8, 200)\n",
    "\n",
    "for ax, lr, title, color in zip(axes, learning_rates, titles, colors):\n",
    "    ax.plot(w_range, cost_1d(w_range), 'k-', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "    w = -1.0  # starting point\n",
    "    path_w = [w]\n",
    "    path_j = [cost_1d(w)]\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        w = w - lr * grad_1d(w)\n",
    "        path_w.append(w)\n",
    "        path_j.append(cost_1d(w))\n",
    "\n",
    "    ax.plot(path_w, path_j, 'o-', color=color, markersize=5, linewidth=1.5, label=f'GD path')\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('J(w)')\n",
    "    ax.set_ylim(-1, 30)\n",
    "    ax.set_xlim(-3, 9)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Too small:  Gets there eventually but wastes computation.\")\n",
    "print(\"Just right: Converges smoothly in a few steps.\")\n",
    "print(\"Too large:  Oscillates around the minimum (may diverge if > 1.0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Batch vs Stochastic vs Mini-Batch\n",
    "\n",
    "| Variant | Gradient computed on | Pros | Cons |\n",
    "|---------|---------------------|------|------|\n",
    "| **Batch GD** | All $n$ samples | Stable, smooth convergence | Slow for large datasets |\n",
    "| **Stochastic GD (SGD)** | 1 sample at a time | Fast updates, can escape local minima | Noisy, oscillates |\n",
    "| **Mini-batch GD** | A batch of $m$ samples | Balance of speed and stability | Requires batch size tuning |\n",
    "\n",
    "- **Batch GD** is what we implement below (and what sklearn uses for linear/logistic regression).\n",
    "- **SGD** and **mini-batch** are essential for deep learning where datasets are too large to fit in memory.\n",
    "- For tree-based models (Random Forest, XGBoost), gradient descent is **not** used -- those models use different optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Convex vs Non-Convex\n",
    "\n",
    "- **Convex** cost function: has exactly **one global minimum** (shaped like a bowl)\n",
    "  - Linear regression (MSE) is convex\n",
    "  - Logistic regression (cross-entropy) is convex\n",
    "  - Gradient descent is **guaranteed** to find the global minimum\n",
    "\n",
    "- **Non-convex** cost function: has **multiple local minima**\n",
    "  - Neural networks are non-convex\n",
    "  - Gradient descent may get stuck in a local minimum\n",
    "\n",
    "For this course, all models we study (linear regression, logistic regression) have convex cost functions, so gradient descent will always converge to the optimal solution (given an appropriate learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convex vs Non-Convex visualization ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Convex\n",
    "x_conv = np.linspace(-4, 4, 200)\n",
    "y_conv = x_conv ** 2\n",
    "axes[0].plot(x_conv, y_conv, 'steelblue', linewidth=2.5)\n",
    "axes[0].scatter([0], [0], color='green', s=100, zorder=5, label='Global minimum')\n",
    "axes[0].set_title('Convex: One Global Minimum', fontsize=13)\n",
    "axes[0].set_xlabel('w')\n",
    "axes[0].set_ylabel('J(w)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Non-convex\n",
    "x_nc = np.linspace(-4, 4, 200)\n",
    "y_nc = x_nc ** 4 - 8 * x_nc ** 2 + 5\n",
    "axes[1].plot(x_nc, y_nc, 'crimson', linewidth=2.5)\n",
    "# Mark local and global minima\n",
    "local_min_x = np.array([-2.0, 2.0])\n",
    "local_min_y = local_min_x ** 4 - 8 * local_min_x ** 2 + 5\n",
    "axes[1].scatter(local_min_x, local_min_y, color='green', s=100, zorder=5, label='Minima')\n",
    "axes[1].scatter([0], [5], color='orange', s=100, zorder=5, marker='^', label='Local maximum')\n",
    "axes[1].set_title('Non-Convex: Multiple Minima', fontsize=13)\n",
    "axes[1].set_xlabel('w')\n",
    "axes[1].set_ylabel('J(w)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Gradient Descent from Scratch\n",
    "\n",
    "We will implement batch gradient descent for simple linear regression (1 feature):\n",
    "\n",
    "$$\\hat{y} = wx + b$$\n",
    "$$J(w, b) = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - wx^{(i)} - b)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate simple linear data ---\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "X_raw = 2 * np.random.rand(n)\n",
    "y_data = 4 + 3 * X_raw + np.random.randn(n) * 0.5\n",
    "\n",
    "# Standardize X for better gradient descent behavior\n",
    "X_mean, X_std = X_raw.mean(), X_raw.std()\n",
    "X_data = (X_raw - X_mean) / X_std\n",
    "\n",
    "plt.scatter(X_data, y_data, alpha=0.6, s=20)\n",
    "plt.xlabel('x (standardized)')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"True relationship: y = 4 + 3*x  (before standardization)\")\n",
    "print(f\"Samples: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gradient descent implementation ---\n",
    "def gradient_descent(X, y, lr=0.1, n_iters=100):\n",
    "    \"\"\"\n",
    "    Batch gradient descent for simple linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n,)  -- feature values\n",
    "    y : array, shape (n,)  -- target values\n",
    "    lr : float             -- learning rate\n",
    "    n_iters : int          -- number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w, b : final parameters\n",
    "    history : dict with 'cost', 'w', 'b' lists\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    w = 0.0   # initialize weight\n",
    "    b = 0.0   # initialize bias\n",
    "\n",
    "    history = {'cost': [], 'w': [], 'b': []}\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        # Forward pass: predictions\n",
    "        y_hat = w * X + b\n",
    "\n",
    "        # Compute cost (MSE)\n",
    "        cost = np.mean((y - y_hat) ** 2)\n",
    "        history['cost'].append(cost)\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (-2 / n) * np.sum(X * (y - y_hat))\n",
    "        db = (-2 / n) * np.sum(y - y_hat)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "# Run gradient descent\n",
    "w_final, b_final, hist = gradient_descent(X_data, y_data, lr=0.1, n_iters=200)\n",
    "\n",
    "print(f\"Learned parameters:  w = {w_final:.4f},  b = {b_final:.4f}\")\n",
    "print(f\"Final cost (MSE):    {hist['cost'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot: Cost vs iterations (convergence) ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cost curve\n",
    "axes[0].plot(hist['cost'], color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Cost (MSE)')\n",
    "axes[0].set_title('Cost vs Iterations -- Convergence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fit line\n",
    "axes[1].scatter(X_data, y_data, alpha=0.5, s=20, label='Data')\n",
    "x_line = np.linspace(X_data.min(), X_data.max(), 100)\n",
    "axes[1].plot(x_line, w_final * x_line + b_final, 'r-', linewidth=2, label=f'Fit: y={w_final:.2f}x+{b_final:.2f}')\n",
    "axes[1].set_xlabel('x (standardized)')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Fitted Line after Gradient Descent')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare different learning rates ---\n",
    "learning_rates_cmp = [0.001, 0.01, 0.1, 0.5]\n",
    "colors_cmp = ['gray', 'steelblue', 'green', 'crimson']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for lr, color in zip(learning_rates_cmp, colors_cmp):\n",
    "    _, _, h = gradient_descent(X_data, y_data, lr=lr, n_iters=200)\n",
    "    ax.plot(h['cost'], color=color, linewidth=2, label=f'$\\\\alpha={lr}$')\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cost (MSE)')\n",
    "ax.set_title('Learning Rate Comparison')\n",
    "ax.set_ylim(0, 30)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"alpha=0.001: Very slow convergence (needs more iterations).\")\n",
    "print(\"alpha=0.01:  Moderate convergence.\")\n",
    "print(\"alpha=0.1:   Fast, smooth convergence.\")\n",
    "print(\"alpha=0.5:   Still converges but less smoothly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Visualizing the Optimization Path\n",
    "\n",
    "We can visualize gradient descent as a path on the **cost surface** (3D) or **contour plot** (2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the cost surface for visualization ---\n",
    "w_range = np.linspace(-2, 4, 100)\n",
    "b_range = np.linspace(3, 8, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Compute cost for each (w, b) pair\n",
    "Cost_surface = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        y_hat = W[i, j] * X_data + B[i, j]\n",
    "        Cost_surface[i, j] = np.mean((y_data - y_hat) ** 2)\n",
    "\n",
    "print(f\"Cost surface computed over a {W.shape[0]}x{W.shape[1]} grid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3D Surface Plot ---\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(W, B, Cost_surface, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "ax.plot(hist['w'], hist['b'], hist['cost'], 'r.-', markersize=4, linewidth=1.5, label='GD path')\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Cost J(w,b)')\n",
    "ax.set_title('Cost Surface with Gradient Descent Path')\n",
    "ax.view_init(elev=30, azim=220)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2D Contour Plot with GD path ---\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "contour = ax.contour(W, B, Cost_surface, levels=30, cmap='viridis')\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot the GD path\n",
    "ax.plot(hist['w'], hist['b'], 'ro-', markersize=3, linewidth=1.5, label='GD path')\n",
    "ax.plot(hist['w'][0], hist['b'][0], 'rs', markersize=12, label='Start')\n",
    "ax.plot(hist['w'][-1], hist['b'][-1], 'r*', markersize=15, label='End')\n",
    "\n",
    "ax.set_xlabel('w', fontsize=13)\n",
    "ax.set_ylabel('b', fontsize=13)\n",
    "ax.set_title('Contour Plot with Gradient Descent Path', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The path starts at (w=0, b=0) and converges to the minimum of the bowl-shaped cost surface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Common Mistakes\n",
    "\n",
    "1. **Wrong learning rate**\n",
    "   - Too small: wastes compute, may appear to \"not learn\"\n",
    "   - Too large: cost oscillates or diverges (NaN values)\n",
    "   - Always plot cost vs iterations to check convergence\n",
    "\n",
    "2. **Not scaling features before gradient descent**\n",
    "   - Features on different scales create elongated contours\n",
    "   - GD zigzags and converges very slowly\n",
    "   - **Always standardize** (zero mean, unit variance) before running GD\n",
    "\n",
    "3. **Expecting gradient descent for tree-based models**\n",
    "   - Decision Trees, Random Forest use recursive splitting, not GD\n",
    "   - Gradient Boosting uses gradients conceptually, but not in the same parameter-update sense\n",
    "   - GD is for **parametric** models: linear regression, logistic regression, neural networks\n",
    "\n",
    "4. **Confusing batch size terminology**\n",
    "   - \"Batch\" GD uses ALL data per update\n",
    "   - \"Mini-batch\" uses a subset (e.g., 32, 64 samples)\n",
    "   - \"Stochastic\" uses 1 sample per update\n",
    "\n",
    "5. **Not running enough iterations**\n",
    "   - If the cost curve has not flattened, the model has not converged\n",
    "   - Always check the cost-vs-iterations plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Exercise\n",
    "\n",
    "**Task:** Extend gradient descent to **multiple features**.\n",
    "\n",
    "1. Generate 2-feature data: `X = np.random.randn(200, 2)`, `y = 3*X[:,0] + 5*X[:,1] + 2 + noise`\n",
    "2. Modify the `gradient_descent` function to handle a weight vector `w` of shape `(2,)` and a scalar bias `b`\n",
    "   - Hint: predictions become `y_hat = X @ w + b`\n",
    "   - Gradient for `w`: `dw = (-2/n) * X.T @ (y - y_hat)`\n",
    "3. Run GD with `lr=0.1` for 500 iterations\n",
    "4. Print the learned `w` and `b` -- do they match the true values?\n",
    "5. Plot cost vs iterations to verify convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# np.random.seed(42)\n",
    "# X_ex = np.random.randn(200, 2)\n",
    "# y_ex = 3 * X_ex[:, 0] + 5 * X_ex[:, 1] + 2 + np.random.randn(200) * 0.5\n",
    "#\n",
    "# def gradient_descent_multi(X, y, lr=0.1, n_iters=500):\n",
    "#     ...\n",
    "#     return w, b, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}