{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization: Geometric Intuition\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain **why** regularization is needed (prevent overfitting by constraining weights)\n",
    "- Write the cost functions for **Ridge (L2)** and **Lasso (L1)** regularization\n",
    "- Explain the **geometric intuition** behind why L1 produces sparse solutions and L2 does not\n",
    "- Describe **ElasticNet** as a combination of L1 and L2\n",
    "- Use sklearn to fit Ridge, Lasso, and ElasticNet and compare their coefficient behavior\n",
    "- Visualize how coefficients change with the regularization strength $\\alpha$\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Loss and cost functions (Notebook 01)\n",
    "- Gradient descent and optimization (Notebook 02)\n",
    "- Linear regression (ML200)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Regularization?](#1)\n",
    "2. [L2 Regularization (Ridge)](#2)\n",
    "3. [L1 Regularization (Lasso)](#3)\n",
    "4. [Geometric Intuition: Why L1 Gives Sparse Solutions](#4)\n",
    "5. [ElasticNet](#5)\n",
    "6. [Code: Comparing Ridge, Lasso, and ElasticNet](#6)\n",
    "7. [Coefficient Paths vs Alpha](#7)\n",
    "8. [Common Mistakes](#8)\n",
    "9. [Exercise](#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Why Regularization?\n",
    "\n",
    "When a model overfits, its weights tend to become **very large** -- the model contorts itself to fit noise in the training data.\n",
    "\n",
    "**Regularization** adds a penalty term to the cost function that discourages large weights:\n",
    "\n",
    "$$J_{\\text{regularized}} = J_{\\text{data}} + \\lambda \\cdot \\text{penalty}(\\mathbf{w})$$\n",
    "\n",
    "This forces the model to find a balance between:\n",
    "- Fitting the training data well (low data loss)\n",
    "- Keeping the weights small (low penalty)\n",
    "\n",
    "The result is a **simpler, more generalizable** model.\n",
    "\n",
    "> **Note:** In sklearn, the regularization strength parameter is called `alpha` (not $\\lambda$). Larger `alpha` = stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. L2 Regularization (Ridge)\n",
    "\n",
    "Ridge regression adds the **sum of squared weights** to the cost function:\n",
    "\n",
    "$$J_{\\text{Ridge}} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2 + \\alpha \\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "**Key properties:**\n",
    "- Shrinks all coefficients **toward zero**, but never exactly to zero\n",
    "- The geometric constraint region is a **circle** (or hypersphere in higher dimensions)\n",
    "- Good when you believe all features are somewhat relevant\n",
    "- Has a closed-form solution: $\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. L1 Regularization (Lasso)\n",
    "\n",
    "Lasso regression adds the **sum of absolute values** of the weights:\n",
    "\n",
    "$$J_{\\text{Lasso}} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "**Key properties:**\n",
    "- Can shrink coefficients **exactly to zero** -- performs automatic **feature selection**\n",
    "- The geometric constraint region is a **diamond** (or cross-polytope)\n",
    "- Useful when you suspect many features are irrelevant\n",
    "- No closed-form solution -- requires iterative optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Geometric Intuition: Why L1 Gives Sparse Solutions\n",
    "\n",
    "The key insight is about where the **MSE contour ellipses** intersect the **constraint region**:\n",
    "\n",
    "- **L2 (Ridge):** The constraint is a **circle**. The MSE contours are most likely to touch the circle at a point where **no coordinate is exactly zero**.\n",
    "- **L1 (Lasso):** The constraint is a **diamond** with **corners on the axes**. The MSE contours are most likely to touch a corner, where **one or more coordinates are exactly zero**.\n",
    "\n",
    "This is why L1 produces **sparse** solutions (many zero coefficients) and L2 does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Geometric visualization: MSE contours + L1 diamond + L2 circle ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create MSE-like elliptical contours (centered off-origin to simulate unconstrained optimum)\n",
    "w1_range = np.linspace(-3, 3, 300)\n",
    "w2_range = np.linspace(-3, 3, 300)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "\n",
    "# Elliptical cost centered at (1.5, 1.0) -- the unconstrained OLS solution\n",
    "ols_w1, ols_w2 = 1.5, 1.0\n",
    "Z = 2 * (W1 - ols_w1) ** 2 + 3 * (W2 - ols_w2) ** 2 + 1.5 * (W1 - ols_w1) * (W2 - ols_w2)\n",
    "\n",
    "# --- L2 (Ridge): circle constraint ---\n",
    "ax = axes[0]\n",
    "ax.contour(W1, W2, Z, levels=15, cmap='Blues', alpha=0.8)\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False, color='green', linewidth=3, linestyle='-', label='L2 constraint (circle)')\n",
    "ax.add_patch(circle)\n",
    "ax.plot(ols_w1, ols_w2, 'r*', markersize=15, label=f'OLS solution ({ols_w1}, {ols_w2})')\n",
    "\n",
    "# Approximate Ridge solution (on the circle, not on an axis)\n",
    "ridge_angle = np.arctan2(ols_w2, ols_w1)\n",
    "ridge_w1 = 0.83 * np.cos(ridge_angle)\n",
    "ridge_w2 = 0.83 * np.sin(ridge_angle)\n",
    "ax.plot(ridge_w1, ridge_w2, 'go', markersize=12, label=f'Ridge solution ({ridge_w1:.2f}, {ridge_w2:.2f})')\n",
    "\n",
    "ax.set_xlabel('$w_1$', fontsize=14)\n",
    "ax.set_ylabel('$w_2$', fontsize=14)\n",
    "ax.set_title('L2 (Ridge): Circle Constraint', fontsize=14)\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(0, color='gray', linewidth=0.5)\n",
    "ax.axvline(0, color='gray', linewidth=0.5)\n",
    "ax.legend(fontsize=9, loc='lower left')\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "# --- L1 (Lasso): diamond constraint ---\n",
    "ax = axes[1]\n",
    "ax.contour(W1, W2, Z, levels=15, cmap='Blues', alpha=0.8)\n",
    "\n",
    "# Draw diamond\n",
    "diamond_size = 1.0\n",
    "diamond_x = [diamond_size, 0, -diamond_size, 0, diamond_size]\n",
    "diamond_y = [0, diamond_size, 0, -diamond_size, 0]\n",
    "ax.plot(diamond_x, diamond_y, 'darkorange', linewidth=3, label='L1 constraint (diamond)')\n",
    "\n",
    "ax.plot(ols_w1, ols_w2, 'r*', markersize=15, label=f'OLS solution ({ols_w1}, {ols_w2})')\n",
    "# Lasso solution touches corner -- w2 = 0\n",
    "ax.plot(1.0, 0.0, 'o', color='darkorange', markersize=12, label=f'Lasso solution (1.0, 0.0)')\n",
    "\n",
    "ax.set_xlabel('$w_1$', fontsize=14)\n",
    "ax.set_ylabel('$w_2$', fontsize=14)\n",
    "ax.set_title('L1 (Lasso): Diamond Constraint', fontsize=14)\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-2.5, 2.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(0, color='gray', linewidth=0.5)\n",
    "ax.axvline(0, color='gray', linewidth=0.5)\n",
    "ax.legend(fontsize=9, loc='lower left')\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left (Ridge):  The contours touch the circle away from the axes -- both w1, w2 are non-zero.\")\n",
    "print(\"Right (Lasso): The contours touch the diamond at a corner -- w2 is exactly zero (sparsity!).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. ElasticNet\n",
    "\n",
    "ElasticNet combines L1 and L2 regularization:\n",
    "\n",
    "$$J_{\\text{ElasticNet}} = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2 + \\alpha \\left[ \\rho \\sum |w_j| + \\frac{1-\\rho}{2} \\sum w_j^2 \\right]$$\n",
    "\n",
    "Where $\\rho$ (`l1_ratio` in sklearn) controls the mix:\n",
    "\n",
    "| `l1_ratio` | Behavior |\n",
    "|------------|----------|\n",
    "| 1.0 | Pure Lasso (L1 only) |\n",
    "| 0.0 | Pure Ridge (L2 only) |\n",
    "| 0.5 | Equal mix |\n",
    "\n",
    "**When to use ElasticNet:**\n",
    "- When you have correlated features (Lasso alone may arbitrarily pick one)\n",
    "- When you want some feature selection (from L1) with stability (from L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Code: Comparing Ridge, Lasso, and ElasticNet\n",
    "\n",
    "We will create a dataset with:\n",
    "- 5 **relevant** features (with true non-zero coefficients)\n",
    "- 15 **irrelevant** features (noise, true coefficient = 0)\n",
    "\n",
    "Then we will see which methods correctly identify the irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create dataset with relevant and irrelevant features ---\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_relevant = 5\n",
    "n_irrelevant = 15\n",
    "n_features = n_relevant + n_irrelevant\n",
    "\n",
    "# Generate features\n",
    "X_all = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# True coefficients: first 5 are non-zero, rest are zero\n",
    "true_coefs = np.zeros(n_features)\n",
    "true_coefs[:n_relevant] = [3.0, -2.0, 1.5, -1.0, 0.5]\n",
    "\n",
    "# Generate target\n",
    "y_all = X_all @ true_coefs + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "feature_names = [f'Relevant_{i+1}' for i in range(n_relevant)] + \\\n",
    "                [f'Noise_{i+1}' for i in range(n_irrelevant)]\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"  - {n_relevant} relevant features with true coefficients: {true_coefs[:n_relevant]}\")\n",
    "print(f\"  - {n_irrelevant} irrelevant noise features (true coefficient = 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit all models ---\n",
    "alpha_val = 0.5\n",
    "\n",
    "models = {\n",
    "    'OLS (no regularization)': LinearRegression(),\n",
    "    'Ridge (L2)': Ridge(alpha=alpha_val),\n",
    "    'Lasso (L1)': Lasso(alpha=alpha_val, max_iter=10000),\n",
    "    'ElasticNet (L1+L2)': ElasticNet(alpha=alpha_val, l1_ratio=0.5, max_iter=10000),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_s, y_train)\n",
    "    train_mse = mean_squared_error(y_train, model.predict(X_train_s))\n",
    "    test_mse = mean_squared_error(y_test, model.predict(X_test_s))\n",
    "    n_zeros = np.sum(np.abs(model.coef_) < 1e-6)\n",
    "    results[name] = {\n",
    "        'coefs': model.coef_,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'n_zeros': n_zeros,\n",
    "    }\n",
    "    print(f\"{name:30s}  Train MSE: {train_mse:.4f}  Test MSE: {test_mse:.4f}  Zero coefs: {n_zeros}/{n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare coefficients across models ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['steelblue' if i < n_relevant else 'lightcoral' for i in range(n_features)]\n",
    "\n",
    "for ax, (name, res) in zip(axes, results.items()):\n",
    "    bars = ax.bar(range(n_features), res['coefs'], color=colors, edgecolor='gray', linewidth=0.5)\n",
    "    ax.axhline(0, color='black', linewidth=0.8)\n",
    "    ax.set_title(f\"{name}\\nZero coefficients: {res['n_zeros']}/{n_features}\", fontsize=12)\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Coefficient Value')\n",
    "    ax.set_xticks(range(n_features))\n",
    "    ax.set_xticklabels([str(i) for i in range(n_features)], fontsize=8)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend to first subplot\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='steelblue', label='Relevant features (0-4)'),\n",
    "                   Patch(facecolor='lightcoral', label='Noise features (5-19)')]\n",
    "axes[0].legend(handles=legend_elements, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OLS:        Fits noise features with non-zero coefficients (overfitting risk).\")\n",
    "print(\"Ridge (L2): Shrinks all coefficients, but none are exactly zero.\")\n",
    "print(\"Lasso (L1): Drives noise feature coefficients to EXACTLY zero (feature selection!).\")\n",
    "print(\"ElasticNet: Some sparsity from L1, some shrinkage from L2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Coefficient Paths vs Alpha\n",
    "\n",
    "How do the coefficients change as we increase the regularization strength $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Coefficient paths: Ridge vs Lasso ---\n",
    "alphas = np.logspace(-3, 2, 100)\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X_train_s, y_train)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "    lasso = Lasso(alpha=a, max_iter=10000)\n",
    "    lasso.fit(X_train_s, y_train)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge coefficient paths\n",
    "ax = axes[0]\n",
    "for j in range(n_features):\n",
    "    color = 'steelblue' if j < n_relevant else 'lightcoral'\n",
    "    lw = 2.0 if j < n_relevant else 0.8\n",
    "    alpha_line = 1.0 if j < n_relevant else 0.5\n",
    "    ax.plot(alphas, ridge_coefs[:, j], color=color, linewidth=lw, alpha=alpha_line)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('$\\\\alpha$ (regularization strength)', fontsize=13)\n",
    "ax.set_ylabel('Coefficient value', fontsize=13)\n",
    "ax.set_title('Ridge (L2): Coefficient Paths', fontsize=14)\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso coefficient paths\n",
    "ax = axes[1]\n",
    "for j in range(n_features):\n",
    "    color = 'steelblue' if j < n_relevant else 'lightcoral'\n",
    "    lw = 2.0 if j < n_relevant else 0.8\n",
    "    alpha_line = 1.0 if j < n_relevant else 0.5\n",
    "    ax.plot(alphas, lasso_coefs[:, j], color=color, linewidth=lw, alpha=alpha_line)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('$\\\\alpha$ (regularization strength)', fontsize=13)\n",
    "ax.set_ylabel('Coefficient value', fontsize=13)\n",
    "ax.set_title('Lasso (L1): Coefficient Paths', fontsize=14)\n",
    "ax.axhline(0, color='black', linewidth=0.8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add shared legend\n",
    "legend_elements = [Patch(facecolor='steelblue', label='Relevant features'),\n",
    "                   Patch(facecolor='lightcoral', label='Noise features')]\n",
    "axes[0].legend(handles=legend_elements, fontsize=10)\n",
    "axes[1].legend(handles=legend_elements, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ridge: All coefficients shrink toward zero but NEVER reach exactly zero.\")\n",
    "print(\"Lasso: Noise features hit zero quickly; relevant features survive longer.\")\n",
    "print(\"       This is automatic feature selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test MSE vs alpha for Ridge and Lasso ---\n",
    "ridge_test_mse = []\n",
    "lasso_test_mse = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X_train_s, y_train)\n",
    "    ridge_test_mse.append(mean_squared_error(y_test, ridge.predict(X_test_s)))\n",
    "\n",
    "    lasso = Lasso(alpha=a, max_iter=10000)\n",
    "    lasso.fit(X_train_s, y_train)\n",
    "    lasso_test_mse.append(mean_squared_error(y_test, lasso.predict(X_test_s)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(alphas, ridge_test_mse, 'o-', color='green', markersize=2, linewidth=2, label='Ridge (L2)')\n",
    "ax.plot(alphas, lasso_test_mse, 's-', color='darkorange', markersize=2, linewidth=2, label='Lasso (L1)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('$\\\\alpha$ (regularization strength)', fontsize=13)\n",
    "ax.set_ylabel('Test MSE', fontsize=13)\n",
    "ax.set_title('Test Error vs Regularization Strength', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_ridge_alpha = alphas[np.argmin(ridge_test_mse)]\n",
    "best_lasso_alpha = alphas[np.argmin(lasso_test_mse)]\n",
    "print(f\"Best Ridge alpha: {best_ridge_alpha:.4f} (Test MSE: {min(ridge_test_mse):.4f})\")\n",
    "print(f\"Best Lasso alpha: {best_lasso_alpha:.4f} (Test MSE: {min(lasso_test_mse):.4f})\")\n",
    "print(\"\\nToo little regularization -> overfitting. Too much -> underfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Not standardizing features before regularization**\n",
    "   - Regularization penalizes large coefficients\n",
    "   - If features have different scales, the penalty is **unfair** -- it penalizes features with large scales more\n",
    "   - **Always** use `StandardScaler` before Ridge, Lasso, or ElasticNet\n",
    "\n",
    "2. **Using too large an alpha**\n",
    "   - Drives all coefficients to near-zero -- the model predicts (roughly) the mean of y\n",
    "   - This is **underfitting** caused by over-regularization\n",
    "   - Use cross-validation (`RidgeCV`, `LassoCV`) to find the optimal alpha\n",
    "\n",
    "3. **Forgetting that the intercept is NOT regularized**\n",
    "   - By default, sklearn does not penalize the intercept (bias term)\n",
    "   - This is correct -- the intercept should be free to shift the predictions\n",
    "\n",
    "4. **Using Lasso with highly correlated features**\n",
    "   - Lasso arbitrarily picks one of the correlated features and zeros out the others\n",
    "   - Use ElasticNet instead -- the L2 component helps with correlated groups\n",
    "\n",
    "5. **Confusing sklearn's alpha with the textbook lambda**\n",
    "   - They are the same concept, just different naming conventions\n",
    "   - Larger alpha = stronger penalty = simpler model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Exercise\n",
    "\n",
    "**Task:** Use cross-validated regularized regression on the California Housing dataset.\n",
    "\n",
    "1. Load data: `from sklearn.datasets import fetch_california_housing`\n",
    "2. Standardize features using `StandardScaler`\n",
    "3. Use `sklearn.linear_model.LassoCV` with `cv=5` to find the best alpha automatically\n",
    "4. Use `sklearn.linear_model.RidgeCV` with a range of alphas\n",
    "5. Print the best alpha and test MSE for both\n",
    "6. Which features does Lasso zero out? Are those features truly unimportant?\n",
    "7. Plot the coefficient comparison between OLS, Ridge (best alpha), and Lasso (best alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# from sklearn.linear_model import LassoCV, RidgeCV\n",
    "#\n",
    "# data = fetch_california_housing()\n",
    "# X_ex, y_ex = data.data, data.target\n",
    "# feature_names_ex = data.feature_names\n",
    "#\n",
    "# ... standardize, fit LassoCV and RidgeCV, compare coefficients ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}