{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Hyperparameter Tuning \u2014 Grid, Random, and Bayesian Search\n",
    "\n",
    "**Module ML600 \u2014 Optimization, Regularization, and Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Distinguish **hyperparameters** from **learned parameters** and explain why tuning matters\n",
    "- Perform exhaustive search with **GridSearchCV**\n",
    "- Perform efficient random search with **RandomizedSearchCV**\n",
    "- Understand **nested cross-validation** for unbiased model evaluation\n",
    "- Describe the concept of **Bayesian optimization** and know where to find libraries (Optuna, scikit-optimize)\n",
    "- Apply best practices: always use CV, report on held-out test sets, avoid common pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Familiarity with scikit-learn estimators (`fit` / `predict` / `score`)\n",
    "- Understanding of cross-validation (see Notebook 03)\n",
    "- Basic knowledge of Random Forests or any tree-based model\n",
    "- Python libraries: `numpy`, `pandas`, `matplotlib`, `seaborn`, `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Hyperparameters vs Parameters](#1)\n",
    "2. [GridSearchCV \u2014 Exhaustive Search](#2)\n",
    "3. [RandomizedSearchCV \u2014 Random Sampling](#3)\n",
    "4. [Grid vs Random: Time and Performance Comparison](#4)\n",
    "5. [Nested Cross-Validation](#5)\n",
    "6. [Bayesian Optimization (Conceptual)](#6)\n",
    "7. [Best Practices](#7)\n",
    "8. [Common Mistakes](#8)\n",
    "9. [Exercise](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV,\n",
    "    cross_val_score, KFold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set:     {X_test.shape[0]} samples')\n",
    "print(f'Features:     {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Hyperparameters vs Parameters\n",
    "\n",
    "| Aspect | **Parameters** | **Hyperparameters** |\n",
    "|--------|---------------|--------------------|\n",
    "| Set by | Learning algorithm during training | Engineer **before** training |\n",
    "| Examples | Weights in linear regression, split thresholds in trees | `n_estimators`, `max_depth`, `learning_rate`, `C` |\n",
    "| How chosen | Optimized by the loss function | Chosen via search + cross-validation |\n",
    "\n",
    "### Why tuning matters\n",
    "\n",
    "- **Default hyperparameters** are sensible starting points but rarely optimal for your data\n",
    "- Under-tuning can leave significant performance on the table\n",
    "- Over-tuning on training data leads to **overfitting**\n",
    "- Proper tuning with CV gives an **honest estimate** of generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: RandomForest with defaults\n",
    "rf_default = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_default.fit(X_train, y_train)\n",
    "default_acc = rf_default.score(X_test, y_test)\n",
    "print(f'Default RandomForest accuracy: {default_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. GridSearchCV \u2014 Exhaustive Search\n",
    "\n",
    "**GridSearchCV** tries **every combination** in a parameter grid and picks the best one via cross-validation.\n",
    "\n",
    "Key arguments:\n",
    "- `estimator` \u2014 the model\n",
    "- `param_grid` \u2014 dict mapping parameter names to lists of values\n",
    "- `cv` \u2014 number of folds (default 5)\n",
    "- `scoring` \u2014 metric to optimize (e.g., `'accuracy'`, `'f1'`, `'roc_auc'`)\n",
    "- `n_jobs` \u2014 parallel jobs (`-1` = all cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "total_combos = 1\n",
    "for v in param_grid.values():\n",
    "    total_combos *= len(v)\n",
    "print(f'Total combinations to evaluate: {total_combos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f'GridSearchCV completed in {grid_time:.2f} seconds')\n",
    "print(f'Best CV accuracy:  {grid_search.best_score_:.4f}')\n",
    "print(f'Best parameters:   {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on held-out test set\n",
    "grid_test_acc = grid_search.score(X_test, y_test)\n",
    "print(f'Test accuracy (GridSearchCV best): {grid_test_acc:.4f}')\n",
    "print(f'Test accuracy (default RF):        {default_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-10 hyperparameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top10 = results_df.nsmallest(10, 'rank_test_score')[[\n",
    "    'params', 'mean_test_score', 'std_test_score', 'mean_train_score', 'rank_test_score'\n",
    "]].reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(range(len(top10)), top10['mean_test_score'], xerr=top10['std_test_score'],\n",
    "        color=sns.color_palette('viridis', len(top10)), edgecolor='black')\n",
    "ax.set_yticks(range(len(top10)))\n",
    "ax.set_yticklabels([str(p) for p in top10['params']], fontsize=7)\n",
    "ax.set_xlabel('Mean CV Accuracy')\n",
    "ax.set_title('Top 10 Hyperparameter Combinations (GridSearchCV)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. RandomizedSearchCV \u2014 Random Sampling\n",
    "\n",
    "When the grid is large, exhaustive search becomes **prohibitively slow**. `RandomizedSearchCV` samples a fixed number (`n_iter`) of random parameter combinations from specified distributions.\n",
    "\n",
    "Advantages:\n",
    "- **Much faster** for large search spaces\n",
    "- Can sample from **continuous distributions** (not just discrete lists)\n",
    "- Empirically reaches near-optimal solutions with far fewer evaluations (Bergstra & Bengio, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distributions for random search\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,           # try 50 random combinations\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f'RandomizedSearchCV completed in {random_time:.2f} seconds')\n",
    "print(f'Best CV accuracy:  {random_search.best_score_:.4f}')\n",
    "print(f'Best parameters:   {random_search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_acc = random_search.score(X_test, y_test)\n",
    "print(f'Test accuracy (RandomizedSearchCV best): {random_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Grid vs Random: Time and Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Default RF', 'GridSearchCV', 'RandomizedSearchCV'],\n",
    "    'Test Accuracy': [default_acc, grid_test_acc, random_test_acc],\n",
    "    'Best CV Accuracy': [None, grid_search.best_score_, random_search.best_score_],\n",
    "    'Time (s)': [None, grid_time, random_time],\n",
    "    'Combinations Tried': [1, total_combos, 50]\n",
    "})\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = ['Default', 'Grid', 'Random']\n",
    "accs = [default_acc, grid_test_acc, random_test_acc]\n",
    "colors = ['#999999', '#2196F3', '#FF9800']\n",
    "axes[0].bar(methods, accs, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Test Accuracy Comparison')\n",
    "axes[0].set_ylim(min(accs) - 0.02, max(accs) + 0.02)\n",
    "for i, v in enumerate(accs):\n",
    "    axes[0].text(i, v + 0.003, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Time comparison\n",
    "axes[1].bar(['Grid', 'Random'], [grid_time, random_time],\n",
    "            color=['#2196F3', '#FF9800'], edgecolor='black')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Search Time Comparison')\n",
    "for i, v in enumerate([grid_time, random_time]):\n",
    "    axes[1].text(i, v + 0.1, f'{v:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Nested Cross-Validation\n",
    "\n",
    "Standard `GridSearchCV` uses a **single** train/test split for final evaluation. This can be optimistic if the test set happens to be \"easy\".\n",
    "\n",
    "**Nested CV** provides an unbiased estimate of generalization:\n",
    "\n",
    "```\n",
    "Outer CV (evaluation)\n",
    "  |--- Fold 1 train -> Inner CV (tuning) -> best params -> score on Fold 1 test\n",
    "  |--- Fold 2 train -> Inner CV (tuning) -> best params -> score on Fold 2 test\n",
    "  |--- ...\n",
    "```\n",
    "\n",
    "- **Inner loop**: tunes hyperparameters via CV\n",
    "- **Outer loop**: evaluates the *entire tuning procedure* on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV: inner loop for tuning, outer loop for evaluation\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Inner search object\n",
    "inner_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, None]\n",
    "    },\n",
    "    cv=inner_cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Outer evaluation\n",
    "nested_scores = cross_val_score(\n",
    "    inner_search, X_train, y_train, cv=outer_cv, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f'Nested CV scores: {nested_scores}')\n",
    "print(f'Nested CV mean:   {nested_scores.mean():.4f} +/- {nested_scores.std():.4f}')\n",
    "print(f'\\nThis gives an unbiased estimate of how well our tuning procedure generalizes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Bayesian Optimization (Conceptual)\n",
    "\n",
    "Both Grid and Random search are **uninformed** \u2014 each trial ignores the results of previous trials.\n",
    "\n",
    "**Bayesian optimization** builds a **surrogate model** (usually a Gaussian Process or Tree Parzen Estimator) of the objective function and uses it to decide which hyperparameters to try next.\n",
    "\n",
    "How it works:\n",
    "1. Evaluate a few random points\n",
    "2. Fit a surrogate model to map hyperparameters -> score\n",
    "3. Use an **acquisition function** (e.g., Expected Improvement) to pick the next point that balances exploration vs. exploitation\n",
    "4. Evaluate that point, update the surrogate, repeat\n",
    "\n",
    "Popular libraries:\n",
    "- **[Optuna](https://optuna.org/)** \u2014 flexible, supports pruning, great visualization\n",
    "- **[scikit-optimize](https://scikit-optimize.github.io/)** \u2014 `BayesSearchCV` with scikit-learn API\n",
    "- **[Hyperopt](http://hyperopt.github.io/hyperopt/)** \u2014 Tree Parzen Estimators\n",
    "\n",
    "> **Note**: Bayesian methods shine when evaluation is expensive (e.g., deep learning). For small sklearn models, RandomizedSearchCV is often sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual illustration: Bayesian optimization flow (pseudocode)\n",
    "# This cell is illustrative -- it does NOT require optuna to be installed.\n",
    "\n",
    "bayesian_pseudocode = \"\"\"\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth    = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(study.best_params)\n",
    "print(study.best_value)\n",
    "\"\"\"\n",
    "print('Bayesian Optimization pseudocode (requires optuna):')\n",
    "print(bayesian_pseudocode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Best Practices\n",
    "\n",
    "1. **Always use cross-validation** during tuning, not a single validation split\n",
    "2. **Report final results on a held-out test set** that was never used during tuning\n",
    "3. **Start broad, then narrow**: use RandomizedSearchCV to find promising regions, then GridSearchCV to fine-tune\n",
    "4. **Fix `random_state`** for reproducibility\n",
    "5. **Use `n_jobs=-1`** to parallelize across CPU cores\n",
    "6. **Check for overfitting**: compare `mean_train_score` vs `mean_test_score` in CV results\n",
    "7. **Consider nested CV** when you need an unbiased estimate of the tuning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting in GridSearchCV results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results_sorted = results.sort_values('rank_test_score').head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = range(len(results_sorted))\n",
    "ax.plot(x, results_sorted['mean_train_score'], 'o-', label='Train CV', color='#2196F3')\n",
    "ax.plot(x, results_sorted['mean_test_score'], 's-', label='Validation CV', color='#FF5722')\n",
    "ax.fill_between(x,\n",
    "                results_sorted['mean_test_score'] - results_sorted['std_test_score'],\n",
    "                results_sorted['mean_test_score'] + results_sorted['std_test_score'],\n",
    "                alpha=0.2, color='#FF5722')\n",
    "ax.set_xlabel('Rank (sorted by validation score)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Train vs Validation CV Accuracy (Top 15 Configurations)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Is Wrong | Fix |\n",
    "|---------|----------------|-----|\n",
    "| **Tuning on the test set** | Test set leaks into model selection, giving an optimistic estimate | Use CV on training data only; evaluate on test set **once** at the end |\n",
    "| **Not using cross-validation** | A single train/val split is noisy and unreliable | Always use `cv >= 3` in `GridSearchCV` / `RandomizedSearchCV` |\n",
    "| **Grid too coarse** | Misses good hyperparameter regions | Start with random search to find promising areas, then refine |\n",
    "| **Grid too fine** | Wastes compute on negligible differences | Focus on hyperparameters that matter most (use feature importance of HP) |\n",
    "| **Ignoring `random_state`** | Results are not reproducible | Set `random_state` in the estimator and in the search object |\n",
    "| **Not reporting test set results** | CV score alone can still be optimistic if you tried many configs | Always hold out a final test set and report that number |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Exercise\n",
    "\n",
    "**Task**: Tune a `GradientBoostingClassifier` on the breast cancer dataset.\n",
    "\n",
    "1. Define a parameter grid with at least:\n",
    "   - `n_estimators`: [50, 100, 200]\n",
    "   - `learning_rate`: [0.01, 0.1, 0.2]\n",
    "   - `max_depth`: [3, 5, 7]\n",
    "2. Run `RandomizedSearchCV` with `n_iter=20` and `cv=5`\n",
    "3. Print the best parameters and best CV score\n",
    "4. Evaluate on the held-out test set\n",
    "5. Compare with the default `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Step 1: Define parameter grid\n",
    "# param_dist_gb = { ... }\n",
    "\n",
    "# Step 2: Run RandomizedSearchCV\n",
    "# rand_gb = RandomizedSearchCV(...)\n",
    "# rand_gb.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Print best params and CV score\n",
    "# print(rand_gb.best_params_)\n",
    "# print(rand_gb.best_score_)\n",
    "\n",
    "# Step 4: Evaluate on test set\n",
    "# print(rand_gb.score(X_test, y_test))\n",
    "\n",
    "# Step 5: Compare with default\n",
    "# gb_default = GradientBoostingClassifier(random_state=42)\n",
    "# gb_default.fit(X_train, y_train)\n",
    "# print(gb_default.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Notebook 4** | Next: [05 \u2014 Feature Selection: Filter, Wrapper, and Embedded](05_Feature_Selection_Filter_Wrapper_Embedded.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}