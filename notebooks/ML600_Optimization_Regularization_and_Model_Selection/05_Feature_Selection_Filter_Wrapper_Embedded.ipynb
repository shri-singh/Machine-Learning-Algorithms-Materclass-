{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Feature Selection \u2014 Filter, Wrapper, and Embedded Methods\n",
    "\n",
    "**Module ML600 \u2014 Optimization, Regularization, and Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain **why feature selection matters** (dimensionality, interpretability, speed)\n",
    "- Apply **filter methods**: correlation, mutual information, variance threshold\n",
    "- Apply **wrapper methods**: Recursive Feature Elimination (RFE)\n",
    "- Apply **embedded methods**: Lasso coefficients, tree-based importance, `SelectFromModel`\n",
    "- **Compare** all three approaches on the same dataset\n",
    "- Avoid common pitfalls (data leakage, relying on a single method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Familiarity with supervised learning (classification / regression)\n",
    "- Understanding of train/test splitting and cross-validation\n",
    "- Basic knowledge of Lasso (L1) regularization and tree-based models\n",
    "- Python libraries: `numpy`, `pandas`, `matplotlib`, `seaborn`, `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Why Feature Selection Matters](#1)\n",
    "2. [Filter Methods](#2)\n",
    "   - 2a. Correlation with Target\n",
    "   - 2b. Mutual Information\n",
    "   - 2c. Variance Threshold\n",
    "3. [Wrapper Methods \u2014 RFE](#3)\n",
    "4. [Embedded Methods](#4)\n",
    "   - 4a. Lasso (L1) Coefficients\n",
    "   - 4b. Tree-Based Feature Importance\n",
    "   - 4c. SelectFromModel\n",
    "5. [Comparison of All Methods](#5)\n",
    "6. [Common Mistakes](#6)\n",
    "7. [Exercise](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, mutual_info_classif, RFE, SelectFromModel\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')  # 0=malignant, 1=benign\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training: {X_train.shape}  |  Test: {X_test.shape}')\n",
    "print(f'Features: {X_train.shape[1]}')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Why Feature Selection Matters\n",
    "\n",
    "| Benefit | Explanation |\n",
    "|---------|-------------|\n",
    "| **Reduce dimensionality** | Fewer features = simpler model, lower risk of overfitting |\n",
    "| **Improve interpretability** | Easier to explain which features drive predictions |\n",
    "| **Speed up training** | Less data to process per iteration |\n",
    "| **Remove noise** | Irrelevant / redundant features add noise and hurt generalization |\n",
    "\n",
    "Three families of methods:\n",
    "\n",
    "1. **Filter**: score features independently of any model (fast, model-agnostic)\n",
    "2. **Wrapper**: use a model to evaluate feature subsets (more accurate, slower)\n",
    "3. **Embedded**: feature selection happens as part of model training (e.g., L1 penalty, tree splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Filter Methods\n",
    "\n",
    "Filter methods rank features using **statistical measures** without training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Correlation with Target\n",
    "\n",
    "For regression (or binary classification mapped to 0/1), Pearson correlation measures linear association between each feature and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute correlation of each feature with the target\n",
    "train_df = X_train.copy()\n",
    "train_df['target'] = y_train.values\n",
    "\n",
    "correlations = train_df.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "correlations.plot(kind='barh', ax=ax, color=sns.color_palette('viridis', len(correlations)))\n",
    "ax.set_xlabel('|Correlation| with Target')\n",
    "ax.set_title('Feature-Target Correlation (Absolute Pearson)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 10 features by |correlation|:')\n",
    "print(correlations.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Mutual Information\n",
    "\n",
    "Mutual information captures **any** (including non-linear) dependency between a feature and the target. `sklearn.feature_selection.mutual_info_classif` estimates MI for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information scores\n",
    "mi_scores = mutual_info_classif(\n",
    "    X_train, y_train, random_state=RANDOM_STATE, n_neighbors=5\n",
    ")\n",
    "mi_series = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "mi_series.plot(kind='barh', ax=ax, color=sns.color_palette('magma', len(mi_series)))\n",
    "ax.set_xlabel('Mutual Information Score')\n",
    "ax.set_title('Feature Ranking by Mutual Information')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 10 features by MI:')\n",
    "print(mi_series.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Variance Threshold\n",
    "\n",
    "Removes features whose variance is below a threshold. Zero-variance features carry no information. This is most useful after scaling or when features are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale first so variances are comparable\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index\n",
    ")\n",
    "\n",
    "# Show feature variances (all should be ~1 after StandardScaler, but let's check raw)\n",
    "raw_var = X_train.var().sort_values(ascending=False)\n",
    "print('Feature variances (raw, top 5):')\n",
    "print(raw_var.head().to_string())\n",
    "\n",
    "# Apply VarianceThreshold on raw features\n",
    "vt = VarianceThreshold(threshold=0.0)  # remove zero-variance only\n",
    "vt.fit(X_train)\n",
    "kept = X_train.columns[vt.get_support()]\n",
    "removed = X_train.columns[~vt.get_support()]\n",
    "print(f'\\nVarianceThreshold (threshold=0): kept {len(kept)}, removed {len(removed)}')\n",
    "if len(removed) > 0:\n",
    "    print(f'Removed features: {list(removed)}')\n",
    "else:\n",
    "    print('No features removed (all have variance > 0).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: side-by-side comparison of correlation vs mutual information rankings\n",
    "comparison_filter = pd.DataFrame({\n",
    "    'Correlation Rank': range(1, len(correlations) + 1),\n",
    "    'MI Rank': [list(mi_series.index).index(f) + 1 for f in correlations.index]\n",
    "}, index=correlations.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_n = 15\n",
    "for i, feat in enumerate(correlations.index[:top_n]):\n",
    "    corr_rank = comparison_filter.loc[feat, 'Correlation Rank']\n",
    "    mi_rank = comparison_filter.loc[feat, 'MI Rank']\n",
    "    ax.plot([0, 1], [corr_rank, mi_rank], 'o-', color=plt.cm.tab20(i), markersize=6)\n",
    "    ax.text(-0.05, corr_rank, feat, ha='right', fontsize=8, va='center')\n",
    "    ax.text(1.05, mi_rank, feat, ha='left', fontsize=8, va='center')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Correlation Rank', 'MI Rank'], fontsize=12)\n",
    "ax.set_ylabel('Rank (1 = best)')\n",
    "ax.set_title(f'Correlation vs MI Rankings (Top {top_n} by Correlation)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Wrapper Methods \u2014 Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE works by:\n",
    "1. Training a model on all features\n",
    "2. Ranking features by importance (e.g., coefficients, feature importances)\n",
    "3. Removing the least important feature(s)\n",
    "4. Repeating until the desired number of features is reached\n",
    "\n",
    "It is more expensive than filter methods but often more accurate because it accounts for feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with LogisticRegression\n",
    "# Scale features first (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=5000)\n",
    "\n",
    "# Select top 10 features\n",
    "rfe = RFE(estimator=lr, n_features_to_select=10, step=1)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "rfe_selected = X_train.columns[rfe.support_]\n",
    "rfe_ranking = pd.Series(rfe.ranking_, index=X_train.columns).sort_values()\n",
    "\n",
    "print('RFE selected features (top 10):')\n",
    "for i, feat in enumerate(rfe_selected, 1):\n",
    "    print(f'  {i}. {feat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFE ranking\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "colors = ['#4CAF50' if r == 1 else '#BDBDBD' for r in rfe_ranking.values]\n",
    "rfe_ranking.plot(kind='barh', ax=ax, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('RFE Ranking (1 = selected)')\n",
    "ax.set_title('Recursive Feature Elimination Rankings')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate: all features vs RFE-selected features\n",
    "# All features\n",
    "lr_all = LogisticRegression(random_state=RANDOM_STATE, max_iter=5000)\n",
    "scores_all = cross_val_score(lr_all, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# RFE features only\n",
    "X_train_rfe = X_train_scaled[:, rfe.support_]\n",
    "X_test_rfe = X_test_scaled[:, rfe.support_]\n",
    "lr_rfe = LogisticRegression(random_state=RANDOM_STATE, max_iter=5000)\n",
    "scores_rfe = cross_val_score(lr_rfe, X_train_rfe, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f'All {X_train.shape[1]} features  -> CV accuracy: {scores_all.mean():.4f} +/- {scores_all.std():.4f}')\n",
    "print(f'RFE {len(rfe_selected)} features -> CV accuracy: {scores_rfe.mean():.4f} +/- {scores_rfe.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Embedded Methods\n",
    "\n",
    "Embedded methods perform feature selection **during model training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Lasso (L1) Coefficients\n",
    "\n",
    "L1 regularization drives some coefficients to **exactly zero**, effectively removing those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LogisticRegression with L1 penalty (saga solver supports L1)\n",
    "lr_l1 = LogisticRegression(\n",
    "    penalty='l1', solver='saga', C=1.0,\n",
    "    random_state=RANDOM_STATE, max_iter=5000\n",
    ")\n",
    "lr_l1.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_coefs = pd.Series(\n",
    "    np.abs(lr_l1.coef_[0]), index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "n_nonzero = (lasso_coefs > 0).sum()\n",
    "n_zero = (lasso_coefs == 0).sum()\n",
    "print(f'L1 Logistic Regression: {n_nonzero} non-zero coefficients, {n_zero} zeroed out')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "colors = ['#E53935' if c > 0 else '#BDBDBD' for c in lasso_coefs.values]\n",
    "lasso_coefs.plot(kind='barh', ax=ax, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('|Coefficient| (L1 Logistic Regression)')\n",
    "ax.set_title('Lasso (L1) Feature Importance')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lasso_selected = lasso_coefs[lasso_coefs > 0].index.tolist()\n",
    "print(f'\\nSelected features ({len(lasso_selected)}): {lasso_selected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Tree-Based Feature Importance\n",
    "\n",
    "Decision trees and ensembles provide `feature_importances_` based on how much each feature reduces impurity (Gini or entropy for classification, MSE for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importances\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "rf.fit(X_train, y_train)  # no scaling needed for trees\n",
    "\n",
    "rf_importances = pd.Series(\n",
    "    rf.feature_importances_, index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rf_importances.plot(kind='barh', ax=ax,\n",
    "                     color=sns.color_palette('YlOrRd_r', len(rf_importances)),\n",
    "                     edgecolor='black')\n",
    "ax.set_xlabel('Feature Importance (Gini)')\n",
    "ax.set_title('Random Forest Feature Importance')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 10 features by RF importance:')\n",
    "print(rf_importances.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. SelectFromModel\n",
    "\n",
    "`SelectFromModel` selects features whose importance is above a threshold (default: `mean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectFromModel with RandomForest\n",
    "sfm_rf = SelectFromModel(rf, threshold='mean')\n",
    "sfm_rf.fit(X_train, y_train)\n",
    "sfm_rf_features = X_train.columns[sfm_rf.get_support()]\n",
    "\n",
    "print(f'SelectFromModel (RF, threshold=mean): {len(sfm_rf_features)} features selected')\n",
    "print(f'Features: {list(sfm_rf_features)}')\n",
    "\n",
    "# SelectFromModel with L1 LogisticRegression\n",
    "sfm_l1 = SelectFromModel(lr_l1, threshold=1e-5)\n",
    "sfm_l1.fit(X_train_scaled, y_train)\n",
    "sfm_l1_features = X_train.columns[sfm_l1.get_support()]\n",
    "\n",
    "print(f'\\nSelectFromModel (L1 LR, threshold=1e-5): {len(sfm_l1_features)} features selected')\n",
    "print(f'Features: {list(sfm_l1_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Lasso vs RF importances side by side\n",
    "compare_embedded = pd.DataFrame({\n",
    "    'Lasso |coef|': lasso_coefs / lasso_coefs.max(),  # normalize to [0,1]\n",
    "    'RF Importance': rf_importances / rf_importances.max()\n",
    "}).sort_values('RF Importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "compare_embedded.plot(kind='barh', ax=ax, width=0.8, edgecolor='black')\n",
    "ax.set_xlabel('Normalized Importance')\n",
    "ax.set_title('Lasso (L1) vs Random Forest Feature Importance (Normalized)')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Comparison of All Methods\n",
    "\n",
    "Let us compare the top-10 features selected by each method and evaluate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect top-10 features from each method\n",
    "top_k = 10\n",
    "\n",
    "methods = {\n",
    "    'Correlation': correlations.head(top_k).index.tolist(),\n",
    "    'Mutual Info': mi_series.head(top_k).index.tolist(),\n",
    "    'RFE (LR)': rfe_selected.tolist(),\n",
    "    'Lasso (L1)': lasso_coefs.head(top_k).index.tolist(),\n",
    "    'RF Importance': rf_importances.head(top_k).index.tolist()\n",
    "}\n",
    "\n",
    "# Print selected features\n",
    "for name, feats in methods.items():\n",
    "    print(f'{name:15s}: {feats}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each feature subset with LogisticRegression (5-fold CV)\n",
    "results = []\n",
    "\n",
    "for method_name, selected_feats in methods.items():\n",
    "    # Scale the selected features\n",
    "    sc = StandardScaler()\n",
    "    X_tr_sub = sc.fit_transform(X_train[selected_feats])\n",
    "    X_te_sub = sc.transform(X_test[selected_feats])\n",
    "    \n",
    "    lr_eval = LogisticRegression(random_state=RANDOM_STATE, max_iter=5000)\n",
    "    cv_scores = cross_val_score(lr_eval, X_tr_sub, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    lr_eval.fit(X_tr_sub, y_train)\n",
    "    test_acc = lr_eval.score(X_te_sub, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Method': method_name,\n",
    "        'Num Features': len(selected_feats),\n",
    "        'CV Accuracy (mean)': cv_scores.mean(),\n",
    "        'CV Accuracy (std)': cv_scores.std(),\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "\n",
    "# Add baseline (all features)\n",
    "lr_base = LogisticRegression(random_state=RANDOM_STATE, max_iter=5000)\n",
    "cv_base = cross_val_score(lr_base, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "lr_base.fit(X_train_scaled, y_train)\n",
    "results.append({\n",
    "    'Method': 'All Features',\n",
    "    'Num Features': X_train.shape[1],\n",
    "    'CV Accuracy (mean)': cv_base.mean(),\n",
    "    'CV Accuracy (std)': cv_base.std(),\n",
    "    'Test Accuracy': lr_base.score(X_test_scaled, y_test)\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CV accuracy\n",
    "sorted_df = results_df.sort_values('CV Accuracy (mean)')\n",
    "colors = ['#4CAF50' if m == 'All Features' else '#2196F3' for m in sorted_df['Method']]\n",
    "axes[0].barh(sorted_df['Method'], sorted_df['CV Accuracy (mean)'],\n",
    "             xerr=sorted_df['CV Accuracy (std)'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('CV Accuracy')\n",
    "axes[0].set_title('5-Fold CV Accuracy by Feature Selection Method')\n",
    "\n",
    "# Test accuracy\n",
    "sorted_df2 = results_df.sort_values('Test Accuracy')\n",
    "colors2 = ['#4CAF50' if m == 'All Features' else '#FF9800' for m in sorted_df2['Method']]\n",
    "axes[1].barh(sorted_df2['Method'], sorted_df2['Test Accuracy'],\n",
    "             color=colors2, edgecolor='black')\n",
    "axes[1].set_xlabel('Test Accuracy')\n",
    "axes[1].set_title('Test Set Accuracy by Feature Selection Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature overlap heatmap: which features appear across methods?\n",
    "all_features = sorted(set(f for feats in methods.values() for f in feats))\n",
    "presence = pd.DataFrame(\n",
    "    {method: [1 if f in feats else 0 for f in all_features]\n",
    "     for method, feats in methods.items()},\n",
    "    index=all_features\n",
    ")\n",
    "presence['Total'] = presence.sum(axis=1)\n",
    "presence = presence.sort_values('Total', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    presence.drop(columns='Total'), annot=True, cmap='YlGn',\n",
    "    linewidths=0.5, cbar_kws={'label': 'Selected (1) / Not (0)'},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Feature Selection Overlap Across Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Features selected by ALL methods:')\n",
    "consensus = presence[presence['Total'] == len(methods)].index.tolist()\n",
    "print(consensus if consensus else 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Common Mistakes\n",
    "\n",
    "| Mistake | Why It Is Wrong | Fix |\n",
    "|---------|----------------|-----|\n",
    "| **Feature selection BEFORE train/test split** | Information from the test set leaks into feature ranking, giving optimistic results | Always split first, then do feature selection on the training set only |\n",
    "| **Using only one method** | Different methods capture different aspects (linear vs non-linear, univariate vs multivariate) | Compare at least one filter + one embedded method |\n",
    "| **Ignoring feature interactions** | Filter methods rank features independently | Use wrapper or embedded methods to capture interactions |\n",
    "| **Selecting too few or too many features** | Too few = underfitting; too many = no benefit | Use cross-validation to choose the optimal number |\n",
    "| **Using correlation for non-linear relationships** | Pearson correlation misses non-linear patterns | Use mutual information or tree-based importance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Exercise\n",
    "\n",
    "**Task**: Apply feature selection to the wine dataset and compare methods.\n",
    "\n",
    "1. Load `sklearn.datasets.load_wine()`\n",
    "2. Split into train/test (80/20, stratify, `random_state=42`)\n",
    "3. Compute mutual information scores on the training set\n",
    "4. Run RFE with `RandomForestClassifier` to select 5 features\n",
    "5. Fit `LogisticRegression` with (a) all features, (b) top-5 MI features, (c) RFE 5 features\n",
    "6. Report 5-fold CV accuracy and test accuracy for each\n",
    "7. Which method produces the best result with only 5 features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Step 1: Load data\n",
    "# wine = load_wine()\n",
    "# X_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "# y_wine = wine.target\n",
    "\n",
    "# Step 2: Split\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(...)\n",
    "\n",
    "# Step 3: Mutual information\n",
    "# mi = mutual_info_classif(X_tr, y_tr, random_state=42)\n",
    "\n",
    "# Step 4: RFE\n",
    "# rfe_wine = RFE(RandomForestClassifier(random_state=42), n_features_to_select=5)\n",
    "\n",
    "# Step 5-6: Evaluate and compare\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Notebook 5** | Next: [06 \u2014 End-to-End ML Project Template](06_End_to_End_ML_Project_Template.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}