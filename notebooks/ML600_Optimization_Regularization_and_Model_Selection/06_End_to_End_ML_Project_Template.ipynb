{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: End-to-End ML Project Template\n",
    "\n",
    "**Module ML600 \u2014 Optimization, Regularization, and Model Selection**\n",
    "\n",
    "A fully runnable walkthrough that covers every step from problem framing to model saving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- **Frame** an ML problem: define the objective, metric, and baseline\n",
    "- **Load and explore** data with summary statistics, distributions, and correlations\n",
    "- **Split** data correctly (train/test, stratification considerations)\n",
    "- **Build preprocessing pipelines** with `ColumnTransformer`\n",
    "- **Train and compare** multiple models using cross-validation\n",
    "- **Analyze errors** with residual plots and worst-prediction inspection\n",
    "- **Tune hyperparameters** with `GridSearchCV`\n",
    "- **Evaluate** the final model on a held-out test set\n",
    "- **Save and reload** a trained pipeline with `joblib`\n",
    "- Follow a **reproducibility checklist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Completed Notebooks 01\u201305 in this module (or equivalent knowledge)\n",
    "- Familiarity with scikit-learn pipelines, cross-validation, and GridSearchCV\n",
    "- Python libraries: `numpy`, `pandas`, `matplotlib`, `seaborn`, `sklearn`, `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Problem Framing](#1)\n",
    "2. [Data Loading and EDA](#2)\n",
    "3. [Data Splitting](#3)\n",
    "4. [Baseline Model](#4)\n",
    "5. [Preprocessing Pipeline](#5)\n",
    "6. [Model Training](#6)\n",
    "7. [Evaluation \u2014 Cross-Validation](#7)\n",
    "8. [Error Analysis](#8)\n",
    "9. [Hyperparameter Tuning](#9)\n",
    "10. [Final Evaluation on Test Set](#10)\n",
    "11. [Model Saving](#11)\n",
    "12. [Reproducibility Checklist](#12)\n",
    "13. [Common Mistakes](#13)\n",
    "14. [Exercise](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Problem Framing\n",
    "\n",
    "Before writing any code, clearly define:\n",
    "\n",
    "| Question | Answer for This Project |\n",
    "|----------|------------------------|\n",
    "| **What are we predicting?** | A quantitative measure of disease progression one year after baseline (regression) |\n",
    "| **What is the target variable?** | `target` column in the diabetes dataset |\n",
    "| **What metric will we optimize?** | RMSE (Root Mean Squared Error) \u2014 lower is better |\n",
    "| **What is a reasonable baseline?** | Predict the mean of the training set (`DummyRegressor`) |\n",
    "| **What data do we have?** | 10 baseline variables: age, sex, BMI, blood pressure, 6 blood serum measurements |\n",
    "| **How many samples?** | 442 |\n",
    "\n",
    "> **Goal**: Build a regression model that significantly outperforms the \"predict the mean\" baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Data Loading and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target, name='target')\n",
    "\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target range:  [{y.min():.0f}, {y.max():.0f}]')\n",
    "print(f'Target mean:   {y.mean():.1f}')\n",
    "print()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values per column:')\n",
    "print(X.isnull().sum().to_string())\n",
    "print(f'\\nTotal missing: {X.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of features\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "for i, col in enumerate(X.columns):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.hist(X[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(col, fontsize=10)\n",
    "plt.suptitle('Feature Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(y, bins=30, color='#FF7043', edgecolor='black', alpha=0.8)\n",
    "ax.axvline(y.mean(), color='black', linestyle='--', label=f'Mean = {y.mean():.1f}')\n",
    "ax.set_xlabel('Disease Progression')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Target Distribution')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "full_df = X.copy()\n",
    "full_df['target'] = y\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "corr = full_df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title('Correlation Matrix (including target)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Features most correlated with target:')\n",
    "target_corr = corr['target'].drop('target').abs().sort_values(ascending=False)\n",
    "print(target_corr.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Data Splitting\n",
    "\n",
    "- We use an 80/20 train/test split\n",
    "- For **classification** tasks, use `stratify=y` to preserve class proportions\n",
    "- For **regression** (our case), stratification is not strictly needed, but we could stratify on binned target values if the distribution is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set:     {X_test.shape[0]} samples')\n",
    "print(f'\\nTrain target mean: {y_train.mean():.1f}')\n",
    "print(f'Test target mean:  {y_test.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Baseline Model\n",
    "\n",
    "A baseline sets the **floor** for model performance. If your model cannot beat the baseline, something is wrong.\n",
    "\n",
    "`DummyRegressor(strategy='mean')` predicts the mean of the training target for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_dummy))\n",
    "baseline_mae = mean_absolute_error(y_test, y_pred_dummy)\n",
    "baseline_r2 = r2_score(y_test, y_pred_dummy)\n",
    "\n",
    "print('=== Baseline (Predict Mean) ===')\n",
    "print(f'RMSE: {baseline_rmse:.2f}')\n",
    "print(f'MAE:  {baseline_mae:.2f}')\n",
    "print(f'R2:   {baseline_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Preprocessing Pipeline\n",
    "\n",
    "Even though the diabetes dataset is already preprocessed (centered, scaled), we build a full pipeline to demonstrate best practices.\n",
    "\n",
    "In a real project, you would handle:\n",
    "- **Numeric features**: imputation + scaling\n",
    "- **Categorical features**: imputation + one-hot encoding\n",
    "\n",
    "We use `ColumnTransformer` to apply different transformations to different column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "# In the diabetes dataset, all features are numeric\n",
    "numeric_features = X_train.columns.tolist()\n",
    "# categorical_features = []  # none in this dataset\n",
    "\n",
    "print(f'Numeric features ({len(numeric_features)}): {numeric_features}')\n",
    "\n",
    "# Build the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        # If you had categorical features:\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "    ],\n",
    "    remainder='passthrough'  # keep any other columns unchanged\n",
    ")\n",
    "\n",
    "print('Preprocessor built.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Model Training\n",
    "\n",
    "We wrap each model in a `Pipeline` so that preprocessing and prediction are a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge (alpha=1.0)': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Ridge(alpha=1.0, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=100, random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(f'Models to compare: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all models on training data\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(f'{name}: fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Evaluation \u2014 Cross-Validation\n",
    "\n",
    "We evaluate each model with 5-fold cross-validation on the **training set** to get an honest estimate before touching the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    # Negative MSE because sklearn maximizes the score\n",
    "    neg_mse = cross_val_score(\n",
    "        pipe, X_train, y_train, cv=5,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    rmse_scores = np.sqrt(-neg_mse)\n",
    "    \n",
    "    r2_scores = cross_val_score(\n",
    "        pipe, X_train, y_train, cv=5, scoring='r2'\n",
    "    )\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV RMSE (mean)': rmse_scores.mean(),\n",
    "        'CV RMSE (std)': rmse_scores.std(),\n",
    "        'CV R2 (mean)': r2_scores.mean(),\n",
    "        'CV R2 (std)': r2_scores.std()\n",
    "    })\n",
    "\n",
    "# Add baseline\n",
    "dummy_neg_mse = cross_val_score(\n",
    "    DummyRegressor(strategy='mean'), X_train, y_train, cv=5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "dummy_r2 = cross_val_score(\n",
    "    DummyRegressor(strategy='mean'), X_train, y_train, cv=5,\n",
    "    scoring='r2'\n",
    ")\n",
    "cv_results.append({\n",
    "    'Model': 'Baseline (Mean)',\n",
    "    'CV RMSE (mean)': np.sqrt(-dummy_neg_mse).mean(),\n",
    "    'CV RMSE (std)': np.sqrt(-dummy_neg_mse).std(),\n",
    "    'CV R2 (mean)': dummy_r2.mean(),\n",
    "    'CV R2 (std)': dummy_r2.std()\n",
    "})\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('CV RMSE (mean)')\n",
    "print('=== Model Comparison (5-Fold CV on Training Set) ===')\n",
    "print(cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "cv_sorted = cv_df.sort_values('CV RMSE (mean)', ascending=True)\n",
    "colors = ['#4CAF50' if 'Baseline' not in m else '#999999' for m in cv_sorted['Model']]\n",
    "axes[0].barh(cv_sorted['Model'], cv_sorted['CV RMSE (mean)'],\n",
    "             xerr=cv_sorted['CV RMSE (std)'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('RMSE (lower is better)')\n",
    "axes[0].set_title('Cross-Validation RMSE')\n",
    "\n",
    "# R2\n",
    "cv_sorted_r2 = cv_df.sort_values('CV R2 (mean)', ascending=True)\n",
    "colors_r2 = ['#2196F3' if 'Baseline' not in m else '#999999' for m in cv_sorted_r2['Model']]\n",
    "axes[1].barh(cv_sorted_r2['Model'], cv_sorted_r2['CV R2 (mean)'],\n",
    "             xerr=cv_sorted_r2['CV R2 (std)'], color=colors_r2, edgecolor='black')\n",
    "axes[1].set_xlabel('R2 (higher is better)')\n",
    "axes[1].set_title('Cross-Validation R2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Error Analysis\n",
    "\n",
    "Before tuning, inspect the errors of the best model to understand **where** it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best model from CV for error analysis\n",
    "# We use cross_val_predict for residual analysis on training data\n",
    "best_model_name = cv_df.sort_values('CV RMSE (mean)').iloc[0]['Model']\n",
    "print(f'Best model for error analysis: {best_model_name}')\n",
    "\n",
    "best_pipe = models[best_model_name]\n",
    "y_pred_cv = cross_val_predict(best_pipe, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "residuals = y_train.values - y_pred_cv\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0].scatter(y_train, y_pred_cv, alpha=0.5, edgecolors='black', linewidth=0.5)\n",
    "min_val = min(y_train.min(), y_pred_cv.min())\n",
    "max_val = max(y_train.max(), y_pred_cv.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title('Actual vs Predicted (CV)')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[1].scatter(y_pred_cv, residuals, alpha=0.5, edgecolors='black', linewidth=0.5)\n",
    "axes[1].axhline(0, color='red', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# 3. Residual distribution\n",
    "axes[2].hist(residuals, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(0, color='red', linestyle='--', lw=2)\n",
    "axes[2].set_xlabel('Residual')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Residual Distribution')\n",
    "\n",
    "plt.suptitle(f'Error Analysis: {best_model_name}', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst predictions analysis\n",
    "error_df = X_train.copy()\n",
    "error_df['actual'] = y_train.values\n",
    "error_df['predicted'] = y_pred_cv\n",
    "error_df['residual'] = residuals\n",
    "error_df['abs_error'] = np.abs(residuals)\n",
    "\n",
    "print('=== Top 10 Worst Predictions (by absolute error) ===')\n",
    "worst = error_df.nlargest(10, 'abs_error')[['actual', 'predicted', 'residual', 'abs_error']]\n",
    "print(worst.to_string())\n",
    "\n",
    "print(f'\\nMean absolute error: {error_df[\"abs_error\"].mean():.2f}')\n",
    "print(f'Median absolute error: {error_df[\"abs_error\"].median():.2f}')\n",
    "print(f'Max absolute error: {error_df[\"abs_error\"].max():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Hyperparameter Tuning\n",
    "\n",
    "We tune the best-performing model using `GridSearchCV` on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tunable pipeline\n",
    "# We tune Ridge and RandomForest, then pick the best\n",
    "\n",
    "# --- Ridge tuning ---\n",
    "ridge_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe, ridge_param_grid, cv=5,\n",
    "    scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True\n",
    ")\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print('=== Ridge Tuning ===')\n",
    "print(f'Best alpha: {ridge_grid.best_params_[\"regressor__alpha\"]}')\n",
    "print(f'Best CV RMSE: {np.sqrt(-ridge_grid.best_score_):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RandomForest tuning ---\n",
    "rf_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [5, 10, 20, None],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe, rf_param_grid, cv=5,\n",
    "    scoring='neg_mean_squared_error', n_jobs=-1, return_train_score=True\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print('=== Random Forest Tuning ===')\n",
    "print(f'Best params: {rf_grid.best_params_}')\n",
    "print(f'Best CV RMSE: {np.sqrt(-rf_grid.best_score_):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models\n",
    "tuned_results = pd.DataFrame([\n",
    "    {'Model': 'Ridge (tuned)', 'CV RMSE': np.sqrt(-ridge_grid.best_score_)},\n",
    "    {'Model': 'Random Forest (tuned)', 'CV RMSE': np.sqrt(-rf_grid.best_score_)},\n",
    "    {'Model': 'Baseline (Mean)', 'CV RMSE': np.sqrt(-dummy_neg_mse).mean()}\n",
    "]).sort_values('CV RMSE')\n",
    "\n",
    "print('=== Tuned Model Comparison (CV RMSE) ===')\n",
    "print(tuned_results.to_string(index=False))\n",
    "\n",
    "# Select final model\n",
    "final_model_name = tuned_results.iloc[0]['Model']\n",
    "if 'Ridge' in final_model_name:\n",
    "    final_model = ridge_grid.best_estimator_\n",
    "else:\n",
    "    final_model = rf_grid.best_estimator_\n",
    "\n",
    "print(f'\\nFinal model selected: {final_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Final Evaluation on Test Set\n",
    "\n",
    "This is the **only time** we use the test set. The number we report here is our best estimate of real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "final_mae = mean_absolute_error(y_test, y_pred_final)\n",
    "final_r2 = r2_score(y_test, y_pred_final)\n",
    "\n",
    "print('=' * 50)\n",
    "print(f'FINAL TEST SET EVALUATION: {final_model_name}')\n",
    "print('=' * 50)\n",
    "print(f'RMSE: {final_rmse:.2f}')\n",
    "print(f'MAE:  {final_mae:.2f}')\n",
    "print(f'R2:   {final_r2:.4f}')\n",
    "print()\n",
    "print(f'Baseline RMSE: {baseline_rmse:.2f}')\n",
    "print(f'Improvement over baseline: {((baseline_rmse - final_rmse) / baseline_rmse * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model comparison table\n",
    "final_comparison = []\n",
    "\n",
    "# Baseline on test set\n",
    "final_comparison.append({\n",
    "    'Model': 'Baseline (Mean)',\n",
    "    'Test RMSE': baseline_rmse,\n",
    "    'Test MAE': baseline_mae,\n",
    "    'Test R2': baseline_r2\n",
    "})\n",
    "\n",
    "# All models on test set\n",
    "for name, pipe in models.items():\n",
    "    yp = pipe.predict(X_test)\n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, yp)),\n",
    "        'Test MAE': mean_absolute_error(y_test, yp),\n",
    "        'Test R2': r2_score(y_test, yp)\n",
    "    })\n",
    "\n",
    "# Tuned models on test set\n",
    "for tag, grid_obj in [('Ridge (tuned)', ridge_grid), ('RF (tuned)', rf_grid)]:\n",
    "    yp = grid_obj.best_estimator_.predict(X_test)\n",
    "    final_comparison.append({\n",
    "        'Model': tag,\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, yp)),\n",
    "        'Test MAE': mean_absolute_error(y_test, yp),\n",
    "        'Test R2': r2_score(y_test, yp)\n",
    "    })\n",
    "\n",
    "final_table = pd.DataFrame(final_comparison).sort_values('Test RMSE')\n",
    "print('=== Complete Model Comparison (Test Set) ===')\n",
    "print(final_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ft = final_table.sort_values('Test RMSE')\n",
    "colors_rmse = ['#E53935' if 'Baseline' in m else '#4CAF50' if 'tuned' in m else '#2196F3'\n",
    "               for m in ft['Model']]\n",
    "axes[0].barh(ft['Model'], ft['Test RMSE'], color=colors_rmse, edgecolor='black')\n",
    "axes[0].set_xlabel('RMSE (lower is better)')\n",
    "axes[0].set_title('Test RMSE Comparison')\n",
    "\n",
    "ft_r2 = final_table.sort_values('Test R2')\n",
    "colors_r2 = ['#E53935' if 'Baseline' in m else '#4CAF50' if 'tuned' in m else '#2196F3'\n",
    "             for m in ft_r2['Model']]\n",
    "axes[1].barh(ft_r2['Model'], ft_r2['Test R2'], color=colors_r2, edgecolor='black')\n",
    "axes[1].set_xlabel('R2 (higher is better)')\n",
    "axes[1].set_title('Test R2 Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11. Model Saving\n",
    "\n",
    "Use `joblib` to save the entire pipeline (preprocessing + model) so it can be loaded later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model pipeline\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'diabetes_best_pipeline.joblib')\n",
    "\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f'Model saved to: {model_path}')\n",
    "print(f'File size: {os.path.getsize(model_path) / 1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and verify\n",
    "loaded_model = joblib.load(model_path)\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "# Verify predictions match\n",
    "assert np.allclose(y_pred_final, y_pred_loaded), 'Predictions do not match!'\n",
    "print('Model loaded and verified -- predictions match exactly.')\n",
    "\n",
    "# Clean up\n",
    "os.remove(model_path)\n",
    "os.rmdir(model_dir)\n",
    "print('Cleaned up saved model files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## 12. Reproducibility Checklist\n",
    "\n",
    "Before sharing your project or deploying a model, verify:\n",
    "\n",
    "- [ ] **`random_state=42`** set in all stochastic components (train_test_split, models, CV)\n",
    "- [ ] **`requirements.txt`** listing all package versions (`pip freeze > requirements.txt`)\n",
    "- [ ] **Pipeline saved** with `joblib.dump()` (includes preprocessor + model)\n",
    "- [ ] **Train/test split** is fixed and documented\n",
    "- [ ] **No test data leakage**: preprocessing fitted on training data only\n",
    "- [ ] **Results reported** on held-out test set (not CV scores)\n",
    "- [ ] **Code is runnable** end-to-end from a clean environment\n",
    "- [ ] **Data source** documented (or data saved/versioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print environment info for reproducibility\n",
    "import sklearn\n",
    "print('=== Environment ===')\n",
    "print(f'numpy:        {np.__version__}')\n",
    "print(f'pandas:       {pd.__version__}')\n",
    "print(f'scikit-learn: {sklearn.__version__}')\n",
    "print(f'seaborn:      {sns.__version__}')\n",
    "print(f'random_state: {RANDOM_STATE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## 13. Common Mistakes\n",
    "\n",
    "| Step | Mistake | Fix |\n",
    "|------|---------|-----|\n",
    "| **Problem Framing** | Jumping to modeling without defining success metric | Define the metric and baseline before any code |\n",
    "| **EDA** | Skipping EDA, missing data issues or class imbalance | Always visualize distributions, check for nulls, correlations |\n",
    "| **Splitting** | Fitting preprocessor on full data (train+test) | Fit only on training data; transform test data |\n",
    "| **Baseline** | No baseline to compare against | Always establish a DummyRegressor/DummyClassifier floor |\n",
    "| **Pipelines** | Separate preprocessing and model steps (risk of data leakage) | Use sklearn `Pipeline` to chain everything |\n",
    "| **Evaluation** | Reporting CV scores as final results | CV is for model selection; final metric comes from held-out test set |\n",
    "| **Error Analysis** | Not inspecting errors before tuning | Residual plots reveal systematic patterns that tuning alone cannot fix |\n",
    "| **Tuning** | Tuning on the test set | Use `GridSearchCV` with CV on training data only |\n",
    "| **Final Report** | Not comparing against baseline in the final table | Always include the baseline in comparison tables |\n",
    "| **Reproducibility** | Forgetting `random_state` or not saving the pipeline | Use a checklist (see Section 12) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## 14. Exercise\n",
    "\n",
    "**Task**: Adapt this template to a **classification** problem.\n",
    "\n",
    "1. Load `sklearn.datasets.load_breast_cancer()`\n",
    "2. Frame the problem: what metric? (accuracy, F1, AUC?). What baseline? (`DummyClassifier`)\n",
    "3. EDA: class balance, feature distributions\n",
    "4. Split: 80/20, `stratify=y`, `random_state=42`\n",
    "5. Baseline: `DummyClassifier(strategy='most_frequent')`\n",
    "6. Fit at least 3 models: `LogisticRegression`, `RandomForestClassifier`, `GradientBoostingClassifier`\n",
    "7. Cross-validate on training set\n",
    "8. Tune the best model with `GridSearchCV`\n",
    "9. Evaluate on test set\n",
    "10. Save the final pipeline\n",
    "11. Create a final comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Follow the same 12-step structure used above, but for classification.\n",
    "\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "# ...\n",
    "\n",
    "# Step 2-12: Follow the template above\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Notebook 6 and Module ML600** | Congratulations on completing the Optimization, Regularization, and Model Selection module!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}