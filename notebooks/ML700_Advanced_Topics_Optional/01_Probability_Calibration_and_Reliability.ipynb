{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Probability Calibration and Reliability\n",
    "\n",
    "**Module**: ML700 Advanced Topics (Optional)  \n",
    "**Notebook**: 01 - Probability Calibration and Reliability  \n",
    "**Status**: OPTIONAL - This notebook covers advanced material beyond the core curriculum.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain what calibrated probabilities are and why they matter for decision-making\n",
    "2. Read and interpret reliability diagrams (calibration curves)\n",
    "3. Compute the Brier score to measure calibration quality\n",
    "4. Apply Platt scaling and isotonic regression to calibrate classifier outputs\n",
    "5. Use `CalibratedClassifierCV` from scikit-learn to calibrate models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of classification (Logistic Regression, Random Forests)\n",
    "- Familiarity with `predict_proba` in scikit-learn\n",
    "- Basic knowledge of train/test splitting and cross-validation\n",
    "- Modules ML300 (Logistic Regression) and ML500 (Trees/Ensembles)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What Are Calibrated Probabilities?](#1.-What-Are-Calibrated-Probabilities?)\n",
    "2. [Why Calibration Matters](#2.-Why-Calibration-Matters)\n",
    "3. [Reliability Diagrams (Calibration Curves)](#3.-Reliability-Diagrams)\n",
    "4. [Brier Score](#4.-Brier-Score)\n",
    "5. [Calibration Methods](#5.-Calibration-Methods)\n",
    "6. [Hands-On: Comparing Calibration on Breast Cancer Data](#6.-Hands-On)\n",
    "7. [When Calibration Matters Most](#7.-When-Calibration-Matters-Most)\n",
    "8. [Common Mistakes](#8.-Common-Mistakes)\n",
    "9. [Summary](#9.-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. What Are Calibrated Probabilities?\n",
    "\n",
    "A classifier is **well-calibrated** if, among all samples it assigns a predicted probability of $p$,\n",
    "the true fraction of positives is approximately $p$.\n",
    "\n",
    "For example, if a model says \"there is a 70% chance of cancer\" for 100 patients,\n",
    "roughly 70 of those patients should actually have cancer.\n",
    "\n",
    "**Key insight**: Many classifiers output scores that look like probabilities (values between 0 and 1)\n",
    "but are NOT actually calibrated probabilities. Random Forests, SVMs, and Naive Bayes are common offenders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Calibration Matters\n",
    "\n",
    "Calibration matters when you use predicted probabilities for **decision-making**, not just ranking:\n",
    "\n",
    "- **Medical diagnosis**: \"80% chance of malignancy\" must actually mean 80%\n",
    "- **Risk scoring**: Insurance pricing, credit scoring\n",
    "- **Threshold selection**: Choosing a cutoff requires trustworthy probabilities\n",
    "- **Combining models**: Ensembling probabilities from different models only works if they are calibrated\n",
    "\n",
    "If you only care about **ranking** (which sample is more likely positive?), calibration matters less.\n",
    "But if you care about the **actual probability values**, calibration is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reliability Diagrams\n",
    "\n",
    "A **reliability diagram** (calibration curve) plots:\n",
    "- **X-axis**: Mean predicted probability in each bin\n",
    "- **Y-axis**: Fraction of positives (true frequency) in each bin\n",
    "\n",
    "A **perfectly calibrated** classifier lies on the **diagonal line** (y = x).\n",
    "\n",
    "- Points **above** the diagonal: model is **under-confident** (actual rate > predicted)\n",
    "- Points **below** the diagonal: model is **over-confident** (actual rate < predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Brier Score\n",
    "\n",
    "The **Brier score** measures the mean squared error between predicted probabilities and actual outcomes:\n",
    "\n",
    "$$BS = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{p}_i - y_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{p}_i$ is the predicted probability of the positive class for sample $i$\n",
    "- $y_i \\in \\{0, 1\\}$ is the true label\n",
    "\n",
    "**Interpretation**:\n",
    "- Brier score ranges from 0 (perfect) to 1 (worst)\n",
    "- Lower is better\n",
    "- A baseline of predicting the class prevalence always gives a reference score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calibration Methods\n",
    "\n",
    "### Platt Scaling (Sigmoid)\n",
    "Fits a logistic regression on the classifier's output scores:\n",
    "$$P(y=1|f) = \\frac{1}{1 + \\exp(Af + B)}$$\n",
    "- Works well when the distortion is sigmoid-shaped\n",
    "- Needs fewer samples (only 2 parameters)\n",
    "- Common for SVMs and boosting methods\n",
    "\n",
    "### Isotonic Regression\n",
    "Fits a non-parametric, non-decreasing function to map scores to probabilities.\n",
    "- More flexible than Platt scaling\n",
    "- Needs more data (can overfit with small datasets)\n",
    "- Works when the distortion is not sigmoid-shaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On: Comparing Calibration on Breast Cancer Data\n",
    "\n",
    "Let us train Logistic Regression and Random Forest, compare their calibration curves,\n",
    "and then apply calibration methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split: train (60%), calibration (20%), test (20%)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Calibration: {X_cal.shape[0]}, Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models\n",
    "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities on test set\n",
    "lr_probs = lr.predict_proba(X_test)[:, 1]\n",
    "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Logistic Regression Brier Score: {brier_score_loss(y_test, lr_probs):.4f}\")\n",
    "print(f\"Random Forest Brier Score:       {brier_score_loss(y_test, rf_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves BEFORE calibration\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "# Perfect calibration reference\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "\n",
    "for name, probs in [(\"Logistic Regression\", lr_probs), (\"Random Forest\", rf_probs)]:\n",
    "    fraction_pos, mean_predicted = calibration_curve(y_test, probs, n_bins=8)\n",
    "    ax.plot(mean_predicted, fraction_pos, 's-', label=name)\n",
    "\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curves (Before Calibration)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Logistic Regression is typically closer to the diagonal.\")\n",
    "print(\"Random Forests tend to push probabilities away from 0 and 1 (S-shaped distortion).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Calibration with CalibratedClassifierCV\n",
    "\n",
    "We will calibrate the Random Forest using both Platt scaling (sigmoid) and isotonic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate Random Forest using Platt scaling (sigmoid)\n",
    "rf_sigmoid = CalibratedClassifierCV(rf, method='sigmoid', cv='prefit')\n",
    "rf_sigmoid.fit(X_cal, y_cal)\n",
    "rf_sigmoid_probs = rf_sigmoid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calibrate Random Forest using Isotonic regression\n",
    "rf_isotonic = CalibratedClassifierCV(rf, method='isotonic', cv='prefit')\n",
    "rf_isotonic.fit(X_cal, y_cal)\n",
    "rf_isotonic_probs = rf_isotonic.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Brier scores\n",
    "print(\"Brier Scores (lower is better):\")\n",
    "print(f\"  RF (uncalibrated):         {brier_score_loss(y_test, rf_probs):.4f}\")\n",
    "print(f\"  RF + Platt (sigmoid):      {brier_score_loss(y_test, rf_sigmoid_probs):.4f}\")\n",
    "print(f\"  RF + Isotonic:             {brier_score_loss(y_test, rf_isotonic_probs):.4f}\")\n",
    "print(f\"  Logistic Regression:       {brier_score_loss(y_test, lr_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves AFTER calibration\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "\n",
    "models_and_probs = [\n",
    "    (\"Logistic Regression\", lr_probs),\n",
    "    (\"RF (uncalibrated)\", rf_probs),\n",
    "    (\"RF + Platt (sigmoid)\", rf_sigmoid_probs),\n",
    "    (\"RF + Isotonic\", rf_isotonic_probs),\n",
    "]\n",
    "\n",
    "for name, probs in models_and_probs:\n",
    "    fraction_pos, mean_predicted = calibration_curve(y_test, probs, n_bins=8)\n",
    "    ax.plot(mean_predicted, fraction_pos, 's-', label=name)\n",
    "\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curves (After Calibration)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When Calibration Matters Most\n",
    "\n",
    "| Use Case | Calibration Needed? | Why |\n",
    "|----------|-------------------|-----|\n",
    "| Medical diagnosis | **Yes** | Decisions based on probability thresholds |\n",
    "| Risk scoring (insurance, credit) | **Yes** | Probabilities directly used in pricing |\n",
    "| Ranking items (search, recommendation) | Less critical | Only relative ordering matters |\n",
    "| Binary classification with fixed threshold | Less critical | Only care about 0.5 boundary |\n",
    "| Combining multiple models | **Yes** | Probabilities from different models must be comparable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Ignoring calibration entirely**: Treating `predict_proba` output as true probabilities without checking\n",
    "2. **Using probabilities for ranking without checking calibration**: If you need actual probability values (not just ranking), you must check calibration\n",
    "3. **Calibrating on the training set**: Always calibrate on a held-out set or use cross-validation\n",
    "4. **Using isotonic regression with small datasets**: Isotonic regression can overfit; prefer Platt scaling with limited data\n",
    "5. **Forgetting to re-calibrate after retraining**: If you retrain the base model, the calibration must be redone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "- **Calibrated probabilities** mean predicted confidence matches actual frequency of outcomes\n",
    "- **Reliability diagrams** visualize calibration; perfect calibration = diagonal line\n",
    "- **Brier score** ($BS = \\frac{1}{n}\\sum(\\hat{p}_i - y_i)^2$) measures calibration quality (lower = better)\n",
    "- **Platt scaling** (sigmoid) fits a logistic curve to calibrate scores (good for small data)\n",
    "- **Isotonic regression** is more flexible but needs more data\n",
    "- Logistic Regression is naturally better calibrated than Random Forests\n",
    "- Use `CalibratedClassifierCV` from scikit-learn for easy calibration\n",
    "- Calibration is essential when probability values drive decisions (medicine, finance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}