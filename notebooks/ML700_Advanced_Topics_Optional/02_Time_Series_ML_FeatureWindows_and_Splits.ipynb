{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Time Series ML - Feature Windows and Splits\n",
    "\n",
    "**Module**: ML700 Advanced Topics (Optional)  \n",
    "**Notebook**: 02 - Time Series ML: Feature Windows and Splits  \n",
    "**Status**: OPTIONAL - This notebook covers advanced material beyond the core curriculum.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain why time series data violates the i.i.d. assumption and why random splits cause data leakage\n",
    "2. Use `TimeSeriesSplit` for walk-forward validation\n",
    "3. Engineer lag features, rolling window features, and date/time features\n",
    "4. Properly fit scalers on training data only within each fold\n",
    "5. Demonstrate the leakage effect of random KFold vs. TimeSeriesSplit\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of train/test splitting and cross-validation (Module ML100)\n",
    "- Familiarity with regression models (Module ML200)\n",
    "- Basic pandas and numpy operations\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Time Series Is Different](#1.-Why-Time-Series-Is-Different)\n",
    "2. [TimeSeriesSplit: Walk-Forward Validation](#2.-TimeSeriesSplit)\n",
    "3. [Feature Engineering for Time Series ML](#3.-Feature-Engineering)\n",
    "4. [Hands-On: Synthetic Time Series Modeling](#4.-Hands-On)\n",
    "5. [Visualizing Train/Test Splits Over Time](#5.-Visualizing-Splits)\n",
    "6. [Leakage Demo: TimeSeriesSplit vs Random KFold](#6.-Leakage-Demo)\n",
    "7. [Common Mistakes](#7.-Common-Mistakes)\n",
    "8. [Summary](#8.-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Time Series Is Different\n",
    "\n",
    "Most ML algorithms assume that data points are **independent and identically distributed (i.i.d.)**.\n",
    "Time series data violates this assumption in two key ways:\n",
    "\n",
    "1. **Temporal dependence**: Nearby observations are correlated (autocorrelation)\n",
    "2. **Temporal ordering**: The order of observations matters; future data must not leak into training\n",
    "\n",
    "**Consequence**: You cannot use random train/test splits or standard KFold cross-validation.\n",
    "Doing so allows the model to \"see the future,\" leading to overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TimeSeriesSplit: Walk-Forward Validation\n",
    "\n",
    "Scikit-learn provides `TimeSeriesSplit`, which implements **walk-forward validation**:\n",
    "\n",
    "```\n",
    "Fold 1: [TRAIN] [TEST]\n",
    "Fold 2: [TRAIN     ] [TEST]\n",
    "Fold 3: [TRAIN          ] [TEST]\n",
    "Fold 4: [TRAIN               ] [TEST]\n",
    "```\n",
    "\n",
    "- Training set grows with each fold\n",
    "- Test set always comes **after** the training set in time\n",
    "- No future information leaks into training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for Time Series ML\n",
    "\n",
    "When using standard ML models (not specialized time series models) on temporal data,\n",
    "you must create features that capture temporal patterns:\n",
    "\n",
    "### Lag Features\n",
    "Shifted values of the target or other variables:  \n",
    "`lag_1 = value(t-1)`, `lag_2 = value(t-2)`, etc.\n",
    "\n",
    "### Rolling Window Features\n",
    "Statistics computed over a sliding window:  \n",
    "`rolling_mean_7 = mean(value[t-7:t])`, `rolling_std_7 = std(value[t-7:t])`\n",
    "\n",
    "### Date/Time Features\n",
    "Extracted from timestamps:  \n",
    "`hour`, `day_of_week`, `month`, `is_weekend`, `is_holiday` (placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Synthetic Time Series Modeling\n",
    "\n",
    "Let us create a synthetic time series, engineer features, and evaluate with `TimeSeriesSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic time series: sine wave + linear trend + noise\n",
    "n_points = 500\n",
    "time_index = pd.date_range(start='2022-01-01', periods=n_points, freq='D')\n",
    "\n",
    "t = np.arange(n_points)\n",
    "trend = 0.02 * t\n",
    "seasonal = 5 * np.sin(2 * np.pi * t / 365.25)  # yearly cycle\n",
    "noise = np.random.normal(0, 1, n_points)\n",
    "values = trend + seasonal + noise\n",
    "\n",
    "df = pd.DataFrame({'date': time_index, 'value': values})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(df.index, df['value'], linewidth=0.8)\n",
    "ax.set_title('Synthetic Time Series (Trend + Seasonality + Noise)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def create_time_series_features(df, target_col='value', lags=[1, 2, 3, 7, 14], windows=[7, 14, 30]):\n",
    "    \"\"\"Create lag, rolling, and date features for time series ML.\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in lags:\n",
    "        result[f'lag_{lag}'] = result[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling window features\n",
    "    for window in windows:\n",
    "        result[f'rolling_mean_{window}'] = result[target_col].shift(1).rolling(window).mean()\n",
    "        result[f'rolling_std_{window}'] = result[target_col].shift(1).rolling(window).std()\n",
    "    \n",
    "    # Date/time features\n",
    "    result['day_of_week'] = result.index.dayofweek\n",
    "    result['month'] = result.index.month\n",
    "    result['day_of_year'] = result.index.dayofyear\n",
    "    result['is_weekend'] = (result.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "df_features = create_time_series_features(df)\n",
    "\n",
    "# Drop rows with NaN from lag/rolling features\n",
    "df_features.dropna(inplace=True)\n",
    "print(f\"Features created. Shape: {df_features.shape}\")\n",
    "print(f\"Columns: {list(df_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "feature_cols = [c for c in df_features.columns if c != 'value']\n",
    "X = df_features[feature_cols].values\n",
    "y = df_features['value'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with TimeSeriesSplit (proper way)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Use a pipeline so scaler is fit on train portion only in each fold\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "ts_scores = cross_val_score(pipeline, X, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "ts_rmse = np.sqrt(-ts_scores)\n",
    "\n",
    "print(\"TimeSeriesSplit Results (RMSE per fold):\")\n",
    "for i, score in enumerate(ts_rmse):\n",
    "    print(f\"  Fold {i+1}: {score:.4f}\")\n",
    "print(f\"  Mean RMSE: {ts_rmse.mean():.4f} (+/- {ts_rmse.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Train/Test Splits Over Time\n",
    "\n",
    "Let us visualize how TimeSeriesSplit divides the data in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TimeSeriesSplit folds\n",
    "fig, axes = plt.subplots(5, 1, figsize=(10, 8), sharex=True)\n",
    "dates = df_features.index\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(dates[train_idx], y[train_idx], 'b-', linewidth=0.7, label='Train')\n",
    "    ax.plot(dates[test_idx], y[test_idx], 'r-', linewidth=0.7, label='Test')\n",
    "    ax.set_ylabel(f'Fold {i+1}')\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "axes[0].set_title('TimeSeriesSplit: Walk-Forward Validation')\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each fold uses only past data for training and future data for testing.\")\n",
    "print(\"The training set grows with each fold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Leakage Demo: TimeSeriesSplit vs Random KFold\n",
    "\n",
    "Let us compare the performance estimates from TimeSeriesSplit (correct) vs random KFold (data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct: TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ts_scores = cross_val_score(pipeline, X, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "ts_rmse_mean = np.sqrt(-ts_scores).mean()\n",
    "\n",
    "# WRONG: Random KFold (causes data leakage for time series)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "kf_rmse_mean = np.sqrt(-kf_scores).mean()\n",
    "\n",
    "print(\"Comparison of Cross-Validation Strategies:\")\n",
    "print(f\"  TimeSeriesSplit Mean RMSE: {ts_rmse_mean:.4f}  (correct, no leakage)\")\n",
    "print(f\"  Random KFold Mean RMSE:   {kf_rmse_mean:.4f}  (WRONG, data leakage!)\")\n",
    "print()\n",
    "if kf_rmse_mean < ts_rmse_mean:\n",
    "    pct = (1 - kf_rmse_mean / ts_rmse_mean) * 100\n",
    "    print(f\"Random KFold appears {pct:.1f}% better -- this is an ILLUSION caused by data leakage.\")\n",
    "    print(\"The model 'sees the future' when random splits mix past and future data.\")\n",
    "else:\n",
    "    print(\"In this case the scores are similar, but random KFold is still methodologically wrong.\")\n",
    "    print(\"With stronger temporal patterns, the leakage effect would be more dramatic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "Random KFold often produces **overly optimistic** error estimates because the model trains on\n",
    "future data points that are correlated with test points. The \"real-world\" performance\n",
    "(predicting genuinely future values) is almost always worse than what random KFold suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Common Mistakes\n",
    "\n",
    "1. **Random splitting time series data**: Always respect temporal order. Use `TimeSeriesSplit` or manual temporal splits.\n",
    "2. **Using future data as features**: Lag features must use `shift(1)` or more. Rolling windows must not include the current value.\n",
    "3. **Not respecting temporal order in scaling**: Fit the scaler on the training portion only. Use `Pipeline` so this happens automatically in cross-validation.\n",
    "4. **Forgetting to drop NaN rows**: Lag and rolling features create NaN values at the beginning of the series. Drop them before modeling.\n",
    "5. **Ignoring the growing training set**: In walk-forward validation, earlier folds have less training data and may perform worse. This is expected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "- Time series data is **not i.i.d.** -- temporal order and autocorrelation must be respected\n",
    "- Use `TimeSeriesSplit` for walk-forward cross-validation (never random KFold)\n",
    "- Key feature types: **lag features**, **rolling window statistics**, **date/time features**\n",
    "- Always fit scalers and transformers on the **training portion only** (use `Pipeline`)\n",
    "- Random KFold on time series causes **data leakage**, producing overly optimistic estimates\n",
    "- The gap between TimeSeriesSplit and random KFold scores reveals the extent of leakage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}