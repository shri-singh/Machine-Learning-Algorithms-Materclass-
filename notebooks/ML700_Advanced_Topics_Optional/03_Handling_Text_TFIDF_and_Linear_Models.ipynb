{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Handling Text - TF-IDF and Linear Models\n",
    "\n",
    "**Module**: ML700 Advanced Topics (Optional)  \n",
    "**Notebook**: 03 - Handling Text: TF-IDF and Linear Models  \n",
    "**Status**: OPTIONAL - This notebook covers advanced material beyond the core curriculum.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the bag-of-words concept and how text is converted to numerical features\n",
    "2. Use `CountVectorizer` to create a term-count matrix\n",
    "3. Use `TfidfVectorizer` to create TF-IDF features\n",
    "4. Train a Logistic Regression classifier on text data\n",
    "5. Inspect top features per class to interpret the model\n",
    "6. Build a text classification pipeline using scikit-learn `Pipeline`\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Logistic Regression (Module ML300)\n",
    "- Familiarity with scikit-learn `fit`/`predict`/`transform` API\n",
    "- Basic understanding of matrices and sparse data\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Text as Features: Bag of Words](#1.-Text-as-Features)\n",
    "2. [CountVectorizer](#2.-CountVectorizer)\n",
    "3. [TF-IDF](#3.-TF-IDF)\n",
    "4. [Hands-On: Text Classification](#4.-Hands-On)\n",
    "5. [Inspecting Top Features Per Class](#5.-Top-Features)\n",
    "6. [Pipeline: TfidfVectorizer + LogisticRegression](#6.-Pipeline)\n",
    "7. [Tuning Text Features](#7.-Tuning)\n",
    "8. [Common Mistakes](#8.-Common-Mistakes)\n",
    "9. [Summary](#9.-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Text as Features: Bag of Words\n",
    "\n",
    "ML models need numerical input. The simplest way to convert text to numbers is the\n",
    "**bag-of-words** (BoW) representation:\n",
    "\n",
    "1. Build a **vocabulary** of all unique words across all documents\n",
    "2. Represent each document as a **vector** of word counts (or frequencies)\n",
    "\n",
    "This ignores word order (hence \"bag\") but works surprisingly well for many classification tasks.\n",
    "\n",
    "**Example**:\n",
    "- Document 1: \"the movie was great\"  \n",
    "- Document 2: \"the movie was terrible\"  \n",
    "- Vocabulary: [great, movie, terrible, the, was]  \n",
    "- Doc 1 vector: [1, 1, 0, 1, 1]  \n",
    "- Doc 2 vector: [0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CountVectorizer\n",
    "\n",
    "Scikit-learn's `CountVectorizer` implements the bag-of-words approach:\n",
    "- Tokenizes text (splits into words)\n",
    "- Builds a vocabulary\n",
    "- Produces a sparse count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CountVectorizer demo\n",
    "sample_docs = [\n",
    "    \"the movie was great and fun\",\n",
    "    \"the movie was terrible and boring\",\n",
    "    \"great acting and a fun story\",\n",
    "    \"terrible plot and boring dialogue\",\n",
    "]\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "X_counts = count_vec.fit_transform(sample_docs)\n",
    "\n",
    "print(\"Vocabulary:\", count_vec.get_feature_names_out())\n",
    "print(\"\\nCount matrix (dense):\")\n",
    "print(X_counts.toarray())\n",
    "print(f\"\\nShape: {X_counts.shape} (4 documents, {X_counts.shape[1]} unique words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) improves on raw counts by\n",
    "down-weighting words that appear in many documents (common words like \"the\") and\n",
    "up-weighting words that are distinctive to specific documents.\n",
    "\n",
    "$$\\text{tfidf}(t, d) = \\text{tf}(t, d) \\cdot \\log\\frac{N}{\\text{df}(t)}$$\n",
    "\n",
    "Where:\n",
    "- $\\text{tf}(t, d)$ = frequency of term $t$ in document $d$\n",
    "- $N$ = total number of documents\n",
    "- $\\text{df}(t)$ = number of documents containing term $t$\n",
    "\n",
    "**Intuition**: A word that appears in every document (like \"the\") gets a low IDF.\n",
    "A word that appears in only one document gets a high IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer demo on the same sample docs\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(sample_docs)\n",
    "\n",
    "print(\"TF-IDF matrix (rounded):\")\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray().round(3),\n",
    "    columns=tfidf_vec.get_feature_names_out(),\n",
    "    index=[f'Doc {i+1}' for i in range(len(sample_docs))]\n",
    ")\n",
    "print(tfidf_df.to_string())\n",
    "print()\n",
    "print(\"Notice: common words like 'and' have lower TF-IDF scores.\")\n",
    "print(\"Distinctive words like 'great', 'terrible' have higher scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Text Classification\n",
    "\n",
    "Let us build a simple sentiment classifier using synthetic movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic movie review dataset\n",
    "positive_reviews = [\n",
    "    \"This movie was absolutely wonderful and heartwarming\",\n",
    "    \"Brilliant acting and a fantastic storyline throughout\",\n",
    "    \"I loved every minute of this beautiful film\",\n",
    "    \"An excellent movie with great performances from the cast\",\n",
    "    \"The best movie I have seen this year truly amazing\",\n",
    "    \"Superb direction and outstanding cinematography made this a joy\",\n",
    "    \"A masterpiece of storytelling with incredible depth and emotion\",\n",
    "    \"Funny charming and thoroughly entertaining from start to finish\",\n",
    "    \"The performances were stellar and the script was brilliant\",\n",
    "    \"A delightful film that exceeded all my expectations\",\n",
    "    \"Wonderfully crafted with amazing attention to detail\",\n",
    "    \"Exceptional acting and a truly moving story\",\n",
    "    \"One of the finest films of the decade absolutely loved it\",\n",
    "    \"A beautiful and inspiring movie that everyone should see\",\n",
    "    \"Gripping suspenseful and deeply satisfying throughout\",\n",
    "    \"The humor was perfect and the characters were lovable\",\n",
    "    \"An outstanding achievement in cinema great work\",\n",
    "    \"Uplifting and powerful with a wonderful message\",\n",
    "    \"The best performance I have ever seen truly remarkable\",\n",
    "    \"A fantastic journey with superb writing and acting\",\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This movie was absolutely terrible and a waste of time\",\n",
    "    \"Awful acting and a boring storyline throughout\",\n",
    "    \"I hated every minute of this dreadful film\",\n",
    "    \"A horrible movie with bad performances from the cast\",\n",
    "    \"The worst movie I have seen this year truly awful\",\n",
    "    \"Poor direction and dull cinematography made this painful\",\n",
    "    \"A disaster of storytelling with no depth or emotion\",\n",
    "    \"Boring predictable and thoroughly disappointing from start to finish\",\n",
    "    \"The performances were weak and the script was terrible\",\n",
    "    \"A dreadful film that failed all my expectations\",\n",
    "    \"Poorly crafted with no attention to detail whatsoever\",\n",
    "    \"Terrible acting and a truly depressing waste of talent\",\n",
    "    \"One of the worst films of the decade absolutely hated it\",\n",
    "    \"A boring and uninspiring movie that nobody should see\",\n",
    "    \"Slow confusing and deeply unsatisfying throughout\",\n",
    "    \"The humor was forced and the characters were annoying\",\n",
    "    \"An embarrassing failure in cinema bad work\",\n",
    "    \"Depressing and pointless with a confused message\",\n",
    "    \"The worst performance I have ever seen truly forgettable\",\n",
    "    \"A terrible journey with awful writing and acting\",\n",
    "]\n",
    "\n",
    "# Combine into dataset\n",
    "texts = positive_reviews + negative_reviews\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)  # 1=positive, 0=negative\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_text)}, Test samples: {len(X_test_text)}\")\n",
    "print(f\"Class distribution (train): {sum(y_train)} positive, {len(y_train) - sum(y_train)} negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize with TF-IDF and train Logistic Regression\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (test):  {X_test_tfidf.shape}\")\n",
    "\n",
    "# Train classifier\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = lr.predict(X_test_tfidf)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspecting Top Features Per Class\n",
    "\n",
    "One advantage of linear models on text is **interpretability**: we can inspect which words\n",
    "are most associated with each class by looking at the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top features per class\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefficients = lr.coef_[0]\n",
    "\n",
    "# Top positive words (highest coefficients -> predict positive class)\n",
    "top_positive_idx = np.argsort(coefficients)[-10:]\n",
    "top_negative_idx = np.argsort(coefficients)[:10]\n",
    "\n",
    "print(\"Top 10 words associated with POSITIVE reviews:\")\n",
    "for idx in reversed(top_positive_idx):\n",
    "    print(f\"  {feature_names[idx]:20s}  coef: {coefficients[idx]:+.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 words associated with NEGATIVE reviews:\")\n",
    "for idx in top_negative_idx:\n",
    "    print(f\"  {feature_names[idx]:20s}  coef: {coefficients[idx]:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "n_top = 10\n",
    "top_pos_idx = np.argsort(coefficients)[-n_top:]\n",
    "top_neg_idx = np.argsort(coefficients)[:n_top]\n",
    "top_idx = np.concatenate([top_neg_idx, top_pos_idx])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "colors = ['#d32f2f' if c < 0 else '#388e3c' for c in coefficients[top_idx]]\n",
    "ax.barh(range(len(top_idx)), coefficients[top_idx], color=colors)\n",
    "ax.set_yticks(range(len(top_idx)))\n",
    "ax.set_yticklabels(feature_names[top_idx])\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('Top Words by Logistic Regression Coefficient')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green bars (positive coef) = words associated with positive sentiment\")\n",
    "print(\"Red bars (negative coef) = words associated with negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline: TfidfVectorizer + LogisticRegression\n",
    "\n",
    "Using a `Pipeline` ensures that the vectorizer is always fit on training data only\n",
    "and avoids data leakage during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validate the pipeline on the full dataset\n",
    "cv_scores = cross_val_score(text_pipeline, texts, labels, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Pipeline Cross-Validation (5-fold):\")\n",
    "for i, score in enumerate(cv_scores):\n",
    "    print(f\"  Fold {i+1}: {score:.2f}\")\n",
    "print(f\"  Mean Accuracy: {cv_scores.mean():.2f} (+/- {cv_scores.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pipeline on full training set and predict on test\n",
    "text_pipeline.fit(X_train_text, y_train)\n",
    "\n",
    "# Predict on new unseen examples\n",
    "new_reviews = [\n",
    "    \"This was a great movie with wonderful acting\",\n",
    "    \"Terrible film I was bored the entire time\",\n",
    "    \"The story was okay but nothing special\",\n",
    "]\n",
    "\n",
    "predictions = text_pipeline.predict(new_reviews)\n",
    "probabilities = text_pipeline.predict_proba(new_reviews)\n",
    "\n",
    "print(\"Predictions on new reviews:\")\n",
    "for review, pred, prob in zip(new_reviews, predictions, probabilities):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"  '{review}'\")\n",
    "    print(f\"    -> {sentiment} (P(positive)={prob[1]:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tuning Text Features\n",
    "\n",
    "Several parameters in `TfidfVectorizer` can significantly affect performance:\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `max_features` | Limit vocabulary size | 5000, 10000, 50000 |\n",
    "| `min_df` | Ignore words appearing in fewer than N documents | 2, 5, 0.01 |\n",
    "| `max_df` | Ignore words appearing in more than N% of documents | 0.9, 0.95 |\n",
    "| `ngram_range` | Include n-grams (word combinations) | (1,1), (1,2), (1,3) |\n",
    "| `sublinear_tf` | Apply sublinear TF scaling (1 + log(tf)) | True/False |\n",
    "\n",
    "**N-grams**: Instead of single words, include pairs or triples:  \n",
    "- Unigrams (1,1): \"not\", \"good\"  \n",
    "- Bigrams (1,2): \"not\", \"good\", \"not good\"  \n",
    "- Trigrams (1,3): \"not\", \"good\", \"not good\", \"not good enough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: effect of n-grams and max_features\n",
    "configs = [\n",
    "    {'ngram_range': (1, 1), 'max_features': None, 'label': 'Unigrams (no limit)'},\n",
    "    {'ngram_range': (1, 2), 'max_features': None, 'label': 'Uni+Bigrams (no limit)'},\n",
    "    {'ngram_range': (1, 2), 'max_features': 50, 'label': 'Uni+Bigrams (max_features=50)'},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            ngram_range=config['ngram_range'],\n",
    "            max_features=config['max_features']\n",
    "        )),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, texts, labels, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Fit to see vocabulary size\n",
    "    pipe.fit(texts, labels)\n",
    "    vocab_size = len(pipe.named_steps['tfidf'].vocabulary_)\n",
    "    \n",
    "    print(f\"{config['label']:40s} | vocab: {vocab_size:4d} | accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Not using pipelines for text**: Fitting `TfidfVectorizer` on the full dataset before splitting causes data leakage. Always use `Pipeline` so the vectorizer is fit only on training data.\n",
    "2. **Too many features without `max_features`**: With large corpora, the vocabulary can be enormous. Use `max_features`, `min_df`, and `max_df` to control vocabulary size.\n",
    "3. **Ignoring n-grams**: Unigrams alone miss important phrases like \"not good\" or \"very bad\". Try `ngram_range=(1, 2)` or `(1, 3)`.\n",
    "4. **Using TF-IDF on very short texts**: For very short texts (tweets, single words), TF-IDF may not be effective because term frequencies are all 0 or 1.\n",
    "5. **Forgetting to use the same vectorizer for prediction**: Always use `transform` (not `fit_transform`) on new data, or use a `Pipeline` which handles this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "- **Bag of words** converts text to numerical features by counting word occurrences\n",
    "- **CountVectorizer** creates raw count matrices; **TfidfVectorizer** adds IDF weighting\n",
    "- TF-IDF formula: $\\text{tfidf}(t,d) = \\text{tf}(t,d) \\cdot \\log\\frac{N}{\\text{df}(t)}$\n",
    "- **Logistic Regression** on TF-IDF features is a strong baseline for text classification\n",
    "- Inspect **model coefficients** to see which words drive predictions (interpretability)\n",
    "- Always use **Pipeline** (TfidfVectorizer + classifier) to prevent data leakage\n",
    "- Tune `ngram_range`, `max_features`, `min_df`, `max_df` for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}