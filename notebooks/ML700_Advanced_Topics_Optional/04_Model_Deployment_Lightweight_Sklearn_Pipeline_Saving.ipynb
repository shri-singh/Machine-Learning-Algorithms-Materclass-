{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Model Deployment - Lightweight Sklearn Pipeline Saving\n",
    "\n",
    "**Module**: ML700 Advanced Topics (Optional)  \n",
    "**Notebook**: 04 - Model Deployment: Lightweight Sklearn Pipeline Saving  \n",
    "**Status**: OPTIONAL - This notebook covers advanced material beyond the core curriculum.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the difference between training and serving, and why saving the full pipeline matters\n",
    "2. Use `joblib` to save and load scikit-learn pipelines\n",
    "3. Build a complete pipeline (preprocessing + model) and persist it to disk\n",
    "4. Verify that a loaded model produces identical predictions\n",
    "5. Follow best practices for model versioning and reproducibility\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of scikit-learn pipelines and `ColumnTransformer` (Module ML600)\n",
    "- Familiarity with `Pipeline`, `fit`, `predict` API\n",
    "- Basic file I/O concepts\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Training vs. Serving](#1.-Training-vs.-Serving)\n",
    "2. [Why Save the Full Pipeline?](#2.-Why-Save-the-Full-Pipeline?)\n",
    "3. [Joblib: Save and Load](#3.-Joblib)\n",
    "4. [Hands-On: Build, Save, Load, Predict](#4.-Hands-On)\n",
    "5. [Pickle vs. Joblib](#5.-Pickle-vs.-Joblib)\n",
    "6. [Best Practices](#6.-Best-Practices)\n",
    "7. [Beyond Joblib: ONNX and Docker](#7.-Beyond-Joblib)\n",
    "8. [Common Mistakes](#8.-Common-Mistakes)\n",
    "9. [Summary](#9.-Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Training vs. Serving\n",
    "\n",
    "In production ML, there are two distinct phases:\n",
    "\n",
    "**Training** (offline):\n",
    "- Access to full dataset\n",
    "- Fit preprocessors (scalers, encoders) and model\n",
    "- Evaluate and tune hyperparameters\n",
    "- Can take minutes to hours\n",
    "\n",
    "**Serving / Inference** (online):\n",
    "- Receive new, unseen data points one at a time (or in small batches)\n",
    "- Apply the same preprocessing and model to produce predictions\n",
    "- Must be fast (milliseconds to seconds)\n",
    "- **Must not re-fit anything** -- only `transform` and `predict`\n",
    "\n",
    "The bridge between training and serving is **model serialization**: saving the trained\n",
    "pipeline to disk so it can be loaded elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Save the Full Pipeline?\n",
    "\n",
    "A common mistake is saving only the model (e.g., the Random Forest) without the preprocessor.\n",
    "This breaks because:\n",
    "\n",
    "- The model expects **preprocessed** input (scaled, encoded, etc.)\n",
    "- At serving time, you need the **exact same** scaler parameters (mean, std) that were fit on training data\n",
    "- Rebuilding the preprocessor requires the original training data, which may not be available\n",
    "\n",
    "**Solution**: Always save the **entire pipeline** (preprocessor + model) as a single artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joblib: Save and Load\n",
    "\n",
    "`joblib` is the recommended way to serialize scikit-learn objects:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save\n",
    "joblib.dump(pipeline, 'model.joblib')\n",
    "\n",
    "# Load\n",
    "loaded_pipeline = joblib.load('model.joblib')\n",
    "```\n",
    "\n",
    "`joblib` is preferred over `pickle` for scikit-learn because it handles large NumPy arrays\n",
    "more efficiently (using memory mapping and compression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Build, Save, Load, Predict\n",
    "\n",
    "Let us build a full pipeline on the breast cancer dataset, save it, load it, and verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sklearn\n",
    "import os\n",
    "import tempfile\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a DataFrame for realistic column-based processing\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set:     {X_test.shape}\")\n",
    "print(f\"Features:     {list(X.columns[:5])} ... ({X.shape[1]} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a full pipeline: ColumnTransformer (scaling) + RandomForest\n",
    "# All features are numeric in this dataset, so we scale all of them\n",
    "numeric_features = list(X.columns)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full pipeline with joblib\n",
    "save_dir = tempfile.mkdtemp()\n",
    "model_path = os.path.join(save_dir, 'breast_cancer_pipeline.joblib')\n",
    "\n",
    "joblib.dump(full_pipeline, model_path)\n",
    "\n",
    "file_size_kb = os.path.getsize(model_path) / 1024\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {file_size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline from disk\n",
    "loaded_pipeline = joblib.load(model_path)\n",
    "\n",
    "print(f\"Loaded pipeline type: {type(loaded_pipeline)}\")\n",
    "print(f\"Pipeline steps: {[step[0] for step in loaded_pipeline.steps]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: loaded model produces IDENTICAL predictions\n",
    "y_pred_original = full_pipeline.predict(X_test)\n",
    "y_pred_loaded = loaded_pipeline.predict(X_test)\n",
    "\n",
    "predictions_match = np.array_equal(y_pred_original, y_pred_loaded)\n",
    "print(f\"Predictions match: {predictions_match}\")\n",
    "\n",
    "# Also check predicted probabilities\n",
    "proba_original = full_pipeline.predict_proba(X_test)\n",
    "proba_loaded = loaded_pipeline.predict_proba(X_test)\n",
    "\n",
    "probas_match = np.allclose(proba_original, proba_loaded)\n",
    "print(f\"Probabilities match: {probas_match}\")\n",
    "\n",
    "if predictions_match and probas_match:\n",
    "    print(\"\\nThe loaded model is identical to the original. Safe to deploy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate serving: predict on new data\n",
    "# In production, this is all you need: load the pipeline and call predict\n",
    "new_sample = X_test.iloc[:3]  # simulate 3 new patients\n",
    "\n",
    "serving_pipeline = joblib.load(model_path)\n",
    "predictions = serving_pipeline.predict(new_sample)\n",
    "probabilities = serving_pipeline.predict_proba(new_sample)\n",
    "\n",
    "print(\"Serving predictions on 3 new samples:\")\n",
    "for i in range(len(new_sample)):\n",
    "    label = data.target_names[predictions[i]]\n",
    "    confidence = probabilities[i].max()\n",
    "    print(f\"  Sample {i+1}: {label} (confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pickle vs. Joblib\n",
    "\n",
    "| Feature | `pickle` | `joblib` |\n",
    "|---------|----------|----------|\n",
    "| Part of standard library | Yes | No (but included with scikit-learn) |\n",
    "| NumPy array handling | Standard serialization | Optimized (memory mapping, compression) |\n",
    "| Large model files | Slower | Faster for large arrays |\n",
    "| API | `pickle.dump/load` | `joblib.dump/load` |\n",
    "| Compression | Manual | Built-in (`compress` parameter) |\n",
    "\n",
    "**Recommendation**: Use `joblib` for scikit-learn models. It is more efficient for objects\n",
    "containing large NumPy arrays (like Random Forests with many trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joblib with compression\n",
    "compressed_path = os.path.join(save_dir, 'breast_cancer_pipeline_compressed.joblib')\n",
    "joblib.dump(full_pipeline, compressed_path, compress=3)\n",
    "\n",
    "original_size = os.path.getsize(model_path) / 1024\n",
    "compressed_size = os.path.getsize(compressed_path) / 1024\n",
    "\n",
    "print(f\"Original size:   {original_size:.1f} KB\")\n",
    "print(f\"Compressed size: {compressed_size:.1f} KB\")\n",
    "print(f\"Compression ratio: {original_size / compressed_size:.1f}x\")\n",
    "\n",
    "# Verify compressed model works identically\n",
    "loaded_compressed = joblib.load(compressed_path)\n",
    "y_pred_compressed = loaded_compressed.predict(X_test)\n",
    "print(f\"Compressed model predictions match: {np.array_equal(y_pred_original, y_pred_compressed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "### Save the entire pipeline, not individual components\n",
    "The pipeline encapsulates all preprocessing steps. Saving just the model means you must\n",
    "recreate the preprocessor at serving time, which is error-prone.\n",
    "\n",
    "### Version your model files\n",
    "Use meaningful file names that include version or date information:\n",
    "```\n",
    "model_v1.0_2024-01-15.joblib\n",
    "model_v1.1_2024-02-01.joblib\n",
    "```\n",
    "\n",
    "### Log scikit-learn version and feature names\n",
    "Version mismatches between training and serving can cause subtle bugs or failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: save metadata alongside the model\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "metadata = {\n",
    "    'model_name': 'breast_cancer_classifier',\n",
    "    'model_version': '1.0',\n",
    "    'sklearn_version': sklearn.__version__,\n",
    "    'numpy_version': np.__version__,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'feature_names': list(X.columns),\n",
    "    'n_features': X.shape[1],\n",
    "    'target_names': list(data.target_names),\n",
    "    'training_samples': X_train.shape[0],\n",
    "    'test_accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(save_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Model metadata saved:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice: verify at load time that versions match\n",
    "def load_model_with_checks(model_path, metadata_path):\n",
    "    \"\"\"Load a model and verify version compatibility.\"\"\"\n",
    "    # Load metadata\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    # Check sklearn version\n",
    "    if meta['sklearn_version'] != sklearn.__version__:\n",
    "        print(f\"WARNING: Model trained with sklearn {meta['sklearn_version']}, \"\n",
    "              f\"but current version is {sklearn.__version__}\")\n",
    "    else:\n",
    "        print(f\"sklearn version match: {sklearn.__version__}\")\n",
    "    \n",
    "    # Load model\n",
    "    pipeline = joblib.load(model_path)\n",
    "    \n",
    "    print(f\"Model loaded: {meta['model_name']} v{meta['model_version']}\")\n",
    "    print(f\"Trained on: {meta['training_date']}\")\n",
    "    print(f\"Expected features: {meta['n_features']}\")\n",
    "    print(f\"Training accuracy: {meta['test_accuracy']:.4f}\")\n",
    "    \n",
    "    return pipeline, meta\n",
    "\n",
    "loaded_pipe, loaded_meta = load_model_with_checks(model_path, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Beyond Joblib: ONNX and Docker (Conceptual)\n",
    "\n",
    "For production deployments beyond simple scripts, two technologies are commonly used:\n",
    "\n",
    "### ONNX (Open Neural Network Exchange)\n",
    "- An open format for representing ML models\n",
    "- Convert scikit-learn models to ONNX for language-agnostic serving (C++, Java, etc.)\n",
    "- Faster inference in some cases\n",
    "- Library: `skl2onnx` for scikit-learn to ONNX conversion\n",
    "\n",
    "### Docker\n",
    "- Package your model, dependencies, and serving code into a container\n",
    "- Ensures identical environment between development and production\n",
    "- Typical pattern: Flask/FastAPI app inside a Docker container that loads the joblib model\n",
    "\n",
    "These are beyond the scope of this notebook, but important to know about for real deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Mistakes\n",
    "\n",
    "1. **Saving the model without the preprocessor**: At serving time, raw data must go through the same preprocessing. Save the entire `Pipeline`.\n",
    "2. **Version mismatch**: A model saved with sklearn 1.2 may not load correctly in sklearn 1.4. Always log and check versions.\n",
    "3. **Not testing the loaded model**: Always verify that `loaded_model.predict(X_test)` matches the original predictions before deploying.\n",
    "4. **Hardcoding feature order**: If the input DataFrame columns change order, predictions will be wrong. Use `ColumnTransformer` with explicit column names.\n",
    "5. **Not saving metadata**: Without metadata (feature names, training date, performance metrics), you cannot audit or debug deployed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "shutil.rmtree(save_dir)\n",
    "print(f\"Cleaned up temporary directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "- **Training vs. serving**: Train offline, serve online. The bridge is model serialization.\n",
    "- **Save the full pipeline**: Always serialize the entire `Pipeline` (preprocessor + model), not just the model.\n",
    "- **joblib** is preferred over pickle for scikit-learn objects (better NumPy array handling).\n",
    "- **Workflow**: `joblib.dump(pipeline, path)` to save, `joblib.load(path)` to restore.\n",
    "- **Best practices**:\n",
    "  - Version your model files\n",
    "  - Log sklearn version and feature names in metadata\n",
    "  - Verify loaded model produces identical results\n",
    "  - Use compression for large models (`compress` parameter)\n",
    "- **Beyond joblib**: ONNX for cross-language serving, Docker for reproducible deployment environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}