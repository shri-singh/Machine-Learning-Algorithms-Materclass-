{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 3: Customer Segmentation (Clustering)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this project you will be able to:\n",
    "\n",
    "- Frame a business problem as an unsupervised learning task\n",
    "- Apply feature scaling as a prerequisite for distance-based algorithms\n",
    "- Use PCA for dimensionality reduction and visualization\n",
    "- Determine the optimal number of clusters using the elbow method and silhouette scores\n",
    "- Implement and compare KMeans, Hierarchical Clustering, and DBSCAN\n",
    "- Profile and interpret clusters for actionable business recommendations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Libraries: numpy, pandas, matplotlib, seaborn, scikit-learn, scipy\n",
    "- Familiarity with clustering concepts and distance metrics\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Problem Statement & Business Context](#1)\n",
    "2. [Data Generation](#2)\n",
    "3. [Exploratory Data Analysis](#3)\n",
    "4. [Preprocessing](#4)\n",
    "5. [Dimensionality Reduction (PCA)](#5)\n",
    "6. [KMeans Clustering](#6)\n",
    "7. [Cluster Visualization](#7)\n",
    "8. [Cluster Profiling](#8)\n",
    "9. [Hierarchical Clustering](#9)\n",
    "10. [DBSCAN](#10)\n",
    "11. [Business Interpretation](#11)\n",
    "12. [Conclusions and Marketing Recommendations](#12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Problem Statement & Business Context\n",
    "\n",
    "**Scenario:** An e-commerce company wants to move away from one-size-fits-all marketing. The marketing team believes different customer segments exist and wants data-driven groups to target with tailored campaigns, personalized offers, and differentiated communication strategies.\n",
    "\n",
    "**Goal:** Identify natural customer segments based on purchasing behavior and demographics. For each segment, provide a profile that the marketing team can use to design targeted campaigns.\n",
    "\n",
    "**Why Clustering?** There are no predefined labels. We do not know how many groups exist or what they look like. This is a classic unsupervised learning problem where the algorithm must discover structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Data Generation\n",
    "\n",
    "We create a synthetic dataset of 500 customers with 7 behavioral and demographic features. The data is designed to have natural cluster structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# We create 4 natural clusters by mixing distributions\n",
    "# Cluster 1: Young, high income, high spending (\"Affluent Millennials\")\n",
    "c1 = 130\n",
    "# Cluster 2: Middle-aged, moderate income, moderate spending (\"Steady Middle Class\")\n",
    "c2 = 150\n",
    "# Cluster 3: Older, high income, low frequency (\"Wealthy Infrequent\")\n",
    "c3 = 100\n",
    "# Cluster 4: Young, low income, budget shoppers (\"Budget Conscious\")\n",
    "c4 = 120\n",
    "\n",
    "def make_cluster(n, age_mu, age_std, income_mu, income_std, spend_mu, spend_std,\n",
    "                 freq_mu, freq_std, recency_mu, recency_std, products_mu, products_std,\n",
    "                 clv_mu, clv_std):\n",
    "    return pd.DataFrame({\n",
    "        \"age\": np.random.normal(age_mu, age_std, n).clip(18, 80).astype(int),\n",
    "        \"annual_income\": np.random.normal(income_mu, income_std, n).clip(15000, 250000).round(0),\n",
    "        \"spending_score\": np.random.normal(spend_mu, spend_std, n).clip(1, 100).round(0),\n",
    "        \"avg_purchase_frequency\": np.random.normal(freq_mu, freq_std, n).clip(0.5, 30).round(1),\n",
    "        \"days_since_last_purchase\": np.random.normal(recency_mu, recency_std, n).clip(1, 365).astype(int),\n",
    "        \"num_products_bought\": np.random.normal(products_mu, products_std, n).clip(1, 100).astype(int),\n",
    "        \"customer_lifetime_value\": np.random.normal(clv_mu, clv_std, n).clip(100, 50000).round(0),\n",
    "    })\n",
    "\n",
    "#                          age     income     spend     freq     recency   products   CLV\n",
    "df1 = make_cluster(c1,    28, 5,  85000, 15000,  80, 10,   12, 3,   15, 10,   40, 12,   8000, 2000)\n",
    "df2 = make_cluster(c2,    42, 8,  55000, 12000,  50, 12,    6, 2,   30, 15,   20, 8,    4000, 1500)\n",
    "df3 = make_cluster(c3,    58, 7,  95000, 20000,  30, 10,    2, 1,  100, 40,   10, 5,    6000, 2500)\n",
    "df4 = make_cluster(c4,    25, 5,  30000, 8000,   65, 12,    8, 3,   20, 10,   25, 8,    1500, 600)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "# Shuffle so clusters are not in order\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "features = df.columns.tolist()\n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axes.ravel(), features)):\n",
    "    ax.hist(df[col], bins=30, edgecolor=\"black\", alpha=0.7, color=\"teal\")\n",
    "    ax.set_title(col, fontsize=11)\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[1, 3].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, square=True)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot (subsample for speed)\n",
    "key_features = [\"annual_income\", \"spending_score\", \"avg_purchase_frequency\", \"customer_lifetime_value\"]\n",
    "sns.pairplot(df[key_features], diag_kind=\"hist\", plot_kws={\"alpha\": 0.4, \"s\": 15})\n",
    "plt.suptitle(\"Pairplot of Key Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Preprocessing\n",
    "\n",
    "Clustering algorithms based on distance (KMeans, Hierarchical, DBSCAN) are sensitive to feature scales. We standardize all features to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Mean of each feature (should be ~0): {X_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"Std of each feature (should be ~1):  {X_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Dimensionality Reduction (PCA)\n",
    "\n",
    "We reduce the 7 features to 2 principal components for visualization. PCA captures the directions of maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.round(4)}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Full PCA for cumulative variance\n",
    "pca_full = PCA(random_state=42).fit(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "            pca_full.explained_variance_ratio_, color=\"steelblue\", edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Principal Component\")\n",
    "axes[0].set_ylabel(\"Explained Variance Ratio\")\n",
    "axes[0].set_title(\"Scree Plot\")\n",
    "\n",
    "# Cumulative variance\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, len(cumvar) + 1), cumvar, \"o-\", color=\"steelblue\")\n",
    "axes[1].axhline(y=0.90, color=\"red\", linestyle=\"--\", label=\"90% threshold\")\n",
    "axes[1].set_xlabel(\"Number of Components\")\n",
    "axes[1].set_ylabel(\"Cumulative Explained Variance\")\n",
    "axes[1].set_title(\"Cumulative Explained Variance\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D scatter plot before clustering\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, s=20, color=\"gray\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "plt.title(\"2D PCA Projection (No Clusters Yet)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. KMeans Clustering\n",
    "\n",
    "We use the elbow method and silhouette scores to determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, inertias, \"o-\", color=\"steelblue\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Number of Clusters (k)\")\n",
    "axes[0].set_ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
    "axes[0].set_title(\"Elbow Method\")\n",
    "axes[0].set_xticks(list(k_range))\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(k_range, silhouette_scores, \"o-\", color=\"coral\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Number of Clusters (k)\")\n",
    "axes[1].set_ylabel(\"Silhouette Score\")\n",
    "axes[1].set_title(\"Silhouette Score vs k\")\n",
    "axes[1].set_xticks(list(k_range))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Best k by silhouette score: {best_k} (score = {max(silhouette_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final KMeans with optimal k\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df[\"kmeans_cluster\"] = kmeans_labels\n",
    "\n",
    "print(f\"KMeans cluster sizes (k={best_k}):\")\n",
    "print(df[\"kmeans_cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PCA scatter colored by KMeans cluster\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels,\n",
    "                     cmap=\"viridis\", alpha=0.6, s=30, edgecolors=\"k\", linewidths=0.3)\n",
    "\n",
    "# Plot cluster centers projected onto PCA space\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "ax.scatter(centers_pca[:, 0], centers_pca[:, 1], c=\"red\", marker=\"X\",\n",
    "           s=200, edgecolors=\"black\", linewidths=2, label=\"Centroids\")\n",
    "\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "ax.set_title(f\"KMeans Clusters (k={best_k}) in PCA Space\")\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-space scatter matrices colored by cluster\n",
    "plot_features = [\"annual_income\", \"spending_score\", \"avg_purchase_frequency\", \"customer_lifetime_value\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "for ax, (i, j) in zip(axes.ravel(), pairs):\n",
    "    for cluster_id in range(best_k):\n",
    "        mask = kmeans_labels == cluster_id\n",
    "        ax.scatter(df.loc[mask, plot_features[i]], df.loc[mask, plot_features[j]],\n",
    "                   alpha=0.5, s=20, label=f\"Cluster {cluster_id}\")\n",
    "    ax.set_xlabel(plot_features[i])\n",
    "    ax.set_ylabel(plot_features[j])\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "plt.suptitle(\"Cluster Assignments in Feature Space\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. Cluster Profiling\n",
    "\n",
    "We compute the mean of each feature per cluster to understand what makes each segment unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"age\", \"annual_income\", \"spending_score\", \"avg_purchase_frequency\",\n",
    "                \"days_since_last_purchase\", \"num_products_bought\", \"customer_lifetime_value\"]\n",
    "\n",
    "cluster_profile = df.groupby(\"kmeans_cluster\")[feature_cols].mean().round(1)\n",
    "cluster_profile[\"count\"] = df.groupby(\"kmeans_cluster\").size()\n",
    "cluster_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart / heatmap of normalized cluster profiles\n",
    "profile_normalized = cluster_profile[feature_cols].copy()\n",
    "for col in feature_cols:\n",
    "    col_min = profile_normalized[col].min()\n",
    "    col_max = profile_normalized[col].max()\n",
    "    if col_max > col_min:\n",
    "        profile_normalized[col] = (profile_normalized[col] - col_min) / (col_max - col_min)\n",
    "    else:\n",
    "        profile_normalized[col] = 0.5\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.heatmap(profile_normalized, annot=cluster_profile[feature_cols].values, fmt=\".1f\",\n",
    "            cmap=\"YlOrRd\", xticklabels=feature_cols,\n",
    "            yticklabels=[f\"Cluster {i}\" for i in range(best_k)])\n",
    "plt.title(\"Cluster Profiles (color = normalized, numbers = actual means)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by cluster for key features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for ax, col in zip(axes.ravel(), feature_cols):\n",
    "    df.boxplot(column=col, by=\"kmeans_cluster\", ax=ax)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "\n",
    "axes[1, 3].set_visible(False)\n",
    "plt.suptitle(\"Feature Distributions by Cluster\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "## 9. Hierarchical Clustering\n",
    "\n",
    "We compare KMeans with Agglomerative (Hierarchical) Clustering. A dendrogram helps visualize how clusters merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Compute linkage matrix on a subsample for a cleaner dendrogram\n",
    "subsample_idx = np.random.choice(len(X_scaled), size=100, replace=False)\n",
    "linkage_matrix = linkage(X_scaled[subsample_idx], method=\"ward\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode=\"level\", p=5, leaf_font_size=8, color_threshold=10)\n",
    "plt.title(\"Dendrogram (Ward Linkage, 100-sample subset)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Agglomerative Clustering with same k as KMeans\n",
    "agg = AgglomerativeClustering(n_clusters=best_k, linkage=\"ward\")\n",
    "agg_labels = agg.fit_predict(X_scaled)\n",
    "\n",
    "df[\"hierarchical_cluster\"] = agg_labels\n",
    "\n",
    "# Compare with KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(kmeans_labels, agg_labels)\n",
    "nmi = normalized_mutual_info_score(kmeans_labels, agg_labels)\n",
    "sil_agg = silhouette_score(X_scaled, agg_labels)\n",
    "\n",
    "print(f\"Hierarchical Clustering (k={best_k}):\")\n",
    "print(f\"  Silhouette Score: {sil_agg:.4f}\")\n",
    "print(f\"\\nAgreement with KMeans:\")\n",
    "print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"  Normalized Mutual Information: {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side PCA visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, (labels, title) in zip(axes, [(kmeans_labels, \"KMeans\"), (agg_labels, \"Hierarchical\")]):\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap=\"viridis\",\n",
    "                         alpha=0.6, s=25, edgecolors=\"k\", linewidths=0.3)\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(f\"{title} Clusters\")\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Cluster\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "## 10. DBSCAN\n",
    "\n",
    "DBSCAN is a density-based algorithm that can find arbitrarily shaped clusters and automatically identifies outliers (label = -1). It does not require specifying k, but needs `eps` (neighborhood radius) and `min_samples` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use k-distance plot to estimate eps\n",
    "k_neighbors = 10\n",
    "nn = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "nn.fit(X_scaled)\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "k_distances = np.sort(distances[:, -1])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_distances, color=\"steelblue\")\n",
    "plt.xlabel(\"Points (sorted by distance)\")\n",
    "plt.ylabel(f\"{k_neighbors}-th Nearest Neighbor Distance\")\n",
    "plt.title(f\"k-Distance Plot (k={k_neighbors}) for eps Estimation\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DBSCAN with estimated eps\n",
    "# Choosing eps from the elbow region of the k-distance plot\n",
    "eps_value = 1.8\n",
    "min_samples_value = 10\n",
    "\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "df[\"dbscan_cluster\"] = dbscan_labels\n",
    "\n",
    "n_clusters_db = len(set(dbscan_labels) - {-1})\n",
    "n_noise = (dbscan_labels == -1).sum()\n",
    "\n",
    "print(f\"DBSCAN Results (eps={eps_value}, min_samples={min_samples_value}):\")\n",
    "print(f\"  Clusters found: {n_clusters_db}\")\n",
    "print(f\"  Noise points:   {n_noise} ({n_noise/len(df):.1%})\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "print(df[\"dbscan_cluster\"].value_counts().sort_index())\n",
    "\n",
    "if n_clusters_db >= 2:\n",
    "    non_noise_mask = dbscan_labels != -1\n",
    "    sil_db = silhouette_score(X_scaled[non_noise_mask], dbscan_labels[non_noise_mask])\n",
    "    print(f\"\\n  Silhouette Score (excl. noise): {sil_db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(sorted(unique_labels), colors):\n",
    "    mask = dbscan_labels == label\n",
    "    if label == -1:\n",
    "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=\"gray\", alpha=0.3,\n",
    "                    s=15, marker=\"x\", label=\"Noise\")\n",
    "    else:\n",
    "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=[color], alpha=0.6,\n",
    "                    s=25, edgecolors=\"k\", linewidths=0.3, label=f\"Cluster {label}\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(f\"DBSCAN Clusters (eps={eps_value}, min_samples={min_samples_value})\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "sil_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
    "sil_hier = silhouette_score(X_scaled, agg_labels)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Algorithm\": [\"KMeans\", \"Hierarchical\", \"DBSCAN\"],\n",
    "    \"Clusters Found\": [best_k, best_k, n_clusters_db],\n",
    "    \"Noise Points\": [0, 0, n_noise],\n",
    "    \"Silhouette Score\": [sil_kmeans, sil_hier,\n",
    "                         sil_db if n_clusters_db >= 2 else float(\"nan\")],\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a>\n",
    "## 11. Business Interpretation\n",
    "\n",
    "Based on the KMeans cluster profiles (our primary segmentation), we assign descriptive segment names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review cluster profiles and assign names\n",
    "print(\"Cluster Profiles for Naming:\")\n",
    "print(\"=\" * 80)\n",
    "for cluster_id in range(best_k):\n",
    "    profile = cluster_profile.loc[cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} (n={int(profile['count'])}):\")\n",
    "    print(f\"  Age:              {profile['age']:.0f}\")\n",
    "    print(f\"  Annual Income:    ${profile['annual_income']:,.0f}\")\n",
    "    print(f\"  Spending Score:   {profile['spending_score']:.0f}\")\n",
    "    print(f\"  Purchase Freq:    {profile['avg_purchase_frequency']:.1f}/month\")\n",
    "    print(f\"  Days Since Last:  {profile['days_since_last_purchase']:.0f}\")\n",
    "    print(f\"  Products Bought:  {profile['num_products_bought']:.0f}\")\n",
    "    print(f\"  Lifetime Value:   ${profile['customer_lifetime_value']:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name segments based on profiles\n",
    "# We rank clusters by CLV and spending to assign names\n",
    "segment_profiles = cluster_profile[feature_cols].copy()\n",
    "\n",
    "# Build a rule-based naming scheme\n",
    "segment_names = {}\n",
    "name_candidates = [\n",
    "    \"High-Value Loyalists\",\n",
    "    \"Affluent Trendsetters\",\n",
    "    \"Steady Regulars\",\n",
    "    \"Budget-Conscious Shoppers\",\n",
    "    \"Dormant High-Earners\",\n",
    "    \"New Explorers\",\n",
    "]\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    p = segment_profiles.loc[cluster_id]\n",
    "    if p[\"spending_score\"] >= 70 and p[\"annual_income\"] >= 70000:\n",
    "        segment_names[cluster_id] = \"High-Value Loyalists\"\n",
    "    elif p[\"spending_score\"] >= 55 and p[\"annual_income\"] < 40000:\n",
    "        segment_names[cluster_id] = \"Budget-Conscious Shoppers\"\n",
    "    elif p[\"days_since_last_purchase\"] >= 60 and p[\"annual_income\"] >= 70000:\n",
    "        segment_names[cluster_id] = \"Dormant High-Earners\"\n",
    "    elif p[\"avg_purchase_frequency\"] >= 5 and p[\"spending_score\"] >= 40:\n",
    "        segment_names[cluster_id] = \"Steady Regulars\"\n",
    "    else:\n",
    "        segment_names[cluster_id] = f\"Segment {cluster_id}\"\n",
    "\n",
    "df[\"segment_name\"] = df[\"kmeans_cluster\"].map(segment_names)\n",
    "\n",
    "print(\"Segment Names:\")\n",
    "for cid, name in sorted(segment_names.items()):\n",
    "    count = (df[\"kmeans_cluster\"] == cid).sum()\n",
    "    print(f\"  Cluster {cid}: {name} (n={count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization with segment names\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cluster_id in range(best_k):\n",
    "    mask = kmeans_labels == cluster_id\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], alpha=0.6, s=30,\n",
    "                edgecolors=\"k\", linewidths=0.3, label=segment_names[cluster_id])\n",
    "\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "plt.title(\"Customer Segments in PCA Space\")\n",
    "plt.legend(fontsize=10, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a>\n",
    "## 12. Conclusions and Marketing Recommendations\n",
    "\n",
    "### Summary of Segments\n",
    "\n",
    "| Segment | Profile | Recommended Strategy |\n",
    "|---------|---------|---------------------|\n",
    "| **High-Value Loyalists** | High income, high spending, frequent purchases | VIP programs, exclusive early access, loyalty rewards |\n",
    "| **Steady Regulars** | Middle income, moderate spending, consistent activity | Cross-sell and upsell, bundle offers, referral incentives |\n",
    "| **Dormant High-Earners** | High income but infrequent, high recency | Win-back campaigns, personalized re-engagement emails, premium product showcases |\n",
    "| **Budget-Conscious Shoppers** | Lower income, decent spending score, price-sensitive | Discount campaigns, value bundles, flash sales, free shipping offers |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **KMeans** was the most effective algorithm for this dataset, producing clean, well-separated clusters with the highest silhouette score.\n",
    "2. **Hierarchical Clustering** produced very similar results, validating the KMeans solution.\n",
    "3. **DBSCAN** identified some outlier customers who may warrant individual attention.\n",
    "4. The optimal number of segments is 4, each with distinct behavioral patterns.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **A/B test** segment-specific marketing campaigns to measure uplift.\n",
    "2. **Build a scoring pipeline** to assign new customers to segments in real time.\n",
    "3. **Monitor segment drift** over time and re-cluster quarterly.\n",
    "4. **Enrich with additional data** (website behavior, email engagement, social media) for finer segmentation.\n",
    "5. **Combine with predictive models** (e.g., churn prediction per segment) for prioritized outreach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}